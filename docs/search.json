[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Statistics for the Experimental Bench Biologist",
    "section": "",
    "text": "Preface\nThis book is an introduction to the statistical analysis of data from biological experiments with a focus on the estimation of treatment effects and measures of the uncertainty of theses estimates. Instead of a flowchart of “which statistical test”, this book emphasizes a regression modeling approach using linear models and extensions of linear models.\n“What what? I learned from the post-doc in my lab that regression was for data with a continuous independent variable and that t-tests and ANOVA were for data with categorical independent variables.” No! This misconception has roots in the history of regression vs. ANOVA and is reinforced by how introductory biostatistics textbooks, and their instructors, choose to teach statistics.\nClassical linear regression, t-tests and ANOVA are all special cases of a linear model. The different linear models in this text are all variations of the equation for a line \\(Y = mX + b\\) using slightly different notation:\n\\[\nY = \\beta_0 + \\beta_1 X\n\\tag{1}\\]\nChapter 8  An introduction to linear models explains the meaning of this notation. Here, just recognize that this is a regression model, but in modern statistics, we use this to not only estimate the effects of a continuous \\(X\\) variable on some response (classical regression) \\(Y\\) but also for the estimation of effects of a categorical treatment variable on some response (as in classical t-tests and ANOVA). A regression model with a categorical treatment variable is possible because the treatment variable is recoded into a numeric indicator variable indicating group membership (“wildtype” or “knockout”). Classical t-tests and ANOVA are equivalent to special cases of regression models but the ANOVA model is usually presented in a different way, one that allows simple “paper and pencil math” (addition, subtraction, multiplication, division). The linear model underneath a classical ANOVA is some variation of\n\\[\n\\overline{Y}_k = \\mu + \\alpha_k\n\\tag{2}\\]\nwhere \\(\\mu\\) is the grand mean and \\(\\alpha_k\\) is the difference between the mean of treatment k and the grand mean.\nIn this text, I use linear model for any model that looks like Equation 1 and ANOVA model for any model that looks like Equation 2 (many statistics textbooks call these “cell-mean models”). It would be more specific to call models that look like Equation 1 a regression model but the word “regression” has a lot of baggage because of how it is taught – as a method for data with continuous \\(X\\) (“independent”) variables. If I said, “this is a book of regression models for analyzing your experimental data”, you might quite reasonably, but incorrectly, assume that the book wasn’t really relevant to your experiments because your independent variables are categorical (“WT” vs.”KO”) and not continuous.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#arent-t-tests-and-anova-good-enough",
    "href": "index.html#arent-t-tests-and-anova-good-enough",
    "title": "Statistics for the Experimental Bench Biologist",
    "section": "Aren’t t-tests and ANOVA good enough?",
    "text": "Aren’t t-tests and ANOVA good enough?\nFor many experiments, the linear models advocated here will give the same p-value as a t-test or ANOVA, which raises the question, why bother with linear models? Some answers include\n\nFlexibility. Linear models and extensions of linear models are all variations of \\(Y = \\beta_0 + \\beta_1 X\\). Generalizations of this basic model include linear models with added covariates or multiple factors, generalized least squares models for heterogeneity of variance or correlated error, linear mixed models for correlated error or hierarchical grouping, generalized linear models for counts and other biological data that don’t have normal distributions, generalized additive models for responses that vary non-linearly over time, causal graphical models for inferring causal effects with observational data, multivariate models for multiple responses, and some machine learning models. This book is not a comprehensive source for any of these methods but an introduction to the common foundations of all.\nIssues in inference. Issues in inference from t-test/ANOVA occur when the data violate the assumptions of independence, homogeneity of variances, or a normal conditional response. Linear (regression!) models were expanded in different ways to specifically model these violations. Many of these models have no ANOVA equivalent – consequently, researchers using ANOVA are forced to use the wrong model or kludgy alternatives, such as non-parametric tests.\nGateway drug. Many of the statistical models used in genomics are variations of the linear models introduced here. There will be a steep learning curve for these methods if your statistical training consists of t-tests, ANOVA, and Mann-Whitney non-parametric tests.\nBiologically meaningful focus. Linear models encourage looking at, thinking about, and reporting estimates of the size of a treatment effect and the uncertainty of the estimate. The estimated treatment effect is the difference in the response between two treatments. If the mean plasma glucose concentration over the period of a glucose tolerance test is 15.9 mmol/l in the knockout group and 18.9 mmol/l in the wildtype group, the estimated effect is -3.0 mmol/l. The magnitude of this effect is our measure of the difference in glucose tolerance between the two treatments. What is the physiological consequence of this difference? Is this a big difference that would excite NIH or a trivial difference that encourages us to pursue some other line of research? I don’t know the answers to these questions– I’m not a metabolic physiologist. Researchers in metabolic physiology should know the answers, but if they do, they don’t indicate this in the literature. Effect sizes are rarely reported in the experimental bench-biology literature.\n\nWhat is reported are p-values. Small p-values give the researchers confidence that “an effect exists” and an abundance of small p-values from a series of experiments that have rigorously probed a system give the researchers confidence that they have discovered knowledge about some biological mechanism. This inference strategy works okay in bench biology. But, p-values can also create misconceptions. For example, extremely small p-values give some researchers the confidence that an effect is large or important. This confidence is unwarranted. P-values are not a useful measure of effect size.\nP-values are neither necessary nor sufficient for good data analysis. But, a p-value is a useful tool in the data analysis toolkit. If the conduction of the experiment and analysis of the results closely approximate the model underlying the computation of the p-value, then a p-value dampens the frequency that we are fooled by randomness and gives a researcher some confidence in the direction (positive or negative) of an effect. Importantly, the estimation of effects and uncertainty and the computation of a p-value are not alternatives. Throughout this text, linear models are used to compute a p-value in addition to the estimates of effects and their uncertainty.\n\n\n\n\n\n\nNHST Blues\n\n\n\nThe emphasis on p-values as the measure to report is a consequence of the “which statistical test?” strategy of data analysis. This practice, known as Null-Hypothesis Significance Testing (NHST), has been criticized by statisticians for many, many decades. Nevertheless, introductory biostatistics textbooks written by both biologists and statisticians continue to organize textbooks around a collection of hypothesis tests, with a great deal of emphasis on “which statistical test?” and much less emphasis on estimation and uncertainty. The NHST/which-statistical-test strategy of learning or doing statistics is easy in that it requires little understanding of the statistical model underneath the tests and its assumptions, limitations, and behavior. The NHST strategy in combination with point-and-click software enables mindless statistics and encourages the belief that statistics is a tool like a word processor is a tool, after all, a rigorous analysis of one’s data requires little more than getting p-values and creating bar plots. Indeed, many PhD programs in the biosciences require no statistics coursework and the only training available to students is from the other graduate students and postdocs in the lab. As a consequence, the biological sciences literature is filled with error bars that imply data with negative values and p-values that have little relationship to the probability of the data given the experimental design. More importantly for science, the reported statistics are often not doing for the study what the researchers and journal editors think they are doing.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#what-is-unusual-about-this-book",
    "href": "index.html#what-is-unusual-about-this-book",
    "title": "Statistics for the Experimental Bench Biologist",
    "section": "What is unusual about this book?",
    "text": "What is unusual about this book?\n\nReal data sets from the recent experimental biology literature. All of the data in this text are from articles in which the researchers have followed open science best practices and made the data freely available by archiving the data with the article. This not only let’s me explain modern statistics relevant to experimental biologist’s experiments using data collected by experimental biologists but also allows me to connect the statistical explanation with the goals of the experiment. A potential downside of using real data is that understanding any analysis requires some level of understanding of the underlying biology and the underlying methods of data collection. I try to give enough background biology to understand the motivation of the experiment and experimental methodology to understand the data.\nBecause I scan many, many articles in experimental biology looking for good archived data to use as examples in this text, I’ve developed an appreciation for the variation in how experimental biologists think about and do statistics. A consequence of this is the recognition of the many ways that researchers use non-best practices and occasionally worst practices. Much of the material in each chapter and one entire chapter (Issues in inference) is devoted to addressing the consequences of these non-best and worst practices.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "1  Analyzing experimental data with a linear model",
    "section": "",
    "text": "1.1 This text is about using linear models to estimate treatment effects and the uncertainty in our estimates. This, raises the question, what is “an effect”?\nAt the end of this text, I provide an example set of analyses for multiple, related experiments. This example is a goal or target; it’s what you will be working towards as you learn from this text. The data for the analysis come from multiple experiments presented in Figure 2 in the article ASK1 inhibits browning of white adipose tissue in obesity. The chapter preceding the analysis is just enough biology to help you understand the biological importance of each experiment. The data for Figure 2 is from a set of experiments exploring the consequences of adipose-tissue specific deletion of the ASK1 signaling protein on multiple, adverse effects of a high-fat diet in mice, including weight gain, glucose intolerance, and increased liver triacylglycerol levels. I chose the data in Figure 2 of this paper because of the diversity of analyses and plot types. My analyses and plots differ slightly from those of the researchers because I implemented better practices – the stuff of this text.\nHere, I use one panel from Figure 2 to outline what the statistical analysis of experimental data is all about. Much of this outline will be repeated in “An introduction to linear models” chapter.\nThe goal of the experiments is to estimate the effect of the adipose-specific ASK1 deletion. To understand what I mean by “an effect”, and to understand how we can estimate an effect by fiting a linear model to data, let’s peak at the results of the fit model.\nThe effect of adipose-specific ASK1 knockout on liver triglyceride (TG).\nFor this experiment, the researchers want to know if knocking out the ASK1 gene in the adipose tissue cells lowers the liver triglyceride (TG) level in mice fed a high-fat diet. To investigate this, the researchers compared the TG levels in a group of knockout mice (“ASK1Δadipo” – Δ is the del operator and refers to a deletion in genetics) to the TG levels in a group of control mice (“ASK1F/F”). Specifically, they measured the difference in the mean TG of the control group from the mean TG of the knockout group.\n\\[\n\\overline{\\texttt{TG}}_\\texttt{ASK1Δadipo} - \\overline{\\texttt{TG}}_\\texttt{ASK1F/F}\n\\]\nThe measured difference in the means is the estimate of the effect of ASK1 deletion on liver TG levels. This estimate is -21.6 µmol per g liver. An effect has the same units as the variable compared (µmol per g liver), a magnitude (21.6 units), and a direction (negative).\nMy version of Figure 2i, which shows the results of the experiment, is Fig. @ref(fig:ask1-fig2i-ggplot-the-model) above. The direction and magnitude of the estimated effect (the measured difference in means) can be mentally reconstructed by comparing the position of the two group means in the lower part of Fig. @ref(fig:ask1-fig2i-ggplot-the-model). The upper part of Fig. @ref(fig:ask1-fig2i-ggplot-the-model) explicitly shows the estimate and shows the the uncertainty in the estimate using 95% confidence intervals.\nThe numbers to make this plot come from the coefficient table of the fit model, shown in Table @ref(tab:ask1-fig2i-intro-coef-table). The second row of this table are the statistics for the effect of the knockout on TG levels. The value in the “Estimate” column is the estimated effect (the measured difference in means). The value in the “Std. Error” column is the standard error of the difference in means (SE), a measure of uncertainty of the estimate. We use this SE to compute the statistic for a t-test, from which we get the p-value, the probability of observing a t-value as large or larger than our observed value, if the null were true. The “null is true” not only means the true effect is zero but also assumes a long list of conditions, such as, a random treatment assignment and homogeneity of variance. We also use the SE to compute the 95% confidence intervals of the difference. Understanding what a standard error is and how to interpret confidence intervals is paramount to practicing good statistics. This is covered in the chapter Variability and Uncertainty (Standard Deviations, Standard Errors, and Confidence Intervals).\nCoefficient table for linear model fit to fig2i data.\n\n\n\nEstimate\nStd. Error\nt value\nPr(&gt;&#124;t&#124;)\n2.5 %\n97.5 %\n\n\n\n\n(Intercept)\n61.47\n4.98\n12.3\n0.000\n50.37\n72.57\n\n\ntreatmentASK1Δadipo\n-21.60\n7.05\n-3.1\n0.012\n-37.30\n-5.90\nIt is hard to overemphasize that what we measure in experiments is estimated effects. The true effect may be larger, smaller, or even in the reverse direction. This text is all about what we can infer about true effects from the statistical analysis of experimental data. This inference requires that we also measure our uncertainty in our estimate of the effect.\nThe measured means in each group are computed from a random sample of mice. If we only cared about the six mice in each group in this experiment, then we would not need to fit a linear model to the data to estimate the effect, we could simply compute each group’s mean and subtract the control mean from the knockout mean. But we care more about something more than these dozen mice because we are trying to discover something general about ASK1 regulation of TG levels in mice, generally (and even in mammals, and especially humans, generally). To make this leap of inference, we use a model to claim that each sample mean is an estimate of the respective population mean. Given this model, we can compute the standard error of each mean and the standard error of the difference in means. A standard error is a measure of the sampling variance of a statistic and, therefore, a measure of the precision of the estimate. The standard error, then, is a measure of uncertainty in the estimate. Here is how to think about precision and uncertainty: if we were to repeat this experiment many, many times, we would generate a long list of mean TG levels for the control mice and a long list of mean TG levels for the knockout mice. The less variable the means in each list, the more precise. By using a model, we do not need to repeat this experiment many times to get a standard error.\nThe model we are going to fit to the Figure 2i data is\nThis is a model of how the Figure 2i data were generated. In this model, \\(y\\) is the liver TG level for some fictional, randomly generated mouse and \\(x\\) is a variable that indicates the condition of the ask1 gene in randomly generated mouse – a value of 0 is given to mice with a functional ASK1 gene and a value of 1 is given to mice with a knocked out gene.\n\\(\\beta_0\\) is the “true” mean of TG in mice fed a high-fat diet and with a functional ASK1 gene. By “true”, I mean the mean that would be computed if we were to measure TG on an infinite number of these mice (exactly what “these mice” means is a good topic for a campfire discussion). The observed mean of the ASK1F/F group is an estimate of \\(\\beta_0\\). The sum \\(\\beta_0\\) + \\(\\beta_1\\) is the true mean of TG in mice fed a high-fat diet but with a knocked out ASK1 gene. This means that \\(\\beta_1\\) is the true difference in the means, or the true effect. The observed difference in means between the ASK1Δadipo and ASK1F/F groups is an estimate of \\(\\beta_1\\). This difference is the estimated effect.\nThe sum \\(\\beta_0 + \\beta_1 x\\) is the expectation of TG, or expected value of TG in a generated mouse given the generating model. This sum equals the true mean of the infinite set of normal mice if \\(x = 0\\) and equals the true mean of the infinite set of ASK1 knockout mice if \\(x = 1\\). All generated control mice have the same expected value of TG. All generated knockout mice have the same expectated value of TG.\n\\(\\varepsilon\\) is the error for the randomly generated mouse. It is a random number sampled from a Normal distribution with a mean of zero and a variance of \\(\\sigma^2\\). The variation in generated mice has a systematic component due to variation in \\(x\\) and a random (or stochastic) component due to variation in \\(\\varepsilon\\).\nBy fitting a model to the data we estimate the parameters \\(\\beta_0\\), \\(\\beta_1\\) and \\(\\sigma\\). It is the estimation of \\(\\sigma\\) that allows us to compute a measure of our uncertainty (a standard error) of our estimates of the means (\\(\\beta_0\\) and \\(\\beta_0 + \\beta_1\\)) and of the difference in the means (\\(\\beta_1\\)).\nLet’s fit this model to the Figure 2i data using R.\nfig2i_m1 &lt;- lm(liver_tg ~ treatment, data = fig2i)\nRobust inference from the model (generalizing from sample to population, including measures of the uncertainty of our estimates, requires that our data approximates the kind of data we’d expect from the data generating model specified above. All rigorous analysis should use specific model checks to evaluate this. First, the “normality check” – we use a quantile-quantile (QQ) plot to see if our data approximate what we’d see if we sampled from a normal distribution.\nset.seed(1)\nqqPlot(fig2i_m1, id=FALSE)\n95% of quantiles computed from fake samples from the generating model above are inside the two dashed lines in the plot. Our measured quantiles are the open circles. The quantiles from a sample that looks like it was sampled from a normal distribution, or “looks Normal”, should be approximately linear and mostly lie within the dashed lines. Our sample looks good. Regardless, inference is pretty robust to moderate departure from Normal.\nSecond, the “homogeneity check” – we use a spread level plot to see if there is some pattern to the variance, for example if the spread of residuals is noticeably bigger in one group than another, or if the spread increases with the fitted value.\nspreadLevelPlot(fig2i_m1, id=FALSE)\n\n\n\n\n\n\n\n\n\nSuggested power transformation:  1.294553\nThis looks pretty good. Given these checks, lets move on and look at the table of model coefficients.\nfig2i_m1_coef &lt;- cbind(coef(summary(fig2i_m1)),\n                        confint(fig2i_m1))\nEstimate\nStd. Error\nt value\nPr(&gt;&#124;t&#124;)\n2.5 %\n97.5 %\n\n\n\n\n(Intercept)\n61.5\n4.98\n12.3\n0.000\n50.4\n72.6\n\n\ntreatmentASK1Δadipo\n-21.6\n7.05\n-3.1\n0.012\n-37.3\n-5.9\nThe two values in the column “Estimate” are the estimates of \\(\\beta_0\\) and \\(\\beta_1\\). The top value (61.5) is the mean of the control mice (the units are µmol/g). The mean of the knockout mice is the sum of the two values (39.9 µmol/g). And the effect of ASK1 deletion on TG levels is simply the second value (-21.6 µmol/g). The standard error of the effect is 7.05 µmol/g.\nWe can use the standard error to compute a t-value (-3.1, in the column “t value”). A t-value is a test statistic. The probability (“p value”) of the significance test is 0.012. This if the probability of sampling a t-value as large or larger than the observed t-value, if we were to sample from a null distribution of t-values (a distribution of sampled t values if the true value of \\(\\beta_1\\) was 0). We can also use the standard error to compute a 95% confidence interval of the effect. The lower bound of this interval is -37.3 µmol/g and the upper bound is -5.9 µmol/g. A confidence interval is another way of communicating uncertainty, and the way advocated in this text. In a 95% confidence interval, 95% of similarly constructed intervals (from hypothetical sampling of six mice from the ASK1 normal population and six mice from the ASK1 knockout population) will contain the true mean. Another way to think about a confidence interval is, it is the range of true differences that are compatible with the data, where compatible means “not rejected” by a t-test (a t-test between the estimated effect and any number inside the interval would return a p-value greater than 0.05).\nHere is how we might report this result in a paper:\nMean TG level in ASK1Δadipo mice on a high-fat diet was 21.6 µmol/g less than that in ASK1F/F mice on a high-fat diet (95% CI: -37.3, -5.9, \\(p = 0.012\\)) (Fig.@ref(fig:ask1-fig2i-ggplot_the_model)).",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Analyzing experimental data with a linear model</span>"
    ]
  },
  {
    "objectID": "intro.html#this-text-is-about-using-linear-models-to-estimate-treatment-effects-and-the-uncertainty-in-our-estimates.-this-raises-the-question-what-is-an-effect",
    "href": "intro.html#this-text-is-about-using-linear-models-to-estimate-treatment-effects-and-the-uncertainty-in-our-estimates.-this-raises-the-question-what-is-an-effect",
    "title": "1  Analyzing experimental data with a linear model",
    "section": "",
    "text": "Better Know: Population\n\n\n\nWhat is a population? In the experimental biology examples in this text, we might consider the population as a very idealized, infinitely large set of mice, or fish, or fruit flies, or communities from which our sample is a reasonably representative subset. For the experiments in the ASK1Δadipo study, the population might be conceived of as the hypothetical, infinitely large set of floxed ASK1 C57BL/6 mice bred in the mouse facility of the researchers under the same rearing conditions. By infinitely large, I mean something like all possible phenotypes that could be born from the nearly-infinite combination of meiosis, gene-gene interactions, genotype-environment interactions, and maternal effects.\n\n\n\n\\[\\begin{align}\ny &= \\beta_0 + \\beta_1 x + \\varepsilon\\\\\n\\varepsilon &\\sim N(0, \\sigma^2)\n\\end{align}\\]",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Analyzing experimental data with a linear model</span>"
    ]
  },
  {
    "objectID": "chapters/setup.html",
    "href": "chapters/setup.html",
    "title": "2  Getting Started – R Projects and R Markdown",
    "section": "",
    "text": "2.1 R vs R Studio\nR is a programming language. It runs under the hood. You never see it. To use R, you need another piece of software that provides a user interface. The software we will use for this is R Studio. R Studio is a slick (very slick) graphical user interface (GUI) for developing R projects.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Getting Started -- R Projects and R Markdown</span>"
    ]
  },
  {
    "objectID": "chapters/setup.html#download-and-install-r-and-r-studio",
    "href": "chapters/setup.html#download-and-install-r-and-r-studio",
    "title": "2  Getting Started – R Projects and R Markdown",
    "section": "2.2 Download and install R and R studio",
    "text": "2.2 Download and install R and R studio\nDownload R for your OS\nDownload R Studio Desktop\nIf you need help installing R and R studio, here is Andy Field’s Installing R and RStudio video tutorial)",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Getting Started -- R Projects and R Markdown</span>"
    ]
  },
  {
    "objectID": "chapters/setup.html#open-r-studio-and-modify-the-workspace-preference",
    "href": "chapters/setup.html#open-r-studio-and-modify-the-workspace-preference",
    "title": "2  Getting Started – R Projects and R Markdown",
    "section": "2.3 Open R Studio and modify the workspace preference",
    "text": "2.3 Open R Studio and modify the workspace preference\n\nOpen R Studio\nClick on R Studio &gt; Preferences to\nClick on General in the left menu\ndiable “Restore .RData into workspace at startup”\nClick on the “Save workspace to .RData on exit” popup menu and choose “Never”\n\n\n\n\n\n\n\n\n\n\nWhat’s going on here? The workspace contains the values of all the objects created by the R code that you’ve run in the working R session. Nothing good comes from this. You want to start each R session with a clean slate, a blank workspace. This means that when you start a new R session, you will need to re-run all your code chunks to start where you left-off at the close of your last R session. This seems tedious but, be warned, bad things will happen if you save the workspace from the last session and re-load this at startup. Trust me. Just don’t do it.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Getting Started -- R Projects and R Markdown</span>"
    ]
  },
  {
    "objectID": "chapters/setup.html#if-you-didnt-modify-the-workspace-preferences-from-the-previous-section-go-back-and-do-it",
    "href": "chapters/setup.html#if-you-didnt-modify-the-workspace-preferences-from-the-previous-section-go-back-and-do-it",
    "title": "2  Getting Started – R Projects and R Markdown",
    "section": "2.4 If you didn’t modify the workspace preferences from the previous section, go back and do it",
    "text": "2.4 If you didn’t modify the workspace preferences from the previous section, go back and do it",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Getting Started -- R Projects and R Markdown</span>"
    ]
  },
  {
    "objectID": "chapters/setup.html#r-markdown-in-a-nutshell",
    "href": "chapters/setup.html#r-markdown-in-a-nutshell",
    "title": "2  Getting Started – R Projects and R Markdown",
    "section": "2.5 R Markdown in a nutshell",
    "text": "2.5 R Markdown in a nutshell\nIn this text, we will write code to analyze data using R Markdown. R markdown is a version of Markdown. Markdown is tool for creating a document containing text (like microsoft Word), images, tables, and code that can be output, or knitted, to the three modern output formats: html (web pages), pdf (reports and documents), and microsoft word (okay, this isn’t modern but it is widely used).\nThe R Markdown, or .Rmd, document contains three components:\n\na YAML header, which specifies formatting and styles for the knitted document\nthe code “chunks”, which are blocks of code that do something\nthe space before and after the code chunks which contains text and any output images or tables from the code chunks.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Getting Started -- R Projects and R Markdown</span>"
    ]
  },
  {
    "objectID": "chapters/setup.html#install-r-markdown",
    "href": "chapters/setup.html#install-r-markdown",
    "title": "2  Getting Started – R Projects and R Markdown",
    "section": "2.6 Install R Markdown",
    "text": "2.6 Install R Markdown\nDirections for installing R Markdown\nR Markdown can output pdf files. The mechanism for this is to first create a LaTeX (“la-tek”) file. LaTeX is an amazing tool for creating professional pdf documents. You do not need PDF output for this text, but I encourage you to download and install the tinytex distribution, which was created especially for R Markdown in R Studio.\nThe tinytex distribution is here.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Getting Started -- R Projects and R Markdown</span>"
    ]
  },
  {
    "objectID": "chapters/setup.html#importing-packages",
    "href": "chapters/setup.html#importing-packages",
    "title": "2  Getting Started – R Projects and R Markdown",
    "section": "2.7 Importing Packages",
    "text": "2.7 Importing Packages\nThe R scripts you write will include functions in packages that are not included in Base R. These packages need to be downloaded from an internet server to your computer. You only need to do this once (although you have to redo it each time you update R). But, each time you start a new R session, you will need to load a package using the library() function. Now is a good time to import packages that we will use\nOpen R Studio and choose the menu item “Tools” &gt; “Install Packages”. In the “packages” input box, insert the names of packages to install the package. The names can be separated by spaces or commas, for example “data.table, emmeans, ggplot2”. Make sure that “install dependencies” is clicked before you click “Install”. Packages that we will use in this book are\n\nImport and wrangling packages\n\n\ndevtools – we use this to install packages that are not on CRAN\nhere – we use this to read from and write to the correct folder\njanitor – we use the function clean_names from this package\nreadxl – elegant importing from microsoft Excel spreadsheets\ndata.table - we use the data.table way to wrangle data in this text.\nstringr – we use this to wrangle character variables\n\n\nanalysis packages\n\n\nemmeans – we use this to compute modeled means and contrasts\nnlme – we use this for gls models\nlme4 – we use this for linear mixed models\nlmerTest – we use this for inference with linear mixed models\nglmmTMB – we use this for generalized linear models\nMASS – we will use glm.nb from this package\nafex – we use this for classic ANOVA linear models\ncar – we use this for model checking\nDHARMa – we use this for model checking generalized linear models\ninsight – we use this to learn about models\n\n\ngraphing and tabling packages\n\n\nggplot2 – we use this for plotting\nggsci – we use this for the color palettes\nggthemes – we use this for the colorblind palette\nggpubr – we use this to make ggplots a bit easier\nggforce – we use this for improved jitter plots\ndabestr – we use this to make several plot types\ncowplot – we use this to combine plots\nknitr – we use this to make kable tables\nkableExtra – we use this to improve kable tables\n\nOnce these are installed, you don’t need to do this again although there will be additional packages that you might install. You simply need to use the library() function at the start of a markdown script.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Getting Started -- R Projects and R Markdown</span>"
    ]
  },
  {
    "objectID": "chapters/setup.html#setup-create-project",
    "href": "chapters/setup.html#setup-create-project",
    "title": "2  Getting Started – R Projects and R Markdown",
    "section": "2.8 Create an R Studio Project for this textbook",
    "text": "2.8 Create an R Studio Project for this textbook\n\nCreate a project folder within the Documents folder (Mac OS) or My Documents folder (Windows OS). All files associated with this book will reside inside this folder. The name of the project folder should be something meaningful, such as “Applied Biostatistics” or the name of your class (for students in my Applied Biostatistics class, this folder could be named “BIO_413”).\nWithin the project folder, create new folders named\n\n“Rmd” – this is where your R markdown files are stored\n“R” – this is where additional R script files are stored\n“data” – this is where data that we download from public archives are stored\n“output” – this is where you will store fake data generated in this class\n“images” – this is where image files are stored\n\nOpen R Studio and click the menu item File &gt; New Project…\nChoose “Existing Directory” and navigate to your project folder\nChoose “Create Project”\nCheck that a “.Rproj” file is in your project folder\nDownload and move the file ggplot_the_model.R into the R folder.\n\n\n\n\n\n\nProject folder with the .Rproj file and all main folders located at the first level of the project\n\n\n\n\nThe project directory should look like that in Figure @ref(fig:project-organization-image). Importantly, the project file (“Applied Biostatistics.Rproj”) and the main folders are all located at the first level within the project folder.\nBug alert If your .Rproj file is somewhere else (on the desktop, in the data folder, etc.), bad things will happen.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Getting Started -- R Projects and R Markdown</span>"
    ]
  },
  {
    "objectID": "chapters/setup.html#working-on-a-project-in-a-nutshell",
    "href": "chapters/setup.html#working-on-a-project-in-a-nutshell",
    "title": "2  Getting Started – R Projects and R Markdown",
    "section": "2.9 Working on a project, in a nutshell",
    "text": "2.9 Working on a project, in a nutshell\n\nWake up, brush teeth, and open the project by double-clicking the .Rproj icon. Alternatively, open R Studio and then use the File &gt; Open Project to open the project. The name of the project will at the top-right of the R Studio window. We always want to work within an open project and the first workflow guarantees this. If we Open R Studio and then open a .Rmd file, we could be working within another project or no project at all. Bad things will happen.\nRun previous code chunks, in order (top to bottom). Write new code in code chunks and run. When we run code, we add R objects to the workspace. The workspace contains the values of all the objects created by the R code that has been run in the working session. When I save the .Rmd file, these values are not saved, only the text and code chunks in the R Markdown document. This is a feature, not a bug.\nWhen we are finished with the session, quit R Studio. If you get a popup window asking if you want to save the workspace, click “No”. Then immediately go back to the section “Open R Studio and modify the workspace preference” above and follow the directions.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Getting Started -- R Projects and R Markdown</span>"
    ]
  },
  {
    "objectID": "chapters/setup.html#setup-create-rmd",
    "href": "chapters/setup.html#setup-create-rmd",
    "title": "2  Getting Started – R Projects and R Markdown",
    "section": "2.10 Create and setup an R Markdown document (Rmd)",
    "text": "2.10 Create and setup an R Markdown document (Rmd)\n\nThe top-left icon in R Studio is a little plus sign within a green circle. Click this and choose “R Markdown” from the pull-down menu.\nGive the file a meaningful title.\nAdd your name in the Author text book. After the first time doing this, R Studio will default to your name.\nR Studio opens a demo document. Delete all text below the first code chunk, starting with the header “## R Markdown”\n\n\n2.10.1 Modify the yaml header\nReplace “output: html_document” in the yaml header with the following in order to create a table of content (toc) on the left side of the page and to enable code folding\noutput:\n  html_document:\n    toc: true\n    toc_float: true\n    code_folding: hide\n\n\n2.10.2 Modify the “setup” chunk\nAll R Markdown documents should start with a setup chunk that loads the packages containing R functions that are not in Base R and defines a few key R objects, such as the name of the data folder. A good practice is to load only the packages used by the chunks in the the document. For a student or researcher new to R and R studio, this can be confusing because they are unlikely to be aware of which functions belong to which package and, therefore, which packages should be loaded.\nHere, I disregard this best practice and offer a general, all-purpose setup chunk for users of this text.\n\nknitr::opts_chunk$set(echo = TRUE)\n\n# wrangling packages\nlibrary(here) # here makes a project transportable\nlibrary(janitor) # clean_names\nlibrary(readxl) # read excel, duh!\nlibrary(data.table) # magical data frames\nlibrary(stringr) # string functions\n\n# analysis packages\nlibrary(emmeans) # the workhorse for inference\nlibrary(nlme) # gls and some lmm\nlibrary(lme4) # linear mixed models\nlibrary(lmerTest) # linear mixed model inference\nlibrary(afex) # ANOVA linear models\nlibrary(glmmTMB) # generalized linear models\nlibrary(MASS) # negative binomial and some other functions\nlibrary(car) # model checking and ANOVA\nlibrary(DHARMa) # model checking\n\n# graphing packages\nlibrary(ggsci) # color palettes\nlibrary(ggpubr) # publication quality plots\nlibrary(ggforce) # better jitter\nlibrary(cowplot) # combine plots\nlibrary(knitr) # kable tables\nlibrary(kableExtra) # kable_styling tables\n\n# ggplot_the_model.R packages not loaded above\nlibrary(insight)\n\n# use here from the here package\nhere &lt;- here::here\n# use clean_names from the janitor package\nclean_names &lt;- janitor::clean_names\n# use transpose from data.table\ntranspose &lt;- data.table::transpose\n\n# load functions used by this text written by me\n# ggplot_the_model.R needs to be in the folder \"R\"\n# if you didn't download this and add to your R folder in your\n# project, then this line will cause an error\nsource_path &lt;- here(\"R\", \"ggplot_the_model.R\")\nsource(source_path)\n\ndata_folder &lt;- \"data\"\nimage_folder &lt;- \"images\"\noutput_folder &lt;- \"output\"\n\necho = TRUE tells knitr to display the code within a code chunk when the R markdown file is knitted. knitr::opts_chunk$set(echo = TRUE) sets echo = TRUE for all code chunks in the containing .Rmd file (not all R markdown files in the project!).\nhere &lt;- here::here makes sure that here uses the function from the here package and not some other package. Huh? Let’s back up – R is an open source project and packages are written by independent programmers and scientists and not employees of some central company. When someone develops a package, they create functions that do stuff. Sometimes developers of different packages create functions that have the same name. There is a function name conflict if we load two packages with the same name. Our R session will use the function of the last loaded package as the function assigned to the name. If we want the name to be used with the function in the previously loaded package, then we need to either re-order the library() statements, or simply re-assign the name to the function that we want. This is what here &lt;- here::here does. This script takes the function here from the package “here” and assigns it to the object here.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Getting Started -- R Projects and R Markdown</span>"
    ]
  },
  {
    "objectID": "chapters/setup.html#lets-play-around-with-an-r-markdown-file",
    "href": "chapters/setup.html#lets-play-around-with-an-r-markdown-file",
    "title": "2  Getting Started – R Projects and R Markdown",
    "section": "2.11 Let’s play around with an R Markdown file",
    "text": "2.11 Let’s play around with an R Markdown file\n\n2.11.1 Create a “fake-data” chunk\n\nCreate a new chunk and label it “fake-data”. Insert the following R script and then click the chunk’s run button\n\n\nset.seed(4)\nn &lt;- 10\nfake_data &lt;- data.table(\n    treatment = rep(c(\"cn\", \"tr\"), each = n),\n    neutrophil_count_exp1 = rnegbin(n*2, \n                                    mu = rep(c(10, 15), each = n),\n                                    theta = 1),\n    neutrophil_count_exp2 = rnegbin(n*2, \n                                    mu = rep(c(10, 20), each = n),\n                                    theta = 1)\n)\n# View(fake_data)\n\nThis chunk creates fake neutrophil counts in two different experiments. The comment (#) sign before View(fake_data) “comments out” the line of code, so it is not run. View the data by highlighting View(fake_data) and choosing “Run selected line(s)” from the Run menu.\n\n\n2.11.2 Create a “plot” chunk\n\nCreate a new chunk and label it “plot”. Insert the following R script and then click the chunk’s run button\n\n\ngg_1 &lt;- ggstripchart(data = fake_data,\n                x = \"treatment\",\n                y = \"neutrophil_count_exp1\",\n                color = \"treatment\",\n                palette = \"jco\",\n                add = \"mean_se\",\n                legend = \"none\") +\n    ylab(\"Neutrophil Count (Exp. 1)\") +\n  stat_compare_means(method = \"t.test\",\n                     label.y = 50,\n                     label = \"p.format\") +\n    NULL\n\ngg_2&lt;- ggstripchart(data = fake_data,\n                x = \"treatment\",\n                y = \"neutrophil_count_exp2\",\n                color = \"treatment\",\n                palette = \"jco\",\n                add = \"mean_se\",\n                legend = \"none\") +\n  ylab(\"Neutrophil Count (Exp 2)\") +\n  stat_compare_means(method = \"t.test\",\n                     label.y = 65,\n                     label = \"p.format\") +\nNULL\n\nplot_grid(gg_1, gg_2, labels = \"AUTO\")\n\n\n\n\n\n\n\n\nEach plot shows the mean count for each group, the standard error of the mean count, and the p-value from a t-test. This statistical analysis and plot are typical of those found in experimental biology journals. This text will teach alterntatives that implement better practices.\n\n\n2.11.3 Knit the Rmd\n\nKnit to an html file\nKnit to a word document\nIf you’ve installed tinytex (or some other LaTeX distribution), knit to a pdf file",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Getting Started -- R Projects and R Markdown</span>"
    ]
  },
  {
    "objectID": "chapters/reading-writing-wrangling.html",
    "href": "chapters/reading-writing-wrangling.html",
    "title": "3  Data – Reading, Wrangling, and Writing",
    "section": "",
    "text": "3.1 Long data format – the way data should be\nSome best practices for archiving data tables, including archived data at journal websites, are illustrated by the format of the data in Table @ref(tab:data-diet-table).1\nFirst six rows of the diet data set.\n\n\nsubject_id\ninstitution\ndiet\nvo2\nlean_mass\nfat_mass\n\n\n\n\nD1\nUC Davis\nHF\n129.5653\n24.33708\n15.862920\n\n\nD10\nUC Davis\nLF\n100.0749\n21.08880\n3.151200\n\n\nD11\nUC Davis\nLF\n118.3097\n22.56076\n6.659238\n\n\nD12\nUC Davis\nLF\n107.7487\n22.11884\n5.351156\n\n\nD13\nUC Davis\nLF\n104.7367\n21.17438\n5.005616\n\n\nD14\nUC Davis\nHF\n138.6420\n24.64885\n16.501150\nThe best practices included in the archived data include\nThe vo2 values separated from the rest of the data and split into separate diet groups.\nIn general, R and almost all other statistical software (one exception is Graphpad Prism) require the data to be in long format.\n\\[\n\\mathbf{b} = (\\mathbf{X}^\\top\\mathbf{X})^{-1}\\mathbf{X}^\\top\\mathbf{y}\n\\]\nwhere \\(\\mathbf{X}\\) is the model matrix, a table of numeric values with each row containing the values of the \\(X\\) variables for an individual (categorical variables such as \\(\\texttt{diet}\\) are recoded into one or more numeric indicator variables indicating group membership). The model matrix is in long format. * A second reason is organizational simplicity. Statistical models often include multiple variables measured on the same individual – the example above with \\(\\texttt{lean\\_mass}\\) added as a covariate is an example. With data in wide format, there is no non-awkward way to match values of \\(\\texttt{lean\\_mass}\\) with values of \\(\\texttt{vo2}\\) from the same individual. Or, in models with crossed factor variables, for example, an experiment with four groups repesenting all four combinations of the variables \\(\\texttt{genotype}\\) (with levels “WT” and “KO”) and \\(\\texttt{treatment}\\) (with levels “Control” and “HFD”), there is no non-awkward way (at least in a table with a single header row) to indicate that the design is two crossed factors. In wide format, the design appears to be a single factor with four groups.\nDon’t confuse a data table with multiple columns of different variables with a data table in wide format. Long and wide are relevant to how a single variable is organized – if split into multiple columns the table is wide, if stacked into a single column, the table is long.\nOne exception to long format is a longitudinal or a repeated measures data set (Chapter @ref(pre-post)), in which the response variable is measured multiple times on each individual. An example might be a pre-post design where a variable is measured both before and after a treatment is applied or an experiment where the response is measured each week for multiple weeks. In univariate models for the analyses of these data, the multiple measures for an individual are stacked into long format. As a consequence, there are now multiple rows of data for each individual (one for each measure of the variable). In multivariate models for the analyses of these data, the multiple measures for an individual are treated as if they are different variables and are spread out into wide format.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Data -- Reading, Wrangling, and Writing</span>"
    ]
  },
  {
    "objectID": "chapters/reading-writing-wrangling.html#data-long",
    "href": "chapters/reading-writing-wrangling.html#data-long",
    "title": "3  Data – Reading, Wrangling, and Writing",
    "section": "",
    "text": "The data table contains all measures on each individual. In most of the archived Excel files that I download from journal websites, the researchers have separated each variable into separate tables and exclude all information from these tables other than the treatment group. For example, researchers would typically archive the vo2 values in Table @ref(tab:data-diet-table) in a separate table within an Excel sheet and with the values split by diet group (high fat “HF” and low fat “LF”)(Figure @ref(fig:data-vo2-table-wide)). Don’t do this.\n\n\n\nWe don’t want to separate each variable into separate tables because we often want to use information from other measures of the same individual in our analysis. For example, for the comparison of \\(\\texttt{vo2}\\) among treatment levels, we would want to add the measure of \\(\\texttt{lean\\_mass}\\) to the model as a covariate (what is often called an ANCOVA model). To do this, we’d need to know which values of \\(\\texttt{vo2}\\) and values of \\(\\texttt{lean\\_mass}\\) belong to the same individual.\nEven if you think you have analyzed the data using a best practice, don’t separate each variable into separate tables. Researchers sometimes fail to recognize best practices. Any re-analysis of published data using best practices often require information from other measures of the same individual. For example, many researchers would compare the ratio of \\(\\texttt{vo2}\\) to \\(\\texttt{total\\_mass}\\) among treatment levels. This isn’t a best practice and if I want to check the researchers conclusions, I’d need a table like Table @ref(tab:data-diet-table) to re-analyze the data.\nEven if you have used what is considered a best practice, don’t separate each variable into separate tables. Sometimes a researcher wants to use data from published data sets to parameterize a mechanistic model. Or, use data from published data sets to explore patterns of relationships among variables, for example how fat mass scales with lean mass (or total mass). For these, we need a table like Table @ref(tab:data-diet-table) to re-analyze the data.\n\n\nThe researchers organized the table with individuals in rows. A data table with all measures for each individual could organize the data with each row or each column belonging to one individual. The best practice for most analysis in R (and most other statistical software) is to organize individuals to rows (there are exceptions in some R packages that analyze genomics datasets).\nThe researchers organized the data in long format – that is, each column contains all measures of a single variable. In general, do not organize the data in wide format, by splitting the values of a variable into multiple columns, with each column representing the values for a different group. An example of wide format is in Figure @ref(fig:data-vo2-table-wide), which is the \\(\\texttt{vo2}\\) subset of data split by the groups in the columne \\(\\texttt{diet}\\). By contrast, the \\(\\texttt{vo2}\\) data are in long format in the original table (Table @ref(tab:data-diet-table)). It might be useful to think of long format for a variable as a single column with the values for each group stacked on top of each other.\n\n\n\nA reason for this is because the long format is consistent with the math underneath the statistical modeling of data. For example, the least squares solution of the linear model is\n\n\n\n\n\n\nThe table includes the raw values. Many researchers normalize or standarize the values of a variable by constructing a ratio of the raw value divided by some standardizing value. Almost always, the best practice is to leave the response variable alone (do not make a ratio) and include the standardizing value as a covariate (or offset) in the model. But, if I want to check th conclusions of a particular result in which the researchers used a ratio as the response variable, then I would need the raw measures and the standardizing values.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Data -- Reading, Wrangling, and Writing</span>"
    ]
  },
  {
    "objectID": "chapters/reading-writing-wrangling.html#data-here",
    "href": "chapters/reading-writing-wrangling.html#data-here",
    "title": "3  Data – Reading, Wrangling, and Writing",
    "section": "3.2 Use the here function to construct the file path",
    "text": "3.2 Use the here function to construct the file path\nImporting data into R can be a struggle for new R users and, unfortunately, most online “how to import” sources give easy but superficial methods that don’t follow best practices for increasing reproducibility or do not allow flexible organization of files within a project.\n(TL;DR – use here() from the here package)\n\ndf &lt;- read.table(file=\"clipboard\") imports data copied to the clipboard, from an Excel/Sheets file or from an open text file. For this to be semi-reproducible, a comment specifying the filename, worksheet and range that was copied is necessary. More problematic (catastrophically so for reproducibility), is, how does a researcher know that they highlighted and copied the correct range in the Excel sheet?\ndf &lt;- read.csv(file.choose()) opens the familiar “open file” dialog box, which lets the user navigate to the file of choice. For this to be semi-reproducible, a comment specifying the filename to import is necessary. The catastrophic problem (for reproducibility) is, how does a researcher know which file was actually opened during an R session? The researcher might think they opened “walker_maine_bee_data_clean_androscoggin.csv” but mistakenly opened “walker_maine_bee_data_clean_aroostook.csv”.\ndf &lt;- read.table(file=\"my_data.txt\") and df &lt;- read_excel(file=\"my_data.xlsx\") are reproducible because the filename is explicitly specified. But, this method requires that “my_data” is physically located in the same folder as the file containing the R script (the .Rmd file in our case) and this violates the best practice of clean project organization with different folders for the different kinds of files (data, R scripts, images, manuscript text, etc.).\nR Studio has an elegant import tool in the environment pane that opens a custom dialog box that allows the researcher to navigate to the file, and to specify what part of the file to import, such as the specific sheet and range for an Excel file. This has the same reproducibility issues as #1 and #2 but R Studio includes the equivalent script, which adds all relevant information for reproducility. One then simply copies and pastes this script into a code chunk and voila! The next time the script is run, the data can be imported from the script without using menus and dialog boxes. Except that..the script does not seem to take into account that the working directory of an R Markdown file is not the project folder but the folder containing the R Markdown file and so this two-step method fails. More personally, I’d prefer to run a chunk that quickly opens the data file instead of re-navigating through my file system and re-specifying the sheet and range every time I re-start the project in a new R session.\n\nThere are at least three solutions to the issues raised above, all requiring some understanding of file paths and directory structure in an operating system. A file such as “my_data.xlsx” has an absolute file path, which is the full address of the file (the filename is something like your house street number). The absolute file path of “my_data.xlsx” might be “/Users/jwalker/Documents/applied-biostatistics/data/my_data.xlsx”. A relative file path is the file path from the working directory. In an R Studio project, the working directory is the project directory, which is the directory containing the .Rproj file. This will be the working directory of the console. Importantly, the working directory of an R Markdown code chunk is the folder containing the saved R Markdown file. An R Studio Notebook is an R Markdown file so the working directory of a notebook code chunk is the folder containing the saved notebook file. If an R Markdown file is located within the rmd folder, which is located within the project folder, then the relative file path to “my_file.xlsx” is “../data/my_file.xlsx”. The “..” tells the file OS to move “up” into the parent directory (which is the project folder) and the “data” tells the file OS to move “down” into the data folder. These are put together into a single address using “/”. The beauty of relative paths is that they remain the same – and so do not break one’s script – if the project folder, and all of its contents including the data folder and the rmd folder, is moved to another location on the hard drive (say into a new “Research” folder). By contrast, the absolute file path changes, which breaks any old script.\nThe three solutions are\n\nCreate a relative path to the file using something like file_path &lt;- \"../data/my_data.xlsx\". This should always work but it fails on some computers. For example, if the project folder is on a Windows OS (but not Mac OS) desktop, the assigned relative address doesn’t seem to look in the folder containing the file.\nCreate a setup chunk that reroutes the working directory to the project folder using the script\n\n\n# use this in a chuck called \"setup\" to force the working directory to be\n# at the level of the project file.\nknitr::opts_knit$set(root.dir = rprojroot::find_rstudio_root_file())\n\nFor this to work, the chunk has to be named “setup”, that is, the text inside the curly brackets at the top of the chunk should be “r setup”. Then, with this chunk, the relative file path is file_path &lt;- \"../data/my_data.xlsx\" if “my_data.xlsx” is immediately inside the data folder which is immediately inside the project folder. This should work on any machine, and should work even if a project folder is moved.\n\nUse the function here(). The most robust solution seems to be using the function here() from the here package. The function works something like this\n\n\ndata_folder &lt;- \"data\" # path to data that are imported\nfile_name &lt;- \"my_data.xlsx\"\nfile_path &lt;- here(data_folder, file_name) # paste together parts of the address\nmy_file &lt;- read_excel(file = file_path)\n\nhere() creates an absolute path, but one that is created on the fly, and will change (or should change) correctly if the project folder is moved on the same machine, to a cloud drive, or to another machine altogether.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Data -- Reading, Wrangling, and Writing</span>"
    ]
  },
  {
    "objectID": "chapters/reading-writing-wrangling.html#learning-from-this-chapter",
    "href": "chapters/reading-writing-wrangling.html#learning-from-this-chapter",
    "title": "3  Data – Reading, Wrangling, and Writing",
    "section": "3.3 Learning from this chapter",
    "text": "3.3 Learning from this chapter\nIt will be easiest to learn from this chapter by starting with a clean R Markdown document for this chapter. Create a new R Markdown file and save it to the “Rmd” folder of your project. To create and setup the Rmd, follow the steps in Create and setup an R Markdown document (Rmd)\nImportant: The import method given here will not work properly until the Rmd file is saved! Get in the habit of creating the file, saving it immediately, and saving it often.\nImportant: The import method will not work properly unless you are working within your R studio project. If you don’t see the name of the project in the upper right of the R Studio toolbar, then you are not in the project. Open the project before moving forward. A good habit is to open R studio by double clicking the project file icon (and not the R Studio icon or the Rmd document that you are working on).\nNotes on the setup chunk from Create and setup an R Markdown document (Rmd)\n\nAs you become a better R programmer, be kind to the future you by loading only the packages necessary for the code in the R Markdown file that you are working on. If your default is to load everything, the future you will be confused why something was installed. Again, we’ve disregarded this best practice in this textbook to make sure that chunks don’t fail because of a missing function.\nBe kind to the future you by commenting on why a package is loaded; usually this is a specific function from the package\nhere &lt;- here::here is my favorite script ever. What is it doing? One can read this as “assign the function here from the here package to the object here” (this is reading the script right to left). Why do this? It turns out the multiple packages define a function called “here”. If any of these packages are loaded after the here package, then here from here won’t work – it will be replaced by here from the more recently loaded package. To make sure that here uses the function from the here package, I simply reassign here from the here package to the object “here” after loading in all packages.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Data -- Reading, Wrangling, and Writing</span>"
    ]
  },
  {
    "objectID": "chapters/reading-writing-wrangling.html#data-working-in-r",
    "href": "chapters/reading-writing-wrangling.html#data-working-in-r",
    "title": "3  Data – Reading, Wrangling, and Writing",
    "section": "3.4 Working in R",
    "text": "3.4 Working in R\n\n3.4.1 Importing data\n\n3.4.1.1 Importing Excel files\nSource article: Lyu, Yang, et al. “Drosophila serotonin 2A receptor signaling coordinates central metabolic processes to modulate aging in response to nutrient choice.” Elife 10 (2021): e59399.\nfile name: elife-59399-fig1-data1-v2.xlsx\nLet’s open the data for Fig. 1H.\nSteps\n\nHighlight and copy the title of the article, which is “Drosophila serotonin 2A receptor signaling coordinates central metabolic processes to modulate aging in response to nutrient choice”\nCreate a new folder within the “data” folder. Name the folder the title of the paper by pasting the name from the clipboard. This is the “data from” folder, since it contains the data from the publication with the title of the folder.\nDownload the .xlsx file and move it into this folder\n\nThe code below uses the function read_excel() from the package readxl. More about the amazing power of this package is the tidyverse page and chapter 11 in the R for Data Science book.\n\ndata_from &lt;- \"Drosophila serotonin 2A receptor signaling coordinates central metabolic processes to modulate aging in response to nutrient choice\"\nfile_name &lt;- \"elife-59399-fig1-data1-v2.xlsx\"\nfile_path &lt;- here(data_folder, data_from, file_name)\n\nfig1h &lt;- read_excel(file_path, \n                    sheet = \"H\") |&gt; # pipe the result to next line\n  clean_names() |&gt; # clean column names and pipe the result to next line\n  data.table() # convert to data.table\n\n# View(fig1h)\n\nThis book will consistently uses this protocol for importing downloaded Excel files. For any tidy archived data in Excel, a user would simply copy this chunk, paste it into their Rmd, and modify the stuff inside the quotes to import the data. But this is mindless programming. Don’t do just this. Understand what each line in the chunk does.\n\n3.4.1.1.1 View the data\nThe very first thing that you should do after importing data, without exception, is to View the imported data. The last line in the code chunk above is a reminder of this, and one of several ways to View the data.\n\n# View(fig1h)\n\nThe comment (or hashtag) symbol (#) tells R to ignore everything in the line following the symbol. Usually a comment symbol is followed by a brief comment on the code and is mostly for the future you – when you come back after a six month hiatus and wonder what a line or block of code is doing, the comment gives you this information. Sometimes a comment is used in front of R code to keep the R code from running. That is the purpose of the comment symbol here.\nView(fig1h) tells R to open a window showing a spreadsheet view of the data.table fig1h. In R studio, this window is opened as a new tab in the Source editor pane (the pane with your Rmd content). To run this code, highlight “View(fig1h)” but not the comment symbol and click the “Run selected line(s)” item in the Run pop-up menu (right side of pane’s tool bar). We don’t include the comment sign in the highlight because this would tell R to ignore everything after it.\nWe can View the data in fig1h more easily by holding the command key (Mac OS) or the control key (Windows) and clicking the mouse in “fig1h” in any of the lines. Even though I use the mouse click method, I like to have the View(fig1h) at the end of my import chunk to remind me to View the data to make sure everthing imported correctly.\nA third, and probably easiest way to View the data is to click on the name of the object in the Enviroment pain (typically upper right pane in the default R Studio layout).\n\n\n3.4.1.1.2 What does each line in the chunk do?\nThe first three lines in the chunk create the directory path to the file. This path includes three variables\n\ndata_folder – assigned to “data” in the setup chunk. “data” is a folder within the project folder that contains (or will contain) all datasets for this text. The data come from many different published papers, so the data for each publication gets its own folder within “data”.\ndata_from – the name of the “data from” folder within “data” containing the data files. In this text, these folder names will always be the name of the published paper.\nfilename – the name of the file to read. There may be multiple data files within the publication’s data folder.\n\nThese are all put together into the absolute path using the function here() from the here package. Take a look at the value of file_path to confirm.\nThe next four numbered lines (starting with fig1h &lt;- read_excel) is composed of three functions that are linked together by the pipe operator |&gt;. Separately, the code would look like this\n\nfig1h &lt;- read_excel(file_path, \n                    sheet = \"H\")\nfig1h &lt;- clean_names(fig1h) # clean column names\nfig1h &lt;- data.table(fig1h) # convert to data.table\n\n\nread_excel imports the data and names it “fig1h”. More technically, the code assigns the imported object to a data.frame object named fig1h. Throughout, this text uses the “figXXX” convention for imported data. Examples online often use conventions like “dat” or “my_data” but these are not very meaningful names in the sense that the name doesn’t convey any information on what the data is.\nclean_names cleans the column names of fig1h. This is explained more below.\ndata.table converts fig1h to a data.table object. A data.table is a data.frame with magical properties. Everyone should know data.table.\n\nAgain, here is the version with the three separate functions piped together with the pipe operator |&gt;\n\nfig1h &lt;- read_excel(file_path, \n                    sheet = \"H\") |&gt; # pipe the result to next line\n  clean_names() |&gt; # clean column names and pipe the result to next line\n  data.table() # convert to data.table\n\nThe pipe operator takes output from one function and sends this to another function. The output from the last function is assigned the name fig1h. Note that we don’t put “fig1h” in the parentheses of the clean_names() and data.table() functions – the pipe operator does this for us.\nA 3rd way to do write these three functions is to nest them like this\n\nfig1h &lt;- data.table(clean_names(read_excel(file_path,\n                    sheet = \"H\"))) # convert to data.table\n\nPrior to the creation of the pipe operator, I nested functions to make code more compact. However, nested functions are often less readable, and I now mostly avoid these and use the pipe operator instead.\n\n\n\n3.4.1.2 More on clean_names()\nLet’s back up to understand the steps, and especially the clean_names step. Here is the first line, which is the line that imports the file.\n\nfig1h &lt;- read_excel(file_path,\n                    sheet = \"H\")\n\nLook at the column names (or column headers in Excel lingo) of the imported data using names or colnames (yes, there are elebenty million ways to do anything in R) (names is very general in that it can be used to return the names of the parts of any list, while colnames is specific to matrix-like objects). Type this into the console, not into your R Markdown chunk:\n\nnames(fig1h)\n\n[1] \"Diet\"             \"Replicate\"        \"Weight (mg/fly)\"  \"TAG (ug/fly)\"    \n[5] \"Protein (ug/fly)\" \"TAG/Protein\"     \n\n\nIn general, it is bad practice to include spaces, parentheses, and special characters such as /, -, $, or ^, in the column names of a data frame because these symbols have specific meanings in R functions. The best practice for spaces is to replace the space with an underscore, for example the column header ““Weight (mg/fly)” could be weight_mg_per_fly. Some coders separate words with a period (weight.mg.per.fly). Others mash words together into a single word like this weightmgperfly but this should generally be avoided because the result can be hard to read. Finally, some coders use Caps to separate words like this WeightMgPerFly. This is easier to read than simple concatenation but the underscore is the easiest to read.\nclean_names from the janitor package is a beautiful function to clean the column names of a data frame including replacing spaces with an underscore and stripping parentheses. The default format is snake_case, which replaces all spaces with underscores and changes any uppercase letter with lowercase. Many coders like to work with all lowercase variable names to avoid having to hit the shift key. I am one of these.\nNow run the line and look at the column names.\n\nfig1h &lt;- clean_names(fig1h)\nnames(fig1h)\n\n[1] \"diet\"           \"replicate\"      \"weight_mg_fly\"  \"tag_ug_fly\"    \n[5] \"protein_ug_fly\" \"tag_protein\"   \n\n\nFinally, run the line\n\nfig1h &lt;- data.table(fig1h)\n\nYou won’t see any difference if you View the data or peak at the column names. But you’ve transformed the data frame to a data.table and given it magical properties.\n\nWorst Practices – resist the temptation to change the column names in the data file, which reduces reproducibility. Leave original data files original. Always increase reproducibility!\n\n\ncolleague blues – Most researchers live in an Excel world and save data in a way that is efficient for computing stuff in Excel but not efficient for statistical analysis using R or other statistical computing software packages (with the exception of Graphpad Prism). Analyzing data will be much less frustrating if the data are saved in a format that facilitates analysis. Best practices for creating data files\n\nhttps://www.youtube.com/watch?time_continue=309&v=Ry2xjTBtNFE – An excellent video introduction to best practices for organizing data in a spreadsheet that will subsequently be analyzed by statistics software.\nBroman, K. W., & Woo, K. H. (2017). Data organization in spreadsheets (No. e3183v1). https://doi.org/10.7287/peerj.preprints.3183v1 – An excelllent review of best practices for organizing data in a spreadsheet.\n\n\n\n\n3.4.1.3 More on read_excel()\nread_excel is a beautifully flexible function because Excel. Data can be in different sheets and there can be different datasets within a single sheet. And, researchers tend to use Excel like a blackboard in that an Excel sheet often contains calculations such as means, standard deviations and t-tests. When using read_excel it is important to send the function enough information to read the correct data. For the fig1h data, if we simply used\n\nfig1h &lt;- read_excel(file_path) |&gt;\n  clean_names() |&gt;\n  data.table()\n\nwithout specifying the sheet, read_excel defaults to reading the first sheet (“A”), which is not what we wanted.\nWe can specify the exact range to important using the range = argument\n\nfig1h &lt;- read_excel(file_path,\n                    sheet = \"H\",\n                    range = \"A1:F49\") |&gt;\n  clean_names() |&gt;\n  data.table()\n\nThis isn’t necessary for these data because the “H” sheet contains only a matrix of data and not extraneous information and the read_excel function is smart enough to figure this out. For many of the data sets in wet bench experimental biology, the range argument will be crucial because multiple datasets are archived on a single sheet.\nWe can also specify other arguments. For example, the following code explicitly tells read_excel to use the first row of the data range to be used as the column names and not the first row of data. TRUE is the default argument, so this isn’t necessary for the fig1h data but if the data we want to import doesn’t have column names (and this can be common in archived data sets) then we need to pass the argument “col_names = FALSE”.\n\nfig1h &lt;- read_excel(file_path,\n                    sheet = \"H\",\n                    range = \"A1:F49\",\n                    col_names = TRUE) |&gt;\n  clean_names() |&gt;\n  data.table()\n\n\n3.4.1.3.1 A quick plot of the fig1h data\nJust for fun, let’s use the ggboxplot function from the ggpubr package to reproduce something close to Fig. 1H. We could make a plot that looks exactly like Fig. 1H but this would require some of the wrangling outlined below.\n\nggboxplot(data = fig1h,\n          x = \"diet\",\n          y = \"tag_protein\",\n          ylab = \"TAG/Protein\",\n          xlab = \"\",\n          add = \"jitter\")\n\n\n\n\n\n\n\n\nNotes on the code to make this plot\n\nthe names of the columns to use as the x and y axes were passed inside quotes. In some R functions, column names are passed inside quotes and in other R functions, the column names are passed as-is. There is little rhyme or reason for this.\n\n\n\n\n3.4.1.4 Importing text files\nSource article: Corrigan, June K., et al. “A big-data approach to understanding metabolic rate and response to obesity in laboratory mice.” Elife 9 (2020): e53560..\nfile name: mmpc_all_phases.csv\nLet’s open the data for Fig. 1.\nSteps\n\nCopy the title of the paper title, which is “A big-data approach to understanding metabolic rate and response to obesity in laboratory mice”\nCreate a new folder within “data”. Name the folder the title of the paper by pasting from the clipboard. This is the “data from” folder, since it contains the data from the publication with the title of the folder.\nClick on the link to the file above, highlight all values in the window by clicking inside the window and typing command-a (Mac os) or control-a (Windows) and copy.\nPaste into a an open, blank text document and save it as “mmpc_all_phases.csv”. If saved elsewhere, move this file into the “A big-data approach to understanding metabolic rate and response to obesity in laboratory mice” folder within the “data” folder.\nIf you don’t have a dedicated text editor such as BBEdit on MacOS, then open a new R (not Rmd!) document in R studio. This will open as a new tab in the Script Editor pane. Paste the data into the R document window. Save it as “mmpc_all_phases.csv”. R Studio will ask to be sure you want the extension to be .csv and not .R. Yes you are sure. If saved elsewhere, move this file into the “A big-data approach to understanding metabolic rate and response to obesity in laboratory mice” folder within the “data” folder.\n\nA .csv file is a comma-delimited text file, which means that the entries of a row are separated by commas. A text file is readable by any text editor software and most other kinds of software. Datasets that are stored as text files are typically saved as either .csv (where the entries of a row are separated by commas) or .txt (where the entries are separated by tabs). The base R way to read a .csv file is using read.csv. The read.table function is more versatile, as the delimiter can be specified. The function fread() from the data.table package is fast, smart, and flexible. It is smart in the sense that it guesses what the delimter is.\n\n# construct file path\ndata_from &lt;- \"A big-data approach to understanding metabolic rate and response to obesity in laboratory mice\"\nfile_name &lt;- \"mmpc_all_phases.csv\"\nfile_path &lt;- here(data_folder, data_from, file_name)\n\n# open file and assign to fig1, then pipe to clean_names\n# to change column names to snake_case\nfig1 &lt;- fread(file_path) |&gt;\n  clean_names()\n\nBe sure to read the section on importing Excel files above to understand this code. Here, as with the import of the Excel file, the first three lines create the directory path to the file. There is no need to pipe fig1 to the data.table function because fread automatically opens the data as a data.table. As with the imported Excel file above, this code sends the data object to clean_names to change the column labels to snake_case.\n\n3.4.1.4.1 A quick plot of the fig1 data\nLet’s use the ggscatter function from the ggpubr package to reproduce something close to Fig. 1D. We could make a plot that looks exactly like Fig. 1D but this would require some additional plot arguments.\n\nggscatter(data = fig1[diet == \"HF\" &\n                    acclimation == TRUE],\n          x = \"total_mass\",\n          y = \"ee\",\n          color = \"institution\",\n          palette = pal_okabe_ito_4,\n          add = \"reg.line\",\n          ylab = \"energy expenditure (kcal/hr)\",\n          xlab = \"body mass (g)\")\n\n\n\n\n\n\n\n\n\n\n\n3.4.1.5 Troubleshooting file import\nIf you get an error that starts with “Error: path does not exist:” then R is not “seeing” your specified file given the path you’ve given it.\n\nMake sure that you are working within the project by checking the project name on the right side of the R Studio tool bar. If the project is “None” or you are working within a different project, then open the new project.\nMake sure you loaded the package here in a “setup” chunk and that you have run the setup chunk\nMake sure you have assigned data_folder &lt;- \"data\" in the setup chunk and have run the chunk.\nMake sure your “data” folder is one level inside your project folder. “one level” means it is not buried deeper inside other folders within the project folder.\nMake sure your “data from” folder (the folder with the title of the publication) is one level inside your “data” folder\nMake sure your data file is one level inside the correct “data from” folder.\nBug alert Make sure you have the name of the “data from …” folder correct in your script. Do not type the name of the folder. Instead, go to the finder and highlight the folder containing the data file, copy the name, return to the R markdown script, type folder &lt;- \"\" and then paste the clipboard (the name of the folder) in between the quote marks.\nBug alert Make sure the file name is correct in the script. As with the folder name, I go to the finder and copy the file name and paste it in place. In Windows use ctrl-a instead of ctrl-c to copy the full filename including the extension.\n\nMore generally, Humans are very good at understanding misspelld and OdDLy capitalized words but the R language (or any computer language) is very literal. R is case sensitive (some programming languages are not). “Prenatal acoustic communication”, “Prenatal Acoustic Communication”, and “prenatal acoustic communication” are all different values. Spelling AND capitalization have to be perfect, not simply close. Spelling includes spaces. A frequent bug is a file name typed as “Prenatal acoustic communication” when the actual name is “Prenatal acoustic communication”. Can you spot the bug? The original (what we need to copy) has two spaces between “acoustic” and “communication” while the incorrect copy has only one.\nSpelling bugs are avoided by simply copying and pasting names of folders, names of files, column names of data frames, and level names of factors, which leads to a general rule of R scripting…\n\n\n3.4.1.6 Rule number one in R scripting {# rule1}\nAlways copy and paste any text that will be inserted into quotes\nDo not try to type it out. You have been warned.\n\n\n\n3.4.2 Data wrangling\nData archived in Excel spreadsheets, at least in wet-bench experimental biology projects, are generally not in a format this is readily analyzed in R, or any statistical software other than perhaps Graphpad Prism. Use these examples as templates for how to import and wrangle Excel-archived data in your project.\n\n3.4.2.1 Reshaping data – Wide to long\n\n3.4.2.1.1 Wide to long – Adipsin data\nSource: Adipsin preserves beta cells in diabetic mice and associates with protection from type 2 diabetes in humans\nPublic source – the Adipsin paper is behind a paywall. A public source of the paper from NIH is available.\nLink to source data\n\n\n\n\n\n\n\n\n\nFig. 1k of the Adipsin paper presents a bar plot of the glucose uptake in response to control (GFP) or adipsin treatment. A screenshot of the Excel-archived data is shown above. The data are in wide format. In wide-format, the values of a single variable (here, this is glucose uptake level) are given in separate columns for each treatment level (group). The values for the GFP group are in Column A and the values for the Adipsin group are in Column B. Wide format is efficient for computations in a spreadsheet, such as computing means and standard deviations of columns of data, and for plotting.\nFor most statistical analyses of experimental data in R (and most statistics software), all values of a single variable should be in a single column. This is called long format. I’ve manually rearranged the data from the archived spread sheet into long format by stacking each group’s values into a single column, shown in the screen capture below. All values of glucose uptake are in a single column. In long format, there needs to be a way to identify which values belong to which group and this is achieved here with column “treatment”. In adition to the treatment column.\n\n\n\n\n\n\n\n\n\nThe difference between wide and long also reflects how we think about statistical analysis. When we do a t-test to compare the means of glucose uptake between GFP and Adipsin groups, we might think we have two things: the set of glucose uptake values for the GFP group and the set of values for the Adipsin group. When we fit a linear model, we also have two things, the variable treatment containing treatment level assignment and the variable glucose_uptake containing the glucose uptake values. In wide format, there is nothing to suggest that treatment is a variable.\nThere are many functions to tidy data from wide to long. melt from the data.table package is especially useful. It is data.table’s version of melt from the reshape2 package.\nThe major arguments of data.table::melt are\nmelt(data, id.vars, measure.vars, variable.name, value.name)\nmelt takes the data in the columns listed in measure.vars and stacks these into a single column named value.name. The names of the columns in measure.vars are the values of the elements in a new column named variable.name. The elements of any column in id.vars are repeated p times, where p is the number of columns that were stacked.\nLet’s melt the three different response variables of the adipsin data and merge them into a single data.table. There are several ways to combine data sets including merge and cbind. We’ll compare these later.\n\nfile_folder &lt;- \"Adipsin preserves beta cells in diabetic mice and associates with protection from type 2 diabetes in humans\"\nfn &lt;- \"41591_2019_610_MOESM3_ESM.xlsx\"\nfile_path &lt;- here(data_folder, file_folder, fn)\n\ntreatment_levels &lt;- c(\"db/db-GFP\", \"db/db-Adipsin\")\n\n# as separate line\nfig_1k_wide &lt;- read_excel(file_path,\n                    sheet = \"Figure 1k\",\n                    range = \"A3:B9\")\nfig_1k_wide &lt;- data.table(fig_1k_wide)\nfig_1k &lt;- melt(fig_1k_wide,\n               measure.vars = treatment_levels,\n               variable.name = \"treatment\",\n               value.name = \"glucose_uptake\")\n\n# or piped -- which do you prefer?\nfig_1k &lt;- read_excel(file_path,\n                    sheet = \"Figure 1k\",\n                    range = \"A3:B9\") |&gt;\n  data.table() |&gt;\n  melt(measure.vars = treatment_levels,\n       variable.name = \"treatment\",\n       value.name = \"glucose_uptake\")\n\n# View(fig_1k) # highlight without the comment sign and \"run selected lines()\" to view\n\nA pretty-good-plot using the ggpubr package\n\n# put warning=FALSE into the chunk header to supress the warning\n\ngg &lt;- ggstripchart(x = \"treatment\",\n                   y = \"glucose_uptake\",\n                   add = \"mean_se\",\n                   data = fig_1k)\n\ngg\n\n\n\n\n\n\n\n\n\n\n3.4.2.1.2 Wide to long – Enteric nervous system data\nSource: Rolig, A. S., Mittge, E. K., Ganz, J., Troll, J. V., Melancon, E., Wiles, T. J., … Guillemin, K. (2017). The enteric nervous system promotes intestinal health by constraining microbiota composition. PLOS Biology, 15(2), e2000689.\nSource data\nLet’s import and reshape the data for figure 2d. Look at the excel file and the data in Fig. 2d. There is a single treament with four levels, but the authors have organized the data in each level in separate columns and used the column header as the level name.\n\n\n\n\n\n\n\n\n\nLet’s melt the data from wide to long by stacking the four columns into a single column “neutrophil_count” and adding a treatment column identifying the group.\n\nfolder &lt;- \"The enteric nervous system promotes intestinal health by constraining microbiota composition\"\nfilename &lt;- \"journal.pbio.2000689.s008.xlsx\"\nfile_path &lt;- here(data_folder, folder, filename)\n\n# figure 2D data\nsheet_i &lt;- \"Figure 2\"\nrange_i &lt;- \"F2:I24\"\nfig_2d_wide &lt;- read_excel(file_path, sheet=sheet_i, range=range_i) |&gt;\n  clean_names() |&gt;\n  data.table()\n\n# change column names by replacing without \"_donor\" in each name\n# these new column names will become the levels of the treatment factor\nnew_colnames &lt;- c(\"gf\", \"wt\", \"sox10\", \"iap_mo\")\nsetnames(fig_2d_wide, old=colnames(fig_2d_wide), new=new_colnames)\n\n# wide to long\nfig_2d &lt;- melt(fig_2d_wide, \n              measure.vars=colnames(fig_2d_wide), \n              variable.name=\"treatment\", \n              value.name=\"neutrophil_count\")\n\n# omit empty rows\nfig_2d &lt;- na.omit(fig_2d)\n\n# re-order factors\nfig_2d[, treatment := factor(treatment,\n                         levels = c(\"wt\", \"gf\", \"sox10\", \"iap_mo\"))]\n\n# View(fig_2d)\n\nTo learn (instead of just copy and modify), it’s best to do this in steps and not run the whole chunk. At each step, look at the result using View. The script above includes three extra wrangling steps.\n\nChanging column names in fig_2d_wide. The column names in wide format will become the treatment level names of the treatment factor after reshaping. It will be easier down the road if these names are shorter and the “_donor” in each name is redundant. The setnames function renames the column names.\nFor these data, the number of measures within the different treatments differs and, as a consequence, there are multiple cells with NA which indicates a missing value. View(fig_2d_wide) (this can be typed in the console) to see this. After reshaping to long format (fig_2d), the rows with missing values become empty rows – there is no useful information in them (View this). To see this, re-run the lines of the chunk up to the line “# omit empty rows”. The na.omit function deletes any row with missing values. Here, this deletes these information-less rows. Be very careful with na.omit. You do not want to delete rows of data that contain information you want.\nFor both analysis and plots, we want to compare values to the control level, which is named “wt” for the fig_2d data. That is, we want “wt” to be the reference level. To achieve this, the levels of the factor treatment need to be re-ordered using the levels argument. (note, I typically do not add “levels =”, but simply pass the list of levels)\n\n\n\n3.4.2.1.3 Wide to long – bee data\nThe example above is pretty easy, because the all columns in the original data frame are melted (stacked). Here is an example in which only a subset of columns are stacked. In addition, only a subset of the remaining columns are retained in the long format data frame. The data are from Panel A of supplement Fig. 8 (https://journals.plos.org/plosbiology/article/file?type=supplementary&id=info:doi/10.1371/journal.pbio.2003467.s019){target=“_blank”} from\nSource: Kešnerová, L., Mars, R.A., Ellegaard, K.M., Troilo, M., Sauer, U. and Engel, P., 2017. Disentangling metabolic functions of bacteria in the honey bee gut. PLoS biology, 15(12), p.e2003467.\nSource data\n\nfolder &lt;- \"Disentangling metabolic functions of bacteria in the honey bee gut\"\nfilename &lt;- \"journal.pbio.2003467.s001.xlsx\"\n\n# figure 2D data\nsheet_i &lt;- \"S8 Fig\"\nrange_i &lt;- \"A2:H12\"\nfile_path &lt;- here(data_folder, folder, filename)\nfig_s8a_wide &lt;- read_excel(file_path,\n                      sheet=sheet_i,\n                      range=range_i) |&gt;\n  clean_names() |&gt;\n  data.table()\n\n# wide to long\nstack_cols &lt;- paste0(\"replicate\", 1:5)\nfig_s8a &lt;- melt(fig_s8a_wide,\n              id.vars = c(\"media\", \"time_h\"),\n              measure.vars = stack_cols, \n              variable.name = \"Replicate\", \n              value.name = \"OD600\") # measure of absorbance at 600nm\n\n\n\n3.4.2.1.4 Wide to long – stacking multiple sets of columns\nArticle: GPR109A mediates the effects of hippuric acid on regulating osteoclastogenesis and bone resorption in mice\nsource data\nThe data are from Fig. 1b\n\nfolder &lt;- \"GPR109A mediates the effects of hippuric acid on regulating osteoclastogenesis and bone resorption in mice\"\nfile_name &lt;- \"42003_2020_1564_MOESM4_ESM.xlsx\"\nfile_path &lt;- here(data_folder, folder, file_name)\n\nfig1b_wide &lt;- read_excel(file_path,\n                    sheet = \"Fig. 1b\",\n                    range = \"C4:R9\",\n                    col_names = FALSE) |&gt;\n  data.table()\n\ntreatment_levels &lt;- c(\"Wild type\", \"GPR109A-/-\")\ntreatment_abbrev &lt;- c(\"wt\", \"ko\")\nmeasures &lt;- c(\"BV/TV\", \"Tb.Th\", \"Tb.Sp\", \"Tb.N\", \"BMD\", \"Cs.Th\", \"BV\", \"MA\")\nnew_colnames &lt;- paste(rep(measures, each = 2),\n                              treatment_abbrev,\n                              sep = \"_\")\nsetnames(fig1b_wide,\n         old = colnames(fig1b_wide),\n         new = new_colnames)\n\n\n\n\n\n\nBV/TV_wt\nBV/TV_ko\nTb.Th_wt\nTb.Th_ko\nTb.Sp_wt\nTb.Sp_ko\nTb.N_wt\nTb.N_ko\nBMD_wt\nBMD_ko\nCs.Th_wt\nCs.Th_ko\nBV_wt\nBV_ko\nMA_wt\nMA_ko\n\n\n\n\n5.229493\n6.697880\n0.0379685\n0.0436415\n0.3383573\n0.3507201\n1.377323\n1.534749\n0.0927138\n0.1125594\n0.1363595\n0.1445718\n0.2296399\n0.2250816\n0.3832690\n0.4268379\n\n\n5.171787\n7.271222\n0.0386800\n0.0425357\n0.3579566\n0.3946835\n1.337071\n1.709440\n0.0930417\n0.1129723\n0.1387652\n0.1430709\n0.1946119\n0.2422370\n0.4216582\n0.3553190\n\n\n4.287585\n9.873900\n0.0385307\n0.0438999\n0.4401164\n0.3019478\n1.112770\n2.249185\n0.0782485\n0.1330846\n0.1334085\n0.1588262\n0.1568608\n0.3700646\n0.3620063\n0.4698913\n\n\n4.308528\n5.557610\n0.0374626\n0.0372947\n0.3576182\n0.3328132\n1.150088\n1.490186\n0.0841214\n0.1007773\n0.1369047\n0.1288561\n0.1555844\n0.2050777\n0.3661425\n0.4413361\n\n\n5.175202\n6.673029\n0.0406120\n0.0400175\n0.4541035\n0.3604716\n1.274303\n1.667530\n0.0921535\n0.1189364\n0.1299124\n0.1438411\n0.1521829\n0.2224382\n0.3578702\n0.4556137\n\n\n\n\n\n\n\nTo analyze each response, the two treatment levels for each measure need to be stacked into a single column. This is easy using melt from the data.table package. Add this to your chunk.\n\nfig1b &lt;- melt(fig1b_wide,\n              measure.vars = list(\n                c(\"BV/TV_wt\", \"BV/TV_ko\"),\n                c(\"Tb.Th_wt\", \"Tb.Th_ko\"),\n                c(\"Tb.Sp_wt\", \"Tb.Sp_ko\"),\n                c(\"Tb.N_wt\", \"Tb.N_ko\"),\n                c(\"BMD_wt\", \"BMD_ko\"),\n                c(\"Cs.Th_wt\", \"Cs.Th_ko\"),\n                c(\"BV_wt\", \"BV_ko\"),\n                c(\"MA_wt\", \"MA_ko\")),\n              variable.name = \"treatment_id\",\n              value.name = measures)\nfig1b[, treatment := ifelse(treatment_id == 1,\n                            treatment_levels[1],\n                            treatment_levels[2])]\nfig1b[, treatment := factor(treatment,\n                            levels = treatment_levels)]\n\n\n\n\n\n\ntreatment_id\nBV/TV\nTb.Th\nTb.Sp\nTb.N\nBMD\nCs.Th\nBV\nMA\ntreatment\n\n\n\n\n1\n5.229493\n0.0379685\n0.3383573\n1.377323\n0.0927138\n0.1363595\n0.2296399\n0.3832690\nWild type\n\n\n1\n5.171787\n0.0386800\n0.3579566\n1.337071\n0.0930417\n0.1387652\n0.1946119\n0.4216582\nWild type\n\n\n1\n4.287585\n0.0385307\n0.4401164\n1.112770\n0.0782485\n0.1334085\n0.1568608\n0.3620063\nWild type\n\n\n1\n4.308528\n0.0374626\n0.3576182\n1.150088\n0.0841214\n0.1369047\n0.1555844\n0.3661425\nWild type\n\n\n1\n5.175202\n0.0406120\n0.4541035\n1.274303\n0.0921535\n0.1299124\n0.1521829\n0.3578702\nWild type\n\n\n\n\n\n\n\n\n\n\n3.4.2.2 Reshaping data – Transpose (turning the columns into rows)\n\n3.4.2.2.1 Transpose – PI3K inhibitors data\nSource: Suppression of insulin feedback enhances the efficacy of PI3K inhibitors\nSource data\nFigure 3A of this publication is a plot of blood glucose level taken on the same individual mice from four treatment groups over six time periods. Data on a single variable such as blood glucose, taken on the same individual at multiple time points, are known as longitudial data but are often mistakenly called repeated measures data. There are mulitple ways to analyze longitudinal data, some goood, some less good. There are two reasonable ways to archive longitudinal data for analysis in R. The Excel-archived data for Figure 3A is neither. A screen capture of two of the four treatment groups is shown below.\n\n\n\n\n\n\n\n\n\nIn the archived data the individual mice are in columns. The measure at each time point is in rows. And the treatment group is in blocks. Typical data for analysis in R should have the individual mice in rows and each variable in columns (an exception in experimental biology is omics data, such as RNA expression levels. Many packages with functions to analyze these data have the genes on each row and the individual on each column). The Figure 3A data are turned on its side. We need to transpose the data, or rotate the matrix 90 degrees (make the columns rows and the rows columns) to turn the data into wide format. From this we can create a new data.table with the data in long format.\n\nfolder &lt;- \"Suppression of insulin feedback enhances the efficacy of PI3K inhibitors\"\nfilename &lt;- \"41586_2018_343_MOESM6_ESM.xlsx\"\nfile_path &lt;- here(data_folder, folder, filename)\npi3k_side &lt;- read_excel(file_path,\n                    sheet = \"Figure 3A (Blood Glucose)\",\n                    range = \"A2:U7\",\n                    col_names = FALSE) |&gt;\n  data.table()\n\n# give columns names as the treatment of each mouse\n# verify n=5 per group\ntreatment_levels &lt;- c(\"Chow\", \"Ketogenic\", \"Metformin\", \"SGLT2i\")\ncolnames(pi3k_side) &lt;- c(\"time\",\n                         rep(treatment_levels, each = 5))\n\n# transpose\n# keep colnames in \"side\" as values of treatment col in \"wide\"\n# make values of \"time\" in \"side\" the colnames in \"wide\"\npi3k_wide &lt;- transpose(pi3k_side,\n                       keep.names = \"treatment\", \n                       make.names = \"time\")\n\n# make a baseline column \npi3k_wide[, glucose_0 := get(\"0\")]\n\n# make-up a mouse id for each mouse\npi3k_wide[, id := paste(treatment, 1:.N, sep = \"_\"), by = treatment]\n\n# make treatement a factor with \"chow\" as reference\npi3k_wide[, treatment := factor(treatment, treatment_levels)]\n\n# make a long version\npi3k_long &lt;- melt(pi3k_wide,\n                  id.vars = c(\"treatment\", \"id\", \"glucose_0\"),\n                  variable.name = \"time\",\n                  value.name = \"glucose\")\n\nNotes\n\nRead the comments on the usage of the keep.names and make.names arguments of transpose. These are powerful.\npi3k_wide has column names that are times (in minutes). This presents wrangling problems (column names shouldn’t be numbers. Here it is useful to create the long format data.table with a time column of numbers). For example, the code above creates copies the column “0” into a new column “glucose_0” using glucose_0 := get(\"0\"). Had the code been glucose_0 := \"0\", all values would be the character “0”. Had the code been glucose_0 := 0, all values would be the number 0. get looks for the column with the name of whatever is inside the parentheses.\n\nLet’s do a quick plot to examine the data\n\nqplot(x = time,\n      y = glucose,\n      data = pi3k_long,\n      color = treatment) +\n  geom_line(aes(group = id))\n\nWarning: `qplot()` was deprecated in ggplot2 3.4.0.\n\n\n\n\n\n\n\n\n\n\n\n\n3.4.2.3 Combining data\nSource Bak, A.M., Vendelbo, M.H., Christensen, B., Viggers, R., Bibby, B.M., Rungby, J., Jørgensen, J.O.L., Møller, N. and Jessen, N., 2018. Prolonged fasting-induced metabolic signatures in human skeletal muscle of lean and obese men. PloS one, 13(9), p.e0200817.\nSource data\nThe data are from a randomized crossover design where 18 men (9 lean and 9 obese) were measured for multiple metabolic markers at two times: 1) in a post-absorptive state after 12 hours overnight fast, and 2) in a prolonged fasting state after 72 hours of fasting. In addition, at each time point, metabolic markers were measured prior to and after an insulin infusion. Here, we want to reproduce values in Table 2, which are measures of mean blood insulin and metabolite levels after 12 hours and 72 hours fasting in both the lean and obese groups.\nA difficulty for the analyst is that the response data are in the “Table 2” sheet but the variable containing the assignment to “lean” or “obese” group is in the “Table 1” sheet. To analyze these response, the two datasets need to be combined into a single data frame. The important consideration when combining data is that like is matched with like. For the fasting dataset, “like” is the subject id, and we have some data for each subject id in Table 1 and other data for the same subject ids in Table 2. This means that we essentially want to glue the columns of table 2 to the columns of table 1 in a way that insures that the correct data for each subject id is on the same row. This is a bit more complicated for these data because Table 1 contains 18 data rows, one for each subject id and Table 2 contains 36 data rows, 2 for each subject id, because each subject has data measured at 12 hours and at 72 hours.\n\n\n3.4.2.4 Subsetting data\nIt is common to see researchers create multiple subsets of data for further processing. This practice should be be discouraged because the same variables will be in multiple data frames and it can be hard to keep track of any processing of variables in the different datasets. Instead, subset the data at the level of analysis.\nThere are many ways to subset data in R. Experienced users tend to divide up into those using base R, those using the tidyverse packages, or those using data.table. Learn one well. This book uses data.table. Before outlining usage in data.table, let’s back up a bit and review different indexing systems.\n\nIn Excel, rows are specified (or “indexed”) by numbers and columns by letters. Every cell has an address, for example C2 is the cell in the 2nd row and 3rd column. Notice that in Excel, the column part of the address comes before the row part.\nIn statistics, it is extremely common to use a system where \\(x_{ij}\\) is the value of the element in the ith row and jth column of the matrix X. Notice that in this notatin, the row index (i) comes before the column index (j).\nIn programming languages, including R, it is extremely common to use a system where my_data[i, j] is the value of the element in the ith row and jth column of the matrix-like object named “my_data” (such as a data frame in R).\ndata.table explicitly refers to the row index and column index as i and j.\n\n\n3.4.2.4.1 Specifying a subset of rows (“observations” or “cases”)\nA subset of rows is specified using either a list of row numbers or\nIn a data.table, a subset of rows is specified using either a list of row numbers or a combination of comparison operators (==, !=, &gt;, &lt;, &gt;=, &lt;=, %in%) and Boolean logic operators (&, |, ! – these are “and”, “or”, “not”) as i.\nLet’s use the pi3k_long data from above to explore this. First, the plot of plasma glucose for all individuals in each treatment group across all time points.\n\nqplot(x = time,\n      y = glucose,\n      data = pi3k_long,\n      color = treatment) +\n  geom_line(aes(group = id))\n\n\n\n\n\n\n\n\npi3k_long[treatment == \"Chow\",]) is the subset of rows in which entries in the column “treatment” take the value “Chow” using the “is equal” (“==”) operator\n\nqplot(x = time,\n      y = glucose,\n      data = pi3k_long[treatment == \"Chow\",],\n      color = treatment) +\n  geom_line(aes(group = id))\n\n\n\n\n\n\n\n\nAnd the subset of rows in which entries in the column “treatment” take any value but “Chow” using the “not equal” operator (“!=”).\n\nqplot(x = time,\n      y = glucose,\n      data = pi3k_long[treatment != \"Chow\",],\n      color = treatment) +\n  geom_line(aes(group = id))\n\n\n\n\n\n\n\n\nThe subset of rows in which entries in the column “treatment” take either the value “Chow” or the value “SGLT2i” by combining two “is equal” (“==”) operators using the OR (“|”) boolean operator\n\nqplot(x = time,\n      y = glucose,\n      data = pi3k_long[treatment == \"Chow\" | treatment == \"SGLT2i\",],\n      color = treatment) +\n  geom_line(aes(group = id))\n\n\n\n\n\n\n\n\nThe subset of rows in which entries in the column “time” take either the value “30” or the value “60” using the “in a list” operator (%in%). The values in the “time” column look like integers but are actually treatment levels (which act like string or character variables).\n\nqplot(x = time,\n      y = glucose,\n      data = pi3k_long[time  %in% c(\"30\", \"60\"),],\n      color = treatment) +\n  geom_line(aes(group = id))\n\n\n\n\n\n\n\n\nThe subset of rows in which entries in the column “time_c” are less than or equal to 60 using the “less than or equal to” operator AND the value in the treatment column is in the list (“Chow”, “SGLT2i”). The two comparisons are combined with the AND (“&”) Boolean operator.\n\npi3k_long[, time_c := as.numeric(as.character(time))]\nqplot(x = time,\n      y = glucose,\n      data = pi3k_long[time_c &lt;= 30 & treatment %in% c(\"Chow\", \"SGLT2i\"),],\n      color = treatment) +\n  geom_line(aes(group = id))\n\n\n\n\n\n\n\n\nThe same result as above but using different operators. I would describe this as, the subset of rows in which entries in the column “time_c” are less than or equal to 60 using the “less than or equal to” operator AND the value in the treatment column is either “Chow” OR “SGLT2i”. The two comparisons are combined with the AND (“&”) Boolean operator. The order of operations is determined by the parentheses, as with all algebra.\n\npi3k_long[, time_c := as.numeric(as.character(time))]\nqplot(x = time,\n      y = glucose,\n      data = pi3k_long[time_c &lt;= 30 & (treatment == \"Chow\" | treatment == \"SGLT2i\"),],\n      color = treatment) +\n  geom_line(aes(group = id))\n\n\n\n\n\n\n\n\n\n\n\n3.4.2.5 Wrangling columns\n\n3.4.2.5.1 Creating new columns that are functions of values in existing columnes\ndata.table allows math within the index arguments.\nExample 1: create a new column whose value is a function of values in other columns\nIn this example, I create a column that contains the number of marked cells as a percent of the total cell count.\n\n# count to fraction - but probably want to analyze as a count! See xxx\nfigx[, marked_cells_perc := marked_cells/total_cells * 100]\n\nNotes\n\n\\(\\texttt{marked\\_cells}\\) and \\(\\texttt{total\\_cells}\\) are columns in the data.table figx\nYou almost certainly want to analyze the original count column and use \\(\\texttt{total\\_cells}\\) as an offset in the model. See the section Use a GLM with an offset instead of a ratio of some measurement per total\n\nExample 2: create a new column whose value is a function of values in other columns\nIn this example, I create the column \\(\\texttt{glucose\\_auc}\\), which is the area under the curve of glucose measures at the different time points of glucose tolerance test. This computation requires the auc function below (typically, I put chunks with functions at the top of a R Markdown document). The computation uses the base R apply function.\n\nauc &lt;- function(x, y, method=\"auc\"){\n  # method = \"auc\", auc computed using trapezoidal calc\n  # method = \"iauc\" is an incremental AUC of Le Floch\n  # method = \"pos.iauc\" is a \"positive\" incremental AUC of Le Floch but not Wolever\n  # method = \"pb.auc\" is AUC of post-time0 values\n  if(method==\"iauc\"){y &lt;- y - y[1]}\n  if(method==\"pos.iauc\"){y[y &lt; 0] &lt;- 0}\n  if(method==\"pb.auc\"){\n    x &lt;- x[-1]\n    y &lt;- y[-1]\n  }\n  n &lt;- length(x)\n  area &lt;- 0\n  for(i in 2:n){\n    area &lt;- area + (x[i] - x[i-1])*(y[i-1] + y[i])\n  }\n  area/2\n}\n\nHere is the implementation. The object time_cols is a vector containing the names of the columns containing the data at each time point.\n\ntime_cols &lt;- c(\"time_0\", \"time_15\", \"time_30\", \"time_60\", \"time_90\", \"time_120\")\nY &lt;- figx_wide[, .SD, .SDcols = time_cols]\nfigx_wide[, glucose_auc := apply(Y, 1, auc, x = times)]\n\n\n\n3.4.2.5.2 Change the reference level of a factor\nFactor levels should always be arranged in a sensible order both for model fitting and for plotting. The first level in the vector is the reference level, which is an important concept for understanding the coefficients of the statistical models fit in this book.\n\ntreatment_levels &lt;- c(\"WT\", \"KO\", \"KO_drug\")\nfigx[, treatment := factor(treatment,\n                           levels = treatment_levels)]\n\n\n\n3.4.2.5.3 Converting a single column with all combinations of a 2 x 2 factorial experiment into two columns, each containing the two levels of a factor\n\n3.4.2.5.3.1 Example 1 (tstrspl)\nThe example data are analyzed in the chapter Linear models with two categorical \\(X\\) – Factorial linear models (“two-way ANOVA”)\nArticle source: TLR9 and beclin 1 crosstalk regulates muscle AMPK activation in exercise\nPublic source\nThe data are from Figure 2j.\nData source\n\ndata_from &lt;- \"TLR9 and beclin 1 crosstalk regulates muscle AMPK activation in exercise\"\nfile_name &lt;- \"41586_2020_1992_MOESM4_ESM.xlsx\"\nfile_path &lt;- here(data_folder, data_from, file_name)\n\ntreatment_levels  &lt;- c(\"WT Resting\",\n                       \"WT Active\",\n                       \"Tlr9-/- Resting\",\n                       \"Tlr9-/- Active\")\nfig2j_wide &lt;- read_excel(file_path,\n                         sheet = \"2j\",\n                         range = \"A5:D13\",\n                         col_names = TRUE) |&gt;\n  data.table()\n\ncolnames(fig2j_wide) &lt;- treatment_levels\n\nfig2j &lt;- melt(fig2j_wide,\n              measure.vars = treatment_levels,\n              variable.name = \"treatment\",\n              value.name = \"glucose_uptake\") |&gt;\n  na.omit() # danger!\n\nfig2j[, c(\"genotype\", \"stimulation\") := tstrsplit(treatment,\n                                                  \" \",\n                                                  fixed = TRUE)]\n\ngenotype_levels &lt;- c(\"WT\", \"Tlr9-/-\")\nstimulation_levels &lt;- c(\"Resting\", \"Active\")\nfig2j[, genotype := factor(genotype,\n                           levels = genotype_levels)]\nfig2j[, stimulation := factor(stimulation,\n                              levels = stimulation_levels)]\n# View(fig2j)\n\n\nMelting the wide data creates a column \\(\\texttt{treatment}\\) containing the column names of the wide data. The column type is character (often called “string” because each the variable is a string of characters). We want to split the string at the space, and put the first part into one new column and the second part into a second new column.\nThe tstrsplit() function from the data.table package does this split for us. The first argument is the name of the column that contains the strings that we want to split. The second argument is the character that we want to split on. c(\"genotype\", \"stimulation\") before := sets the names of the new columns containing. This is a really elegant solution.\nI replaced the original column names of initial import for several reasons:\n\n\n“Electrical Stimulation” is too long and will make the coefficient names unwieldy. This could be shortened to “Stimulation”, but…\nI don’t want a factor name to include a level with the same name. I like the factor name \\(\\texttt{stimulation}\\), so I I renamed the level “Electrical Stimulation” to “Active”.\nWe are splitting the column names using the space character ” ” and two of the original column names have two spaces, the 2nd inside what should be a single level (“Electrical Stimulation”)\nThe read_excel function removed a space in the last column name, which is read as “Tlr9-/-Electrical Stimulation”.\n\nHere is a peek at the data prior to melting wide to long\n\n\n\n\n\nWT Resting\nWT Active\nTlr9-/- Resting\nTlr9-/- Active\n\n\n\n\n5.437741\n9.511914\n7.969706\n7.793486\n\n\n6.026892\n9.901527\n9.660112\n9.561649\n\n\n7.731616\n13.038920\n6.709424\n8.055529\n\n\n6.285710\n9.665391\n5.259744\n7.739374\n\n\n7.343797\n8.590693\n8.244721\n10.312190\n\n\n\n\n\n\n\nAnd a peek of the first three and last three rows after the melt.\n\n\n\n\n\ntreatment\nglucose_uptake\ngenotype\nstimulation\n\n\n\n\nWT Resting\n5.437741\nWT\nResting\n\n\nWT Resting\n6.026892\nWT\nResting\n\n\nWT Resting\n7.731616\nWT\nResting\n\n\nTlr9-/- Active\n10.312190\nTlr9-/-\nActive\n\n\nTlr9-/- Active\n7.943574\nTlr9-/-\nActive\n\n\nTlr9-/- Active\n9.292673\nTlr9-/-\nActive\n\n\n\n\n\n\n\n\n\n3.4.2.5.3.2 Example 2\nSource: Tauriello, D., Palomo-Ponce, S., Stork, D. et al. TGFβ drives immune evasion in genetically reconstituted colon cancer metastasis. Nature 554, 538–543 doi:10.1038/nature25492\nSource data\nfilename: “41586_2018_BFnature25492_MOESM10_ESM.xlsx”\nsheet: “Fig. 4h-tumours”\n\n\nWarning in warn_micro_mu(string = string, replace = replace): Watch out!  The mu or micro symbol is in the input string, and may have been converted to 'm' while 'u' may have been expected.  Consider adding the following to the `replace` argument:\nThe following characters are in the names to clean but are not replaced: \\u00b5\n\n\nThe analysis of the data in Fig. 4h specifies a single \\(X\\) variable “Treatment” with four levels (or groups): “Con”, “Gal”, “aPD-L1”, and “Gal+aPD-L1”. These levels indicate that the design is actually factorial with two factors, each with two levels. The first factor has levels “no Gal” and “Gal”. The second factor has levels “no aPD-L1”, “aPD-L1”. The single column Treatment “flattens” the 2 X 2 factorial design to a 4 x 1 design. In general, we would want to analyze an experiment like this as factorial model, because this allows us to make inferences about the interaction effect between the two factors. For these inferences, we need a standard error, or a confidence interval, or a p-value of the estimate, which we can easily get from the factorial model. In order to analyze the data with a factorial model, we need to create two new columns – one column is the factor variable containing the two levels of Gal and one column is the factor variable containing the two levels of aPD-L1.\n\ngal_levels &lt;- c(\"no Gal\", \"Gal\")\ntumor[, gal := ifelse(treatment == \"Gal\" | treatment == \"Gal+aPD-L1\",\n                      gal_levels[2],\n                      gal_levels[1])]\n\napd_levels &lt;- c(\"no aPD-L1\", \"aPD-L1\")\ntumor[, apdl1 := ifelse(treatment == \"aPD-L1\" | treatment == \"Gal+aPD-L1\",\n                      apd_levels[2],\n                      apd_levels[1])]\n\n# re-order factor levels\ntumor[, gal:=factor(gal, gal_levels)]\ntumor[, apdl1:=factor(apdl1, apd_levels)]\n\nA way to check the results to make sure that our conversion is correct is to compute the sample size for the 2 x 2 combinations, but include the original treatment column in the by list.\n\ntumor[!is.na(num_positive_per_mm), .(N=.N), by=.(treatment, gal, apdl1)]\n\n    treatment    gal     apdl1     N\n       &lt;char&gt; &lt;fctr&gt;    &lt;fctr&gt; &lt;int&gt;\n1:        Con no Gal no aPD-L1   124\n2:        Gal    Gal no aPD-L1    89\n3:     aPD-L1 no Gal    aPD-L1   101\n4: Gal+aPD-L1    Gal    aPD-L1    58\n\n\nThat looks good.\nBug alert If you break Rule #1, and type in the treatment level “Gal+aPD-L1” as “Gal + aPD-L1”, then you will get new columns containing junk.\n\n\n    treatment    gal     apdl1     N\n       &lt;char&gt; &lt;fctr&gt;    &lt;fctr&gt; &lt;int&gt;\n1:        Con no Gal no aPD-L1   124\n2:        Gal    Gal no aPD-L1    89\n3:     aPD-L1 no Gal    aPD-L1   101\n4: Gal+aPD-L1 no Gal no aPD-L1    58\n\n\nRemember Rule #1. Always copy and paste any text that will be inserted into quotes. This is easily done here by typing unique(tumor$treatment) into the console. This function returns the unique values of the column “treatment” of the data.table “tumor”.\n\nunique(tumor$treatment) [1] “Con” “Gal” “aPD-L1” “Gal+aPD-L1”\n\nNow, copy the name of a level and paste into your code. Repeat until done.\n\n\n\n\n3.4.2.6 Missing data\nSource: Deletion of Cdkn1b in ACI rats leads to increased proliferation and pregnancy-associated changes in the mammary gland due to perturbed systemic endocrine environment\nSource data\nSupplement Figure 1F of this paper shows weight as a function of age class and genotype for the whole body and 8 organs. There are some missing weights in the Excel-archived data. These missing data are designated with a minus “-” sign. To import these data in correctly, use the na = argument in the read_excel function.\n\nfile_folder &lt;- \"Deletion of Cdkn1b in ACI rats leads to increased proliferation and pregnancy-associated changes in the mammary gland due to perturbed systemic endocrine environment\"\nfile_name &lt;- \"journal.pgen.1008002.s008.xlsx\"\nfile_path &lt;- here(data_folder, file_folder, file_name)\n  \nfig_s1f &lt;- read_excel(file_path,\n                    sheet = \"all weights\",\n                    range = \"A2:K57\",\n                    na = \"-\",\n                    col_names = TRUE) |&gt;\n  clean_names() |&gt;\n  data.table()\n\nfig_s1f[, genotype := factor(genotype, c(\"+/+\", \"-/-\"))]\nfig_s1f[, age_class := ifelse(age_at_sac_wks &lt;= 6.0, \"4-6\", \"8+\")]\n\n# View(fig_s1f)\n\nNotes\n\nIn R, a value of “NA” represents missing.\nThe default value for na = is an empty (or blank) cell (not a space but a cell that is empty).\nna = accepts a list of strings, for example na = c(\"\", \"-99\", \"--\") that will all be read as na.\n\n\n3.4.2.6.1 Handling missing data\n\n3.4.2.6.1.1 Many base R functions used for summary measures require NA handling\n\nmean(fig_s1f[, ovary]) # returns \"NA\"\n\n[1] NA\n\nmean(fig_s1f[, ovary], na.rm = TRUE) # returns the mean\n\n[1] 0.2489524\n\nsd(fig_s1f[, ovary]) # returns \"NA\"\n\n[1] NA\n\nsd(fig_s1f[, ovary], na.rm = TRUE) # returns the mean\n\n[1] 0.151694\n\nsum(fig_s1f[, ovary]) # returns \"NA\"\n\n[1] NA\n\nsum(fig_s1f[, ovary], na.rm = TRUE) # returns the mean\n\n[1] 10.456\n\n\nThere are many ways to get the sample size for a particular variable. Be careful if using length() which counts NA as part of the vector of values.\n\n\n3.4.2.6.1.2 The !is.na function is useful\n\nlength(fig_s1f[, ovary])\n\n[1] 55\n\nlength(fig_s1f[!is.na(ovary), ovary])\n\n[1] 42\n\n\nNotes\n\n!is.na(ovary) is taking the subset of rows of fig_s1f for which the value of “ovary” is not NA (!is.na is read “not is.na”)\n\nThis is especially useful if you are creating your own code uses counts. Here I create a table of means, standard error of the mean, and 95% CIs of the mean for each genotype group. But first, this script generates the wrong N for each group (since there are missing values), although the mean and SD are correct.\n\nfig_s1f[, .(mean = mean(spleen, na.rm = TRUE),\n            n = .N,\n            sd = sd(spleen, na.rm = TRUE)),\n        by = genotype]\n\n   genotype      mean     n         sd\n     &lt;fctr&gt;     &lt;num&gt; &lt;int&gt;      &lt;num&gt;\n1:      -/- 0.5801333    21 0.13680480\n2:      +/+ 0.2956667    34 0.04460855\n\n\nTo compute the correct n, which will be necessary for computing the SE and the CI, use !is.na\n\nspleen_summary &lt;- fig_s1f[!is.na(spleen), .(mean = mean(spleen),\n            n = .N,\n            sd = sd(spleen)),\n        by = genotype]\nspleen_summary[, se := sd/sqrt(n)]\nspleen_summary[, lower := mean + se*qt(.025, (n-1))]\nspleen_summary[, upper := mean + se*qt(.975, (n-1))]\nspleen_summary\n\n   genotype      mean     n         sd         se     lower     upper\n     &lt;fctr&gt;     &lt;num&gt; &lt;int&gt;      &lt;num&gt;      &lt;num&gt;     &lt;num&gt;     &lt;num&gt;\n1:      -/- 0.5801333    15 0.13680480 0.03532285 0.5043734 0.6558933\n2:      +/+ 0.2956667    27 0.04460855 0.00858492 0.2780201 0.3133132\n\n\n\n\n3.4.2.6.1.3 ggplot functions automatically handle missing values\nwith a useful warning.\n\nqplot(x = body_wt_g_sac,\n      y = spleen,\n      color = genotype,\n      data = fig_s1f)\n\nWarning: Removed 13 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\n\n\n3.4.2.6.1.4 Regression model functions (lm, glm, gls, etc.) handle missing values by default\nMissing data in regression model functions such as lm are handled using the argument na.action = and the default is “na.omit”, which omits any rows that contain a missing value in one or more of the model variables (it includes rows if these contain missing values only in the columns not included in the model). It’s as if the user took the subset of data including only the columns containing the model variables and then deleted any row with missing values.\nHere is the coefficient table of the fit model object that did not explictly tell the lm function how to handle missing data.\n\nm1 &lt;- lm(spleen ~ body_wt_g_sac + genotype,\n         data = fig_s1f)\ncoef(summary(m1))\n\n                Estimate   Std. Error   t value     Pr(&gt;|t|)\n(Intercept)   0.04238009 0.0242993900  1.744081 8.902319e-02\nbody_wt_g_sac 0.00167493 0.0001506493 11.118067 1.170042e-13\ngenotype-/-   0.23760586 0.0147600545 16.097898 8.072069e-19\n\n\nHere is the coefficient table of the fit model object that did explicitly tell lm how to handle missing data, using the argument na.action = \"na.exclude\". These coefficient tables are the same.\n\nm2 &lt;- lm(spleen ~ body_wt_g_sac + genotype,\n         data = fig_s1f,\n         na.action = \"na.exclude\")\ncoef(summary(m2))\n\n                Estimate   Std. Error   t value     Pr(&gt;|t|)\n(Intercept)   0.04238009 0.0242993900  1.744081 8.902319e-02\nbody_wt_g_sac 0.00167493 0.0001506493 11.118067 1.170042e-13\ngenotype-/-   0.23760586 0.0147600545 16.097898 8.072069e-19\n\n\n\n\n\n3.4.2.6.2 But…beware of fitted, predicted, or residual values from regression model functions unless you’ve explictly told the function how to handle missing values\nUse na.action = \"na.exclude\" if you want to add the fitted (or predicted) values or residuals as new columns in the original data object (fig_sf1). Compare the length of the fitted values vector from models m1 (using the default “na.omit”) and m2 (using the “na.exclude”).\n\nlength(fitted(m1))\n\n[1] 42\n\nlength(fitted(m2))\n\n[1] 55\n\n\nThere are 55 observations (rows in the data) but only 42 complete rows with no missing values. The vector of fitted values from m1 has 42 fitted values. The vector of fitted values from m2 has 55 elements, the 42 fitted values plus 13 NA elements.\nThis is important if we want to do something like add the fitted values (or residuals, or some function of these) to the original data object (fig_sf1). Here I compute the spleen weights adjusted to the mean body weight of the control (“+/+”) group using the residuals from m1 and m2.\n\nmean_x_control &lt;- mean(fig_s1f[genotype == \"+/+\", body_wt_g_sac])\nb &lt;- coef(m1)\nfig_s1f[, spleen_adj_m1 := b[1] +\n          b[2]*mean_x_control +\n          b[3]*(as.integer(genotype)-1 +\n          residuals(m1))]\n\nWarning in as.integer(genotype) - 1 + residuals(m1): longer object length is\nnot a multiple of shorter object length\n\nfig_s1f[, spleen_adj_m2 := b[1] +\n          b[2]*mean_x_control +\n          b[3]*(as.integer(genotype)-1 +\n          residuals(m2))]\n# View(fig_s1f)\n\nThe computation of “spleen_adj_m1” returns a warning that the values of residuals(m1) were recycled (the first 42 elements of the new column were filled with the 42 residuals and the last 13 elements of the new column were filled with the first 13 residuals) – after the first row of missing data, all of these computed adjusted values are wrong. Using residuals(m2), the adjusted values are matched to the correct row and the rows with missing variables do not have an adjusted value (because there is no residual to compute this).\n\n\n\n\n3.4.3 Saving data\nFor many projects, it is uncommon to save data. I might save simulated data if it takes a long time (tens of minutes to hours or even days) to generate these and I simply want to work with the simulated data in the future and not have to regenerate it. Or I might save wrangled data if it takes a long time to import and wrangle and I want to analyze the wrangled data in the future and not have to re-import and re-wrangle it.\nIf the data will only be used in this or future R projects, the data can be saved as an R object using saveRDS()\n\ndata_from &lt;- \"Drosophila serotonin 2A receptor signaling coordinates central metabolic processes to modulate aging in response to nutrient choice\"\noutfile_name &lt;- \"elife-59399-fig1h.Rds\"\nsave_file_path &lt;- here(data_folder, data_from, outfile_name)\nsaveRDS(object = fig1h, file = save_file_path)\n\n# to read this use\nfig1h &lt;- readRDS(save_file_path)\n\nReading a large .Rds file is very fast compared to reading the same data stored as a text file. However, if the data need to be imported into some other software, such as a spreadsheet, then save the data as a text file.\n\n# save the data to correct data_from folder\ndata_from &lt;- \"Drosophila serotonin 2A receptor signaling coordinates central metabolic processes to modulate aging in response to nutrient choice\"\n\n# tab delimited\noutfile_name &lt;- \"elife-59399-fig1h.txt\"\nsave_file_path &lt;- here(data_folder, data_from, outfile_name)\nwrite.table(fig1h, save_file_path, sep = \"\\t\", quote = FALSE)\n\n# comma delimited\noutfile_name &lt;- \"elife-59399-fig1h.csv\"\nsave_file_path &lt;- here(data_folder, data_from, outfile_name)\nwrite.table(fig1h, save_file_path, sep = \",\", quote = FALSE)\n\nLook at your project directory to make sure the file is where it should be! We used write.table() to create a tab-delimited text file using sep = \"\\t\" to specify tabs to separate the row elements. “ is the standard character string for a tab. Check in your data_from folder (with the name”Drosophila serotonin 2A receptor signaling coordinates central metabolic processes to modulate aging in response to nutrient choice”) and open the file in a text editor.\n\n\n3.4.4 Exercises\n\nImport and pretty-good-plot the data for Figure 2i of the Adipsin paper. You will need to download and archive the Excel file for “Figure 2”. Store this within the “Adipsin preserves beta cells…” folder.\n\n\nThe data are the percent of cells staining for NKX6.1, which is a transcription factor protein that regulates beta cell development in the pancreas. Beta cells sense glucose levels in the blood and secrete insulin. Disruption of the insulin signaling system results in Diabetes mellitus.\nThe data are in wide format, with each treatment group in a separate column. The data need to be melted into long format with a new column called “treatment”.\nThis will give you a pretty good plot of the data (if the data object is named “adipsin_fig2i”)\n\n\nggstripchart(data = adipsin_fig2i,\n             x = \"treatment\",\n             y = \"nkx6_1\",\n             add = \"mean_se\")\n\n\n\n\n\n\n\n\n\nImport and quick pretty-good-plot the data for Figure 3b of the PI3K paper. You will need to download and archive the Excel file for “Figure 3”. Store this within the “Suppression of insulin feedback enhances…” folder.\n\n\nThe data are c-peptide levels in response to the treatments. C-peptide is cleaved from the pro-insulin polypeptide and circulates in the blood and is a marker of how much insulin is being produced by the beta cells of the pancreas.\nThe data are in wide format, with each treatment group in a separate column. The data need to be melted into long format with a new column called “treatment”.\nModify the code from exercise 1 to pretty-good-plot the data as in exercise 1.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Data -- Reading, Wrangling, and Writing</span>"
    ]
  },
  {
    "objectID": "chapters/reading-writing-wrangling.html#footnotes",
    "href": "chapters/reading-writing-wrangling.html#footnotes",
    "title": "3  Data – Reading, Wrangling, and Writing",
    "section": "",
    "text": "Corrigan, June K., et al. “A big-data approach to understanding metabolic rate and response to obesity in laboratory mice.” Elife 9 (2020): e53560.)↩︎",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Data -- Reading, Wrangling, and Writing</span>"
    ]
  },
  {
    "objectID": "chapters/p-values.html",
    "href": "chapters/p-values.html",
    "title": "6  P-values",
    "section": "",
    "text": "6.1 A p-value is the probability of sampling a value as or more extreme than the test statistic if sampling from a null distribution\nThe test statistic in the table above is a t-value. For this specific model, this t-value is precisely the t-value one would get if they executed a classical Student t-test on the two groups of liver TG values. Importantly, this is not generally true. For many of the models in this text, a t-value is computed but this is not the t-value that would be computed in a classical t-test.\nWhen we do a t-test, we get a p-value. The probability returned in a t-test is \\(p = \\mathrm{prob}(t \\ge t_{obs} | H_0)\\). Read this as “the p-value is the probability of observing a t-value that is greater than or equal to the observed t-value, given the null model is true.” Probability, in this text, is a long run frequency of sampling. The specific probability associated with the effect of treatment on liver TG is the long-run frequency of observing a t-value as big or bigger than the observed t-value (the one you actually got with your data) if the null is true. Let’s parse this into “long run frequency of observing a t-value as big or bigger than the observed t-value” and “null is true”.\nA thought experiment: You open a google sheet and insert 12 standard, normal random deviates (so the true mean is zero and the true variance is one) in Column A, rows 1-12. You arbitrarily assign the first six values (rows 1-6) to treatment A and the second six values (rows 7-12) to treatment B. You use the space immediately below these data to compute the mean of treatment A, the mean of treatment B, the difference in means (A - B), and a t-value. Unfortunately, google sheets doesn’t have a t-value function so you’d have to compute this yourself. Or not, since this is a thought experiment. Now “fill right” or copy and paste these functions into 999 new columns. You now have 1000 t-tests. The expected value of the difference in means is zero (why?) but the actual values will form a normal distribution about zero. Most will be close to zero (either in the negative or positive direction) but some will be further from zero. The expected t-value will also be zero (why?) and the distribution of these 1000 t-values will look normal but the tails are a little fuller. This row of t-values is a null distribution, because in generating the data we used the exact same formula for the values assigned to A and the values assigned to B. Now think of a t-value in your head, say 0.72 (remember that t-values will largely range from about -3 to +3 although the theoretical range is \\(-\\infty\\) to \\(+\\infty\\). What is the probability of observing a t of 0.72 or bigger if the null is true? Look at the row of t-values! Count the number of \\(t \\ge 0.72\\) and then divide by the total number of t-values in the row (1000) and you have a probability computed as a frequency. But remember the frequentist definition is the long run frequency, or the expected frequency at the limit (when you’ve generated not 1000 or even 1,000,000 but an infinite number of columns and t-values).\nSome asides to the thought experiment: First, why “as big or bigger” and not just the probability of the value itself? The reason is that the probability of finding the exact t is 1/infinity, which doesn’t do us much good. So instead we compute the probability of finding t as big, or bigger, than our observed t. Second, the t-test probability described above is a “one-tail probability”. Because a difference can be both in the positive direction and the negative direction, we usually want to count all the \\(t \\ge 0.72\\) and the \\(t \\le -0.72\\) and then add these two counts to compute the frequency of as extreme or more extreme values. This is called a “two-tailed probability” because we find extremes at both tails of the distribution. Third, we don’t really count \\(t \\ge 0.72\\) but take advantage of the beautiful mathematical properties of the theoretical t distribution, which allows us to compute the frequentist probability (expected long range frequency) given the t-value and the degrees of freedom using the t-distribution.\nNow what do I mean with the phrase “null is true”? Most people equate “null is true” with “no difference in means” but the phrase entails much more than this. Effectively, the phrase is short for “all assumptions used to compute” the p-value (see the Sander Greenland quote at the start of this chapter). A p-value is based on modeling the real data with a theoretical sample in which all the values were randomly sampled from the same distribution and the assignment of the individual values to treatment was random. Random sampling from the same distribution has three important consequences. First, random assignment to treatment group means that the expected means of each group are the same, or put differently, the expected difference in means between the assigned groups is zero. Second, random assignment to treatment also means that the expected variances of the two groups are equal. And third, random sampling means that the values of each point are independent – we cannot predict the value of one point knowing information about any other point. Here is what is super important about this: a p-value is less reliable if any one of these consequences is untrue about our data. A low p-value could arise from a difference in true means, or it could arise from a difference in true variances, or it could arise if the \\(Y\\) values are not independent of each other. This is why we need certain assumptions to make a p-value meaningful for empirical data. By assuming independent error and homogenous (equal) variances in our two samples, a low p-value is evidence of unequal means.\nLet’s summarize: A pretty good definition of a p-value is: the long-run frequency of observing a test-statistic as large or larger than the observed statistic, if the null were true. A more succinct way to state this is\n\\[\np = \\mathrm{prob}(t \\ge t_o | H_o)\n\\]\nwhere t is a hypothetically sampled t-value from a null distribution, \\(t_o\\) is the observed t-value, and \\(H_o\\) is the null hypothesis. Part of the null hypothesis is the expected value of the parameter estimated is usually (but not always) zero – this can be called the nil null. For example, if there is no ASK1 deletion effect on liver TG levels, then the expected difference between the means of the control and knockout mice is zero. Or,\n\\[\n\\mathrm{E}(\\bar{Y}_{knockout} - \\bar{Y}_{control} | H_o) = 0.0\n\\]",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>P-values</span>"
    ]
  },
  {
    "objectID": "chapters/p-values.html#pump-your-intuition-creating-a-null-distribution",
    "href": "chapters/p-values.html#pump-your-intuition-creating-a-null-distribution",
    "title": "6  P-values",
    "section": "6.2 Pump your intuition – Creating a null distribution",
    "text": "6.2 Pump your intuition – Creating a null distribution\nThe mean liver_tg in the knockout treatment is 21.6 µmol less than the mean liver_tg in the control treatment. This is the measured effect, or the observed differences in means. How confident are we in this effect? Certainly, if the researchers did the experiment comparing two control (ASK1F/F) groups, instead of a control and treatment group, they would measure some difference in their means simply because of sampling (that is which mice were sampled for the experiment). So let’s reframe the question: are the observed differences unusually large compared to a distribution of differences that would occur if there were no effect? That is, if the “null were true”. To answer this, we compare our observed difference to this null distribution. This comparison gives the probability (a long-run frequency) of “sampling” a random difference from the null distribution of differences that is as large, or larger, than the observed difference.\nWhat is a null distribution? It is the distribution of a statistic (such as a difference in means, or better, a t-value) if the null were true. Here, I hope to pump your intuition by generating a null distribution that is relevant to the ASK1 liver TG data. See if you can understand the script before reading the explanation below.\n\nseed &lt;- 1\nn_rep &lt;- 10^5 # number of replicate experiments\n\nmu &lt;- mean(fig2i[treatment == \"ASK1F/F\", liver_tg]) \nsigma &lt;- sd(fig2i[treatment == \"ASK1F/F\", liver_tg])\n\nn &lt;- nrow(fig2i[treatment == \"ASK1F/F\",])\n\nd_null &lt;- numeric(n_rep)\nfor(rep in 1:n_rep){\n  sample_1 &lt;- rnorm(n, mean = mu, sd = sigma)\n  sample_2 &lt;- rnorm(n, mean = mu, sd = sigma)\n  d_null[rep] &lt;- mean(sample_2) - mean(sample_1)\n}\n\nd_data &lt;- data.table(\n  \"d_null\" = d_null\n)\nggplot(data = d_data,\n       aes(x = d_null)) +\n  geom_histogram(bins = 30,\n                 fill = pal_okabe_ito[1],\n                 color = \"black\") +\n  ylab(\"Count\") +\n  theme_pubr()\n\n\n\n\nNull distribution for the difference in means of two samples from the same, inifinitely large population with a true mean and standard deviation equal to the observed mean and standard deviation of the ASK1 liver TG data.\n\n\n\n\nWhat have we done above? using the rnorm function, we’ve simulated an infinitely large population of mice that have a distribution of liver TG values similar to that of the mice assigned to the control (ASK1F/F) group. The true mean (\\(\\mu\\)) and standard deviation (\\(\\sigma\\)) of the simulated TG level are equal to the observed mean and standard deviation of the TG levels of the control mice.\n\nrandomly sample 6 values from this population of simulated liver TG values and assign to \\(\\texttt{sample\\_1}\\). We sample 6 values because that is the sample size of our control in the experiment.\nrandomly sample 6 values from the same population of simulated liver TG values and assign to \\(\\texttt{sample\\_2}\\).\ncompute the difference of the means: \\(\\bar{Y}_{sample\\_2} - \\bar{Y}_{sample\\_1}\\).\nrepeat 1-3 100,000 times, each time saving the difference in means.\nplot the distribution of the 100,000 differences using a histogram\n\nThe distribution of the differences is a null distribution. Notice that the mode of the null distribution is at zero, and the mean (-0.04966) is close to zero (if we had set \\(n\\) to infinity, the mean would be precisely zero). The expected difference between the means of two random samples from the same population is, of course, zero. Don’t gloss over this statement if that is not obvious. The tails extend out to a little more than +20 and -20. What this means is that it would be uncommon to randomly sample a value from this distribution of differences as or more extreme than our observed difference, -21.6. By “more extreme”, I mean any value more negative than -21.6 or more postive than 21.6. So it would be uncommon to sample a value from this distribution whose absolute value is as or more extreme than 21.6. How uncommon would this be?\n\ndiff_obs &lt;- fig2i_m1_coef[\"treatmentASK1Δadipo\", \"Estimate\"]\nnull_diff_extreme &lt;- which(abs(d_null) &gt; abs(diff_obs))\nn_extreme &lt;- length(null_diff_extreme)\n(p_d_null = n_extreme/n_rep)\n\n[1] 0.00568\n\n\nIn the 100,000 runs, only 568 generated data with an absolute difference as large or larger than 21.6 (an “absolute difference” is the absolute value of the difference). The frequency of differences as large or larger than our observed difference is 0.00568. This frequency is the probability of sampling a difference as or more extreme than the observed difference “under the null”. It is a p-value, but it is not the p-value in the coefficient table. This is because the p-value in the coefficient table is computed from a distribution of t-values, not raw differences. This raises the question, what is a t-distribution, and a t-value, more generally?",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>P-values</span>"
    ]
  },
  {
    "objectID": "chapters/p-values.html#a-null-distribution-of-t-values-the-t-distribution",
    "href": "chapters/p-values.html#a-null-distribution-of-t-values-the-t-distribution",
    "title": "6  P-values",
    "section": "6.3 A null distribution of t-values – the t distribution",
    "text": "6.3 A null distribution of t-values – the t distribution\nA t-test is a test of differences between two values. These could be\n\nthe difference between the means of two samples (a “two-sample” t-test)\nthe difference between a mean of a sample and some pre-specified value (a “one-sample” t-test)\nthe difference between a coefficient from a linear model and zero\n\nA t-test compares an observed t-value to a t-distribution. The null distribution introduced above was a distribution of mean differences under the null. A distribution of mean differences under the null is very specific to the mean and standard deviation of the population modeled and the sample size of the experiment. This isn’t generally useful, since it will be unique to every study (at least it wasn’t generally useful prior to the time of fast computers. One could, and some statisticians do, compute p-values using the algorithm above). A t-distribution is a way of transforming a null distribution of mean differences, which is unique to the study, into a distribution that is a function of sample size only.\nA t-distribution is a distribution of t-values under the null, where a t-value is a difference standardized by its standard error. For a two-sample t-test, this is\n\\[\nt = \\frac{\\bar{y}_2 - \\bar{y}_1}{SE_{\\bar{y}_2 - \\bar{y}_1}}\n\\tag{6.1}\\]\nThe numerator is the effect while the denominator is the precision of the estimate. Like many test statistics, a t-value is a signal-to-noise ratio – the effect is the signal and the SE of the difference is the noise.\nA t distribution looks like a standard, normal distribution, except the tails are heavy, meaning there are more large-ish values than the normal. Like the standard normal distribution, large t-values are unlikely under the null and, therefore, a large t has a low probability – or p-value – under the null.\nLooking at the equation for the two-sample t-test above, it is easy to see that three features of an experiment are associated with large t and small p-values: 1) big effect size (the numerator of the equation), 2) small sample standard deviations (which results in small standard errors of the difference, the denominator of Equation 6.1, and 3) large sample size (which results in small standard errors of the difference). As a quick-and-dirty generalization, absolute t-values greater than 3 are uncommon if the null is true.\nThe p-value for a t-test comes from comparing the observed t to a null t distribution and “counting” the values that are more extreme than the observed t. The p-value is the relative frequency of these more extreme values (relative to the total number of t-values in the distribution). I have “counting” in quotes because nothing is really counted – there are an infinite number of t-values in the t-distribution. Instead, the t-distribution function is integrated to compute the fraction of the total area under the curve with t-values more extreme than the observed value. In a two-tailed test, this fraction includes both tails (positive t-values more positive than \\(|t|_{observed}\\) and negative t-values more negative than \\(-|t|_{observed}\\).\nLet’s repeat the simulation of a null distribution of mean differences above but add the computation of the t-value for each replicate comparison in order to generate a null distribution of t-values. Importantly, I’ve also changed bits of the code to more properly think about what a computed t-value is. These changes are:\n\nI want to think of the first sample as being assigned to “WT” and the second sample as being assigned to “KO”. But, the KO sample is drawn from the same distribution (the same hat of numbers) as the WT sample – this guarantees that there is no difference in expected mean.\nI want to think of the observed variances of the WT and KO samples as sampled variances from this fake distribution. Therefore, I give the variance of the fake distribution the average of the observed WT and KO samples. The true (population) standard deviation (\\(\\sigma\\)) of the simulated data, then, is the square root of this averaged variance.\n\nI show the script, but don’t just cut and paste the code. Spend time thinking about what the each line does. Explore it by copying parts and pasting into console.\n\nout_path &lt;- here(\"output\",\"chap_pval-tnull.Rds\")\ndo_it &lt;- FALSE # set this to TRUE to run and save to output folder\nif(do_it){\n  seed &lt;- 1\n  n_rep &lt;- 10^5 # number of iterations\n  \n  mu &lt;- mean(fig2i[treatment == \"ASK1F/F\", liver_tg])\n  sd_control &lt;- sd(fig2i[treatment == \"ASK1F/F\", liver_tg])\n  sd_knockout &lt;- sd(fig2i[treatment == \"ASK1Δadipo\", liver_tg])\n  sigma &lt;- sqrt((sd_control^2 + sd_knockout^2)/2)\n  \n  n &lt;- nrow(fig2i[treatment == \"ASK1F/F\",])\n  \n  treatment &lt;- rep(c(\"WT\", \"KO\"), each = n) |&gt;\n    factor(levels = c(\"WT\", \"KO\")) # need this for for-loop\n  t_null &lt;- numeric(n)\n  t_null_manual &lt;- numeric(n)\n  for(rep in 1:n_rep){\n    wt_sample &lt;- rnorm(n, mean = mu, sd = sigma)\n    ko_sample &lt;- rnorm(n, mean = mu, sd = sigma)\n    \n    # way no.1 - compute the t-tests using the linear model\n    y &lt;- c(wt_sample, ko_sample)\n    m1 &lt;- lm(y ~ treatment)\n    t_null[rep] &lt;- coef(summary(m1))[\"treatmentKO\", \"t value\"]\n    \n    # way no. 2 - compute the t-tests manually!\n    # check to make sure these are the same as t_null !!!\n    diff &lt;- mean(ko_sample) - mean(wt_sample)\n    se_diff &lt;- sqrt(sd(ko_sample)^2/n + sd(wt_sample)^2/n)\n    t_null_manual[rep] &lt;- diff/se_diff\n  }\n  saveRDS(t_null, out_path)\n}\n\nt_null &lt;- readRDS(out_path)\n\nt_data &lt;- data.table(\n  \"t_null\" = t_null\n)\n\nggplot(data = t_data,\n       aes(x = t_null)) +\n  geom_histogram(bins = 30,\n                 fill = pal_okabe_ito[1],\n                 color = \"black\") +\n  ylab(\"Count\") +\n  theme_pubr()\n\n\n\n\nNull distribution of t-values. The simulation generated 10,000 t-tests with a true null.\n\n\n\n\nNow let’s use this null distribution of t-values to compute a p-value\n\n# what is the p-value?\n# the p-value is the number of t-values in t_null_2 that are as large\n# or larger than the observed t. Large, negative t-values\n# are as unlikely under the null as large, positive t-values.\n# To account for this, we want to use absolute values in our counts\n# this is a \"two-tail test\"\n\n# first assign the observed t-value\nt_obs &lt;- fig2i_m1_coef[\"treatmentASK1Δadipo\", \"t value\"]\n\n\n# now count the number of t-values in t_dis as big or bigger than this\n# include the observed value as one of these (so add 1 to the count)\ncount &lt;- sum(abs(t_null) &gt;= abs(t_obs))\n\n# the p-value is the frequency of t_dis &gt;= t_obs, so divide\n# count by the total number of t-values in the distribution.\n# Again add one since the observed value counts as a sample\n(p_ASK1 &lt;- count/(n_rep))\n\n[1] 0.01146\n\n\nHey that looks pretty good! Compare this to the p-value in the coefficient table above.\nA p-value can be computed by counting the number of simulated t-values, including the observed value, that are equal to or more extreme (in either the positive or negative direction) than the observed t. Including the observed t, there are 1146 values that are more extreme than that observed. An approximate measure of p is this count divided by 100,001 (why is 1 added to the denominator?), which is 0.01146. This simulation-based p-value is very (very!) close to that computed from the observed t-test.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>P-values</span>"
    ]
  },
  {
    "objectID": "chapters/p-values.html#p-values-from-the-perspective-of-permutation",
    "href": "chapters/p-values.html#p-values-from-the-perspective-of-permutation",
    "title": "6  P-values",
    "section": "6.4 P-values from the perspective of permutation",
    "text": "6.4 P-values from the perspective of permutation\nA very intuitive way to think about p-values as a frequency is random permutation. A permutation is a re-arrangement of items. If there is an effect of ASK1 deletion on liver TG, then the arrangement of the values in the treatment column matters. If there is no effect of ASK1 deletion on liver TG, then the arrangement of the values in the treatment column does not matter.\nThink about the structure of the liver TG data: there are two columns, treatment, which contains the assigned treatment, and liver_tg. The values in the treatment column were randomly assigned prior to the start of the experiment. If there is a negative effect of ASK1 deletion on liver TG, then assginment matters – the values in the liver_tg column for the ASK1Δadipo rows will be smaller than, on average, the values in the ASK1F/F rows. That is, a specific value of aliver_tg is what it is because of the value of treatement in the same row. Assignment to ASK1F/F or ASK1Δadipo changes the expected value of liver_tg. But, if there were no true effect, then assignment to ASK1F/F or ASK1Δadipo does not change the expected value of liver_tg. The expected value of every cell in the liver_tg column would be the same regardless of what is in the treatment column.\nIn our thought experiment, let’s leave the values in the treatment column be, and just randomly re-arrange or permute the values in the liver_tg column. What is the new expected diference in liver TG between the rows assigned to ASK1F/F and the rows assigned to ASK1Δadipo? The expected difference is Zero. Because the liver_tg values were randomly re-arranged, they cannot be caused by treatment assignment.\nA permutation is a random re-arrangement of values in a column. Consider the many thousands of permutations of the values in the liver_tg column. A difference in means can be computed from each of these permuations and a distribution of differences can be generated. Is the observed difference extreme relative to the other values in this distribution? This is a permutation test – it compares an observed statistic to a distribution of the statistic computed over many thousands of permutations.\nLet’s create a script for a permutation test\n\nset.seed(1)\nn_iter &lt;- 5000 # number of random permutations\n\ny &lt;- fig2i[, liver_tg]\nx &lt;- fig2i[, treatment]\n\nperm_diff &lt;- numeric(n_iter) # this is a vector to hold the difference in means each iteration\n\nfor(iter in 1:n_iter){\n  xbar1 &lt;- mean(y[x == \"ASK1F/F\"])\n  xbar2 &lt;- mean(y[x == \"ASK1Δadipo\"])\n  \n  perm_diff[iter] &lt;- xbar2 - xbar1\n  \n  # permute y\n  y &lt;- sample(y, replace=FALSE)\n  # note that, when i=1, the first \"permutation\" is the original arrangement\n}\n\ndiff_data &lt;- data.table(\n  Difference = perm_diff\n)\n\nggplot(data = diff_data,\n       aes(x = Difference)) +\n  geom_histogram(bins = 30,\n                 fill = pal_okabe_ito[1],\n                 color = \"black\") +\n  ylab(\"Count\") +\n  theme_pubr()\n\n\n\n\n\n\n\n\nFrom this distribution of distances generated by random permuation of the response, we can compute a permutation p-value.\n\n(p_permute &lt;- sum(abs(perm_diff) &gt;= abs(perm_diff[1]))/n_iter)\n\n[1] 0.0076",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>P-values</span>"
    ]
  },
  {
    "objectID": "chapters/p-values.html#parametric-vs.-non-parametric-statistics",
    "href": "chapters/p-values.html#parametric-vs.-non-parametric-statistics",
    "title": "6  P-values",
    "section": "6.5 Parametric vs. non-parametric statistics",
    "text": "6.5 Parametric vs. non-parametric statistics\nA statistic such as the difference in mean liver TG between ASK1Δadipo and ASK1F/F groups does not have “a” p-value. A p-value is the probability of observing an event given a model of how the event was generated. For the p-value in the coefficient table above, the event is sampling a t-value from a modeled t distribution that is as or more extreme than the observed t-value. The model generating the null distribution of t-values includes random sampling from a distribution that is defined by specific parameters (in this case, a mean and a variance), these parameters define the location and shape of the distribution of values that could be sampled. A p-value computed from a distribution that is defined by a set of parameters is a parametric p-value.\nFor the p-value computed using the permutation test, the event is the probability of of computing a difference of means from a randomly permuted set of \\(Y\\) as or more extreme than the observed difference of means. The distribution of differences from the permutated \\(Y\\) data sets was not generated by any of the known distributions (normal, Poisson, binomial, etc.) given a specific value of parameters. Consequently, the permutation p-value is non-parametric.\nThe validity of all p-values depends on a set of model assumptions, which differ from model to model. The permutation p-value has fewer assumptions than a parametric p-value because no distribution is assumed (the permutation p-value is distribution-free).",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>P-values</span>"
    ]
  },
  {
    "objectID": "chapters/p-values.html#frequentist-probability-and-the-interpretation-of-p-values",
    "href": "chapters/p-values.html#frequentist-probability-and-the-interpretation-of-p-values",
    "title": "6  P-values",
    "section": "6.6 frequentist probability and the interpretation of p-values",
    "text": "6.6 frequentist probability and the interpretation of p-values\n\n6.6.1 Background\nThere are at least three different meanings of probability.\n\nsubjective probability is the probability that an individual assigns to an event based on prior knowledge and the kinds of information considered reliable evidence. For example, if I asked a sample of students, what is the probability that a 30c homeopathic medicine could clear a Streptococcus infection from your respiratory system, their answers would differ because of variation in their knowledge of basic science, including chemistry and physics, their knowledge of what homeopathic medicines are, and how they weight different kinds of evidence.\nclassical probability is simply one divided by the number of possible unique events. For example, with a six-sided die, there are six possible unique events. The probability of rolling a 2 is \\(\\frac{1}{6}\\) and the probability of rolling an odd number is \\(\\frac{1}{2}\\).\nfrequentist probability is based on the concept of long run frequency. If I roll a die 10 times, the frequency of rolling a 2 will be approximately \\(\\frac{1}{6}\\). If I roll the die 100 times, the frequency of rolling a two will be closer to \\(\\frac{1}{6}\\). If I roll the die 1000 times, the frequency of rolling the die will be even closer to \\(\\frac{1}{6}\\). So the frequentist definition is the expected frequency given an infinite number of rolls. For events with continous outcomes, a frequentist probability is the long run frquency of observing an outcome equal to or more extreme that that observed.\n\n\n\n6.6.2 This book covers frequentist approaches to statistical modeling and when a probability arises, such as the p-value of a test statistic, this will be a frequentist probability.\nWhen we do a t-test, we get a p-value. There are several ways to think about this probability. The most compact way is \\(P(data | null)\\), which is literally read as the probability of the data given the null (or “conditional” on the null), but is really short for the probability of the data, or something more extreme than the data, given that the null hypothesis is true. The “probability of the data” is kinda vague. More specifically, we mean the probability of some statistic about the data such as the difference in means between group A and group B or the t-value associated with this difference. So, a bit more formally, the probability returned in a t-test is \\(\\mathrm{prob}(t \\ge t_{obs} | H_0)\\). This is the long run frequency of observing a t-value as big or bigger than the observed t-value (the one you actually got with your data) if the null is true. Let’s parse this into “long run frequency of observing a t-value as big or bigger than the observed t-value” and “null is true”.\nA thought experiment: You open a google sheet and insert 12 standard, normal random deviates (so the true mean is zero and the true variance is one) in Column A, rows 1-12. You arbitrarily assign the first six values (rows 1-6) to treatment A and the second six values (rows 7-12) to treatment B. You use the space immediately below these data to compute the mean of treatment A, the mean of treatment B, the difference in means (A - B), and a t-value. Unfortunately, google sheets doesn’t have a t-value function so you’d have to compute this yourself. Or not, since this is a thought experiment. Now ``fill right’’ or copy and paste these functions into 999 new columns. You now have 1000 t tests. The expected value of the difference in means is zero (why?) but the actual values will form a normal distribution about zero. Most will be close to zero (either in the negative or positive direction) but some will be further from zero. The expected t-value will also be zero (why?) and the distribution of these 1000 t-values will look normal but the tails are a little fuller. This row of t-values is a null distribution, because in generating the data we used the exact same formula for the values assigned to A and the values assigned to B. Now think of a t-value in your head, say 0.72 (remember that t-values will largely range from about -3 to +3 although the theoretical range is \\(-\\infty\\) to \\(+\\infty\\). What is the probability of observing a t of 0.72 or bigger if the null is true? Look at the row of t-values! Count the number of \\(t \\ge 0.72\\) and then divide by the total number of t-values in the row (1000) and you have a probability computed as a frequency. But remember the frequentist definition is the long run frequency, or the expected frequency at the limit (when you’ve generated not 1000 or even 1,000,000 but an infinite number of columns and t-values).\nSome asides to the thought experiment: First, why “as big or bigger” and not just the probability of the value itself? The reason is that the probability of finding the exact t is 1/infinity, which doesn’t do us much good. So instead we compute the probability of finding t as big, or bigger, than our observed t. Second, the t-test probability described above is a “one-tail probability”. Because a difference can be both in the positive direction and the negative direction, we usually want to count all the \\(t \\ge 0.72\\) and the \\(t \\le -0.72\\) and then add these two counts to compute the frequency of as extreme or more extreme values. This is called a “two-tailed probability” because we find extremes at both tails of the distribution. Third, we don’t really count \\(t \\ge 0.72\\) but take advantage of the beautiful mathematical properties of the theoretical t distribution, which allows us to compute the frequentist probability (expected long range frequency) given the t-value and the degrees of freedom using the t-distribution.\nNow what do I mean with the phrase “null is true”? Most people equate “null is true” with ``no difference in means’’ but the phrase entails much more than this. Effectively, the phrase means that the p-value is based on modeling the real data with a theoretical sample in which all the points were randomly sampled from the same distribution and that the assignment of the individual points to treatment was random. This model means the theoretical sample has three properties: First, random assignment to treatment after sampling from the same distribution means that the expected means are the same, or put differently, the expected difference in means between the assigned groups is zero. Second, random assignment to treatment after sampling from the same distribution also means that the expected variances of the two groups are equal. And third, random sampling means that the values of each point are independent – we cannot predict the value of one point knowing information about any other point. Here is what is super important about this: if we get a really low p-value, any one of these consequences may be untrue about our data, for example it could be that the true means of the two treatment groups really are different, or it could mean it is the variances that differ between the two groups, or it could mean that the data (or technically, the errors) are not independent of each other. This is why we need certain assumptions to make a p-value meaningful for empirical data. By assuming independent error and homogenous (equal) variances in our two samples, a low p-value is evidence of unequal means.\n\n\n6.6.3 Two interpretations of the p-value\nSince we want to be working scientists who want to use p-values as a tool, we need to know how to interpret (or use) the p-value to make reasonable inferences and how to avoid mis-interpreting the p-value and making unreasonable or even incorrect inferences. Two different interpretations of the p-value arose during the development of frequentist statistics. Ronald Fisher (who developed the bulk of the framework of frequentist statistics) thought of the p-value as a quantitative measure of evidence against the null hypothesis. Jerzy Neyman and Egon Pearson (Neyman-Pearson) thought of the p-value as a qualitative, threshold metric used for decision making – to act as if there is an effect. Modern researchers in biology typically use an interpretation that is an odd hybrid of the two, which often leads to illogical inference. Regardless, understanding the distinction between Fisher and Neyman-Pearson will inform how we write up our results in a manuscript.\nFisher was working in the context of an agricultural experiments, the goal of which was to discover better agricultural practices – how do the yields in these five varieties of crop differ under this agricultural practice? Fisher thought of p as evidence against the null; the smaller the p, the stronger the evidence that the mean of the two sampling distributions differ given all model assumptions are true. Fisher never thought of a single experiment as definitive. Any decision following an experiment is only partly informed by the p-value and Fisher offered no formal rule about what p-value lies on the threshold of this decision.\nNeyman-Pearson thought of p as the necessary and sufficient information to make a decision between accepting the null (or at least not rejecting the null) or rejecting the null and accepting an alternative hypothesis. This decision balances two sorts of errors: Type I (false positives), which they called \\(\\alpha\\), and Type II (false negatives), which they called \\(\\beta\\). A false positive occurs when the null is rejected (because \\(p &lt; \\alpha\\)) but there is no effect of treatment (the null is true). A false negative occurs when the test fails to reject the null (because \\(p &gt; \\alpha\\)) but there actually is an effect (the null is false). \\(\\alpha\\) is set by the experimenter and is the long-term frequency (or “rate”) of false positives when the null is true that the experimenters are willing to accept.\nAfter setting \\(\\alpha\\), the experimenter designs the experiment to achieve an acceptable rate of \\(\\beta\\). Since \\(\\beta\\) is the false negative rate, \\(1-\\beta\\) is the rate of not making a false negative error. Or, stated without the double negative, \\(1-\\beta\\) is the rate of rejecting the null (“finding an effect”) when there really is an effect. This is called the power of the experiment. An experiment with high power will have a low probability of a Type II error. An experiment with low power will have a high probability of a Type II error. Power is partly determined by sample size, the bigger the sample the smaller the p-value, all other things equal (think about why in the context of the formula for the t-value). Power is a function of error variance, both the natural variance and the component added because of measurement error (think about why in the context of the formula for the t-value). Power is also a function of \\(\\alpha\\). If we set a low \\(\\alpha\\) (say, \\(\\alpha=0.01\\)), the test is conservative. We are more likely to fail to reject the null even if the null is false. A researcher can increase power by increasing sample size, using clever strategies to reduce measurement error, or increasing alpha.\nAn experimenter sets \\(\\alpha\\), computes the sample size needed to achieve a certain level of power (\\(1-\\beta\\)), and then does the experiment. A thoughtful researcher will set \\(\\alpha\\) after considering and weighing the pros and cons of different levels of \\(\\alpha\\). If false positives have costly consequences (expense, time, deleterious side-effects), then set \\(\\alpha\\) to a low value, such as 0.01 or 0.001. For example, if an initial screen has identified a previously unknown candidate that potentially functions in the focal system of the researcher, then a researcher might decide to set a low \\(\\alpha\\) (0.001) in the initial tests of this candidate to avoid devoting time, personnel, and expense to chasing a phantom (a false-positive candidate). If false positives have trivial consequences, then set \\(\\alpha\\) to a high value, such as 0.05, or 0.1, or even 0.2. For example, if the initial tests of a candidate in a functional system are cheap and fast to construct, then a researcher might choose to set a high \\(\\alpha\\) for the screen that identifies candidates. False positive candidates don’t cost the lab much effort to identify them as false, but missing positive candidates because of a small \\(\\alpha\\) (which results in low power) at the screen stage costs the researcher the discovery of a potentially exciting component of the functional system.\nIn Fisher’s interpretation, there is no \\(\\alpha\\), no \\(\\beta\\), no alternative hypothesis, and no sharp decision rule. Instead, in Fisher, p is a continuous measure of evidence against the null and its value is interpreted subjectively by an informed and knowledgeable expert using additional information to make decisions. Neyman-Pearson rejected Fisher’s conception of p as evidence against the null arguing that a single experimental p-value is too noisy without embedding it into a more formal system of of decision making that maintains long-term type I error rates at \\(\\alpha\\), given a certain power. In Neyman-Pearson, p is compared to a threshold, \\(\\alpha\\) and this alone makes the decision. In Neyman-Pearson, p is not treated as continuous information. \\(p=0.00000001\\) is no more evidence to use to reject the null than \\(p=0.049\\).\n\n\n6.6.4 NHST\nMost biology researchers today interpret p using a combination of Fisher and Neyman-Pearson concepts in what has become known as Null Hypothesis Significance Testing (NHST).\n\nNearly all papers in biology either explicitly state something like “P values &lt; 0.05 were considered to be statistically significant” or implicitly use 0.05 as the “level of significance” (\\(\\alpha\\)). Comparing a p-value to a pre-defined \\(\\alpha\\) is Neyman-Pearson.\nUnlike Neyman-Pearson, there is little evidence that researchers are thoughtfully considering the level of \\(\\alpha\\) for each experiment. Instead, researchers mindlessly choose \\(\\alpha=0.05\\) because this is what everyone else uses.\nUnlike Neyman-Pearson, but somewhat in the spirit of Fisher, researchers, journals, and textbooks, advocate polychotomizing a statistically significant p into “significance bins” – three asterisks for \\(p &lt; 0.001\\), two asterisks for \\(0.001 &lt; p &lt; 0.01\\), and one asterisk for \\(0.01 &lt; p &lt; 0.05\\)). This is not Neyman-Pearson. Again, Neyman-Pearson developed a system to control the long-run frequency of Type I error, which is controlled by a strict use of \\(\\alpha\\). If an observed p-value is in the *** bin or the * bin is meaningless in a system using Neyman-Pearson. There is only “accept” (\\(p \\ge \\alpha\\)) or “reject” (\\(p &lt; \\alpha\\)).\nMany researchers report exact p-values when \\(p &lt; 0.05\\) but “n.s.” (not significant) when \\(p &gt; 0.05\\). Reporting exact p-values is Fisher. Reporting n.s. is Neyman-Pearson.\nMany researchers further polychomotomize the p-value space just above 0.05 by using language such as “marginally significant”.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>P-values</span>"
    ]
  },
  {
    "objectID": "chapters/p-values.html#some-major-misconceptions-of-the-p-value",
    "href": "chapters/p-values.html#some-major-misconceptions-of-the-p-value",
    "title": "6  P-values",
    "section": "6.7 Some major misconceptions of the p-value",
    "text": "6.7 Some major misconceptions of the p-value\nSetting the type I error rate \\(\\alpha\\) to 0.05 is so pervasive that I’m going to simply use “0.05” instead of “alpha” in discussing misconceptions.\n\n6.7.1 Misconception: \\(p &gt; 0.05\\) means there is no effect of treatment\nMany researchers believe that if \\(p &gt; 0.05\\) then “there is no effect.” A frequentist hypothesis test cannot show that an effect doesn’t exist, only that the null has a low probablity of producing a test statistic as extreme or more extreme than the observed effect. Even if there is a true effect of treatment, a high p-value can occur because of\n\na low signal:noise ratio, where the signal is the true effect size (the magnitude of the true difference in response) and the noise is the combination of intrinsic (biological) and extrinsic (experimental) error.\na small sample size, where small is relative to the sample size necessary for high power.\n\nThe statement “There is no effect of knockout on glucose tolerance” is not a valid conclusion of a frequentist hypothesis test. The similar statement “We found no effect of knockout on glucose tolerance” is misleading because a frequentist hypothesis test can neither find an effect nor find no effect.\n\n\n6.7.2 Misconception: a p-value is repeatable\nMany researchers believe that a p-value is a precise measure – that if the experiment were replicated, a similar p would result. This belief requires at least two misconceptions. First, if the null were true, then any p-value is equally likely. \\(p=0.00137\\) is just as likely as \\(p=0.492\\). In other words, if the null were true, the p-value is not replicable at all! Second, the p-value is highly dependent on the sample, and can be highly variable among replications, but there is no true p-value, so there can be no estimate or standard error. Let’s explore these.\n\n6.7.2.1 The incredible inconsistency of the p-value\nHow replicable is the conclusion of an experiment if the p-value for a t-test is 0.03? If our conclusion is based on \\(p &lt; 0.05\\), then the conclusion is not very replicable. The simulation below shows the results of 15 replicates of an experiment with true power of 40%. There are five “significant” results (one less than expected) but several replicates have very high p-values.\n\n\n\n\n\nVariability of p-values when the power is 0.4\n\n\n\n\n\n\n6.7.2.2 What is the distribution of p-values under the null?\nI often ask students, “if there is no true effect (no difference in means), and we were to repeat an experiment thousands of times, what is the most likely p-value?”. A common answer (although answers are uncommon) is \\(p = 0.5\\). Sometimes I rephrase the question, if there is no true effect (no difference in means), and we were to repeat an experiment thousands of times, what do you think the distribution of p-values would look like?” The typical answer to this is a the distribtion will look like a normal curve with the peak at 0.5, (presumably the tails abruptly stop at 0 and 1).\n\n\n\n\n\n\n\n\n\n\n\n\n6.7.3 Misconception: 0.05 is the lifetime rate of false discoveries\nAn important and widespread misconception is that if a researcher consistently uses \\(\\alpha=0.05\\), then the frequency of incorrectly concluding an effect exists, or “discovering” an effect, over the lifetime of the researcher, will be 5%. This is incorrect. \\(\\alpha\\) is the rate of false positives in the subset of tests in which the null hypothesis is true. \\(\\alpha\\) is the Type I error rate.\nOur mental conception of “lifetime rate of false discoveries” is the False Discovery Rate, and is the frequency of false positives divided by the frequency of positives (the sum of false and true positives).\nTo pump or intution about the differences between the Type I error rate and the False Discovery Rate, imagine we test\n\n1000 null hypotheses over a lifetime\n60% are true nulls, this means there are 600 true nulls and 400 true effects\nalpha is 5%. This means we expect to find \\(p \\le 0.05\\) 30 times (\\(0.05 \\times 600\\)) when the null is true\npower is 25%. This means we expect to find \\(p \\le 0.05\\) 100 times (\\(0.25 \\times 400\\)) when the null is false\nWe have made \\(30 + 100=130\\) “discoveries” (all experiments with \\(p \\le 0.05\\)), but\n30 of the 130, or 23%, are “false discoveries”. This is the false discovery rate.\n\nThink about this. If the null is never true, you cannot have a false discovery – every \\(p \\le 0.05\\) is a true discovery (the false discovery rate is 0%). And if the null is always true, every \\(p &lt; 0.05\\) is a false discovery (the false discovery rate is 100%).\n\n\n6.7.4 Misconception: a low p-value indicates an important effect\nMany researchers write results as if they believe that a small p-value means the effect is big or important. This may misconception may arise because of the ubiquitous use of “significant” to indicate a small p-value and “very” or “extremely” or “wicked” significant to indicate a really small p-value. Regardless, this is a misconception. A small p-value will usually result when there is high power (but can occur even if power is low) and power is a function of effect size, variability (the standard deviation), and sample size. A small p could result from a large effect size but can also result with a small effect size if the sample size is big enough.\nThis is easy to simulate (see script below). Let’s model the effect of the genotype of a gene on height\n\nset.seed(1)\nrho &lt;- 0.5\nn &lt;- 10^4\ngenotype &lt;- c(\"+/+\", \"+/-\", \"-/-\")\nSigma &lt;- diag(2)\nSigma[1,2] &lt;- Sigma[2,1] &lt;- rho\nX &lt;- rmvnorm(n, mean=c(0,0), sigma=Sigma)\ncolnames(X) &lt;- c(\"X1\", \"X2\")\nbeta &lt;- c(0.05, 0.05)\ny &lt;- X%*%beta + rnorm(n)\nfit &lt;- lm(y ~ X)\ncoefficients(summary(fit))\n\n               Estimate Std. Error   t value     Pr(&gt;|t|)\n(Intercept) 0.007472959 0.01007946 0.7414046 4.584656e-01\nXX1         0.044304824 0.01154709 3.8368830 1.253725e-04\nXX2         0.048228101 0.01170855 4.1190490 3.835033e-05\n\n\n\n\n6.7.5 Misconception: a low p-value indicates high model fit or high predictive capacity\nOn page 606, of Lock et al “Statistics: Unlocking the Power of Data”, the authors state in item D “The p-value from the ANOVA table is 0.000 so the model as a whole is effective at predicting grade point averages.” This is incorrect. A p-value is not a measure of the predictive capability of a model because the p-value is a function of the signal, noise (unmodeled error), and sample size while predictive ability is a function of just the signal:noise ratio. If the signal:noise ratio is tiny, the predictive ability is small but the p-value can be tiny if the sample size is large. This is easy to simulate (see script below). The whole-model p-value is exceptionally small (0.00001002) but the relative predictive ability, measured by the \\(R^2\\), is near zero (0.002).\n\nset.seed(1)\nrho &lt;- 0.5\nn &lt;- 10^4\nSigma &lt;- diag(2)\nSigma[1,2] &lt;- Sigma[2,1] &lt;- rho\nX &lt;- rmvnorm(n, mean=c(0,0), sigma=Sigma)\ncolnames(X) &lt;- c(\"X1\", \"X2\")\nbeta &lt;- c(0.05, -0.05)\ny &lt;- X%*%beta + rnorm(n)\nfit &lt;- lm(y ~ X)\nsummary(fit)\n\n\nCall:\nlm(formula = y ~ X)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.6449 -0.6857  0.0148  0.6756  3.6510 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  0.007473   0.010079   0.741 0.458466    \nXX1          0.044305   0.011547   3.837 0.000125 ***\nXX2         -0.051772   0.011709  -4.422  9.9e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.008 on 9997 degrees of freedom\nMultiple R-squared:  0.0023,    Adjusted R-squared:  0.002101 \nF-statistic: 11.52 on 2 and 9997 DF,  p-value: 1.002e-05",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>P-values</span>"
    ]
  },
  {
    "objectID": "chapters/p-values.html#what-the-p-value-does-not-mean",
    "href": "chapters/p-values.html#what-the-p-value-does-not-mean",
    "title": "6  P-values",
    "section": "6.8 What the p-value does not mean",
    "text": "6.8 What the p-value does not mean\n\np is not the probability of the null being true. More formally, this probability is \\(Prob(null | data)\\) but our p-value is \\(P(data | null)\\). These are not the same. \\(P(null | data)\\) is the probability of the null being true given the data. \\(P(data | null)\\) is the probability of our data, or something more extreme than our data, conditional on a true null.\n\\(1-p\\) is not the probability of the alternative\np is not a measure of effect size.\np in one experiment is not the same level of evidence against the null as in another experiment\np is not a great indicator of which is more likely, H0 or H1.\nIf one treatment level has \\(p &lt; 0.05\\) and another treatment level has \\(p &gt; 0.05\\), this is not evidence that the treatment levels have different effects on the outcome.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>P-values</span>"
    ]
  },
  {
    "objectID": "chapters/p-values.html#on-using-p-values-in-experimental-biology",
    "href": "chapters/p-values.html#on-using-p-values-in-experimental-biology",
    "title": "6  P-values",
    "section": "6.9 On using p-values in experimental biology",
    "text": "6.9 On using p-values in experimental biology\nThe flow of experiments in the article GDF15 mediates the effects of metformin on body weight and energy balance is typical of that in many experimental biology studies.\n\n(Figure 1) A cited observational studying showing an association between metformin and increased blood levels of the peptide hormone GDF15 in humans led to experiments to confirm this association in humans and mice. The positive results inferred from \\(p &lt; 0.05\\) led to follow-up experiments in Figures 2, 3, and 4.\n(Figure 2) Experiments with GDF15 knockout and GFRAL (the GDF15 receptor) knockout to probe if GDF15 is necessary for the metformin-associated weight loss, food intake reduction, and energy expenditure increase. The positive results inferred from \\(p &lt; 0.05\\) led to the conclusion that GDF15 signaling is a mediator of metformin-induced weight loss via GDF15 signaling of both food intake and energy expenditure.\n(Figure 3) Experiments with GDF15 knockout and GFRAL (the GDF15 receptor) knockout to probe if GDF15 is necessary for metformin-associated regulation of glucose homeostasis. The negative results inferred from \\(p &gt; 0.05\\) led to the conclusion that GDF15 signaling is a not a mediator of metformin-induced decreases in fasting glucose and insulin.\n(Figure 4) Experiments to probe which tissues respond to metformin by upregulating GDF15 expression. The positive results inferred from \\(p &lt; 0.05\\) led to follup up, confirmatory experiments using alternative methods and tissue sources. The positive results inferred from \\(p &lt; 0.05\\) in these original and follow up experiments led to the conclusion that metformin upregulates GDF15 expression in the small intestine, colon, rectum, and kidney.\n\nThe researchers are using the p-values as a tool not just to draw conclusions about effects and lack of effects but, importantly to identify follow-up experiments. And positive results (\\(p &lt; 0.05\\)) in this publication will motivate this research group and others to execute follow-up experiments in future studies. Here, and in effectively all other experimental biology papers, p-values are not used in a Neyman-Pearson framework despite the ubiquitous statement “we consider \\(p &lt; 0.05\\) to be significant” in the methods. Instead, smaller p-values seem to give the researchers greater confidence in the conclusion. And, the decision strategy seems to be, if the p-value is at least close to 0.05 and we expect a treatment effect given available information, any p value close to but not less than 0.05 is good enough to act as if there is an effect.\n\nabandon p-values and emphasize effect sizes and uncertainty. Problem is biological consequences of effect size is unknown and some assays use units in which effect sizes are meaningless.\nabandon p-values in favor or model selection or model averaging\nabandon frequentist statistics in favor of Bayesian methods. Not going to happen. Decisions still have to be made.\nabandon decisions based on p-values for decisions based on Bayes factors\nabandon “signficance”. Worry is that researchers will then claim effects when p = 0.1 or whatev. Who cares?\ncost probably more to optimistic p-values due to pseudoreplication or not controlling for FDR or not sufficient replication, so continue as is but do better statistics.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>P-values</span>"
    ]
  },
  {
    "objectID": "chapters/p-values.html#better-reproducibility",
    "href": "chapters/p-values.html#better-reproducibility",
    "title": "6  P-values",
    "section": "6.10 Better reproducibility",
    "text": "6.10 Better reproducibility\n\ntrue randomization and selection, blind scoring\nknowledge of statistics helps with experimental design\nuse best practice statistics especially accounting for pseudoreplication and blocking\ncombine experiments properly\nexperiments used to make decisions to move forward should 1) replicate experiments and 2) use conservative p-values (ideally results shouldn’t need a formal test)\n\nxxx finish section",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>P-values</span>"
    ]
  },
  {
    "objectID": "chapters/p-values.html#multiple-testing-and-controlling-for-false-positives",
    "href": "chapters/p-values.html#multiple-testing-and-controlling-for-false-positives",
    "title": "6  P-values",
    "section": "6.11 Multiple testing and controlling for false positives",
    "text": "6.11 Multiple testing and controlling for false positives\nBench biologists compute a bunch of p-values in every paper. For example, in a 2 x 2 factorial experiment (e.g. WT-Chow, WT-HFD, KO-Chow, KO-HFD), there are six differences between means (pairwise comparisons) plus an interaction effect. Researchers typically compute p-values for multiple measured outcomes per experiment. If there are 5 outcomes from a 2 x 2 factorial experiment, researchers might report 35 p-values. The number of potential p-values, can rise very quickly above this if one or more of the variables are measured at multiple time points, such as weekly body weight measurements over 12 weeks, or multiple glucose measurements over the 120 minute duration of a glucose tolerance test. And this is just Figure 1!\nIf these p-values are used to claim an effect (a discovery) or used to pursue a line of follow-up experiments, there will be multiple false positives. Here, I’ll define a false positive as either False positives lead to non-reproducible research. Researchers want to limit false positives because it is costly in both time and money to pursue lines of research based on a mistaken model of how something works. Exciting, positive results from one study spawn follow-up experiments not just by the original research team but also by other research groups. So, researchers want other researchers to limit false positives. And of course, funding agencies want researchers to limit false positives.\n\n6.11.1 Controlling the family-wise error rate when all tests are testing the same hypothesis",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>P-values</span>"
    ]
  },
  {
    "objectID": "chapters/p-values.html#recommendations",
    "href": "chapters/p-values.html#recommendations",
    "title": "6  P-values",
    "section": "6.12 Recommendations",
    "text": "6.12 Recommendations\n\nSimply report the exact p-value, along with a CI of the estimate.\n\n\nP-values are noisy, there is little reason to report more than two significant digits (report “\\(p = 0.011\\)” not “\\(p = 0.0108\\)”) although some journals recommend more than two significant digits.\nFor high p-values, report “\\(p = 0.23\\)” not “\\(p = n.s.\\)”.\nFor small p-values, there is little reason to report more than one significant digit (report “\\(p = 0.0002\\)” not “\\(p = 0.00018\\)”).\nFor really small p-values, there is little reason to report the exact p-value (report “\\(p &lt; 0.0001\\)” and not “\\(p = 2.365E-11\\)”). Recognize that “really small” is entirely arbitrary. Rafael Irizarry suggested that p-values less than something like the probability of being killed by lightning strike should be reported as “\\(p &lt; m\\)”, where \\(m\\) is the probability of being killed by lightning strike2. According Google University, this is 0.00000142 in one year or 0.00033 in one lifetime. This text will use \\(p &lt; ls\\)” for p-values less than 0.0001 – the lifetime probability of being killed by lightning strike in someone that spends too much time indoors analyzing data.\n\n\nIf \\(p &lt; 0.05\\) (or some other \\(\\alpha\\)) do not report this as “significant” – in fact, avoid the word “significant”. In the english language, “significant” implies big or important. Small p-values can result even with trivially small effects if \\(n\\) is big or sample variation is small. The phrase “ASK1 knockout had a significant effect on reducing liver TG (\\(p = 0.011\\))” is\n\n\npotentially misleading, if we interpret “significant” to mean “having a large effect on the regulation of liver TG”,\nwrong, if we interpret “significant” to mean “there is an ASK1 knockout effect”. A low p-value is evidence that the effect of ASK1 knockout is not zero, but I would wager that knocking out any gene expressed in white adipose cells will have some effect (however small) on liver TG.\n\n\nIf a decision needs to be made (“do we devote time, expense, and personel to pursue this further?”), then a p-value is a useful tool. If p is smaller than say 0.001, this is pretty good evidence that the data is not a fluke of sampling, as long as we are justifiably confident in all the assumptions that went into computing this p-value. A replicate experiment with a small p-value is better evidence. If p is closer to 0.01 or 0.05, this is only weak evidence of a fluke because of the sampling variability of p. A replicate experiment with a small p-value is much better evidence.\n\n\n6.12.1 Primary sources for recommendations\n\nStatistical tests, P values, confidence intervals, and power: a guide to misinterpretations.\n“Q: Why do so many colleges and grad schools teach p = 0.05? A: Because that’s still what the scientific community and journal editors use. Q: Why do so many people still use p = 0.05? A: Because that’s what they were taught in college or grad school.” – ASA Statement on Statistical Significance and P-Values\n“We then discuss our own proposal, which is to abandon statistical significance. We recommend dropping the NHST paradigm—and the p-value thresholds intrinsic to it—as the default statistical paradigm for research, publication, and discovery in the biomedical and social sciences.” – Abandon Statistical Significance\n“We conclude, based on our review of the articles in this special issue and the broader literature, that it is time to stop using the term “statistically significant” entirely. Nor should variants such as “significantly different,” “\\(p&lt;0.05\\),” and “nonsignificant” survive, whether expressed in words, by asterisks in a table, or in some other way.” – Moving to a World Beyond “p &lt; 0.05”\n“We agree, and call for the entire concept of statistical significance to be abandoned.”–Scientists rise up against statistical significance",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>P-values</span>"
    ]
  },
  {
    "objectID": "chapters/p-values.html#problems",
    "href": "chapters/p-values.html#problems",
    "title": "6  P-values",
    "section": "6.13 Problems",
    "text": "6.13 Problems\nProblem 1 – simulate the distribution of p under the null. There are many ways to do this but a straightforard approach is to\n\nCreate a \\(2n \\times m\\) matrix of random normal deviates with mean 0 and sd 1\nDo a t-test on each column, with the first \\(n\\) values assigned to one group and the remaining \\(n\\) values assigned to the second group. Save the p-value from each.\nPlot a histogram of the p-values.\nWhat is the distribution? What is the most likely value of p?\n\nProblem 2 – simulate power. Again, many ways to do this but following up on Problem 1. 1. Create a \\(2n \\times m\\) matrix of random normal deviates with mean 0 and sd 1 2. Add an effect to the first \\(n\\) values of each column. Things to think about a. what is a good effect size to add? The effect/sd ratio, known as Cohen’s d, is a relative (or standardized) measure of effect size. Cohen suggest 0.2, 0.5, and 0.8 as small, medium, and large standardized effects. b. should the same effect be added to each individual? Yes! It is the random component that captures the individual variation in the response. 3. Do a t-test on each column of the matrix, using the first \\(n\\) values in group 1 and the remaining \\(n\\) values in group 2. Save the p-values for each. 4. Compute the power, the relative frequency \\(p \\le 0.05\\). 5. Repeat with different values of \\(n\\), effect size, and sd, but only vary one at a time. How does power vary with these three parameters?",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>P-values</span>"
    ]
  },
  {
    "objectID": "chapters/p-values.html#footnotes",
    "href": "chapters/p-values.html#footnotes",
    "title": "6  P-values",
    "section": "",
    "text": "Amrhein, V., Trafimow, D. and Greenland, S., 2019. Inferential statistics as descriptive statistics: There is no replication crisis if we don’t expect replication. The American Statistician, 73(sup1), pp.262-270.↩︎\nhttps://twitter.com/rafalab/status/1310610623898808320↩︎",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>P-values</span>"
    ]
  },
  {
    "objectID": "chapters/inference-errors.html",
    "href": "chapters/inference-errors.html",
    "title": "7  Errors in inference",
    "section": "",
    "text": "7.1 Classical NHST concepts of wrong\nAs described in chapter (p-values), two types of error occur in classical Neyman-Pearson hypothesis testing, and in the NHST version that dominates modern practice. Type I error occurs when the null hypothesis is true but the p-value of the test is less than \\(\\alpha\\). This is a false positive, where a positive is a test that rejects the null. Type II error occurs when the null hypothesis is false but the p-value of the test is greater than \\(\\alpha\\). This is a false negative, where a negative is a test that accepts (or fails to reject) the null. Power is not an error but the frequency of true, positive tests (or the frequency of avoiding Type II error). \\(\\alpha\\) is not an error but the rate of Type I error that a researcher is willing to accept. Ideally, a researcher sets \\(\\alpha\\) based on an evaluation of the pros and cons of Type I and Type II error for the specific experiment. In practice, researchers follow the completely arbitary practice of setting \\(\\alpha = 0.05\\).\nWhy should a researcher care about \\(\\alpha\\) and power? Typically, most researchers don’t give \\(\\alpha\\) much thought. And power is considered only in the context of calculating a sample size for an experiment for a grant proposal. But researchers should care about rates of Type I error and power because these (and similar concepts) can help guide decisions about which model to fit to a specific dataset.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Errors in inference</span>"
    ]
  },
  {
    "objectID": "chapters/inference-errors.html#classical-nhst-concepts-of-wrong",
    "href": "chapters/inference-errors.html#classical-nhst-concepts-of-wrong",
    "title": "7  Errors in inference",
    "section": "",
    "text": "7.1.1 Type I error\nIn classical Neyman-Pearson hypothesis testing, an important property of a hypothesis test is the size of a test, which may include an entire procedure that culminates in a hypothesis test. “Size” is a weird name for the probability of rejecting the null when the null is true. Size is not \\(\\alpha\\). \\(\\alpha\\) is the nominal value – size is the actual value under a specific parameterization of the model.\nIt would probably come as a surprise to most researchers to learn that the size of some common tests used with data that look like the researcher’s data is not 0.05. “used with data that look like the researcher’s data” is important here – a t-test doesn’t have one size. With data that conform to the assumptions (independence, homogeneity, normality), the size of a t-test is \\(\\alpha\\). But with any violation, especially when the sample size differs between groups, the size of the t-test can move away from \\(\\alpha\\). A test that has a size that is less than \\(\\alpha\\) is “conservative” (fewer nulls are rejected than we think, so the status quo is more often maintained). A test that has a size that is greater than \\(\\alpha\\) is “anti-conservative”, or “liberal” (more nulls are rejected than we think, so the status quo is less often maintained). More conservative tests reduce power. More liberal tests artificially increase power and increase our rate of false rejection, which can mean “false discovery” if p-values are used as the arbiter of discovery.\n\n7.1.1.1 Size example 1: the size of a t-test vs. a permutation test, when the data meet the assumptions\n\nn_iter &lt;- 10000\nout_path &lt;- here(\"output\", \"chap_infer_error-p_data_1.Rds\")\ndo_it &lt;- FALSE # set this to TRUE to run and save to the output folder\nif(do_it){\n  set.seed(1)\n  n &lt;- 10\n  p_t &lt;- numeric(n_iter)\n  p_perm &lt;- numeric(n_iter)\n  \n  treatment &lt;- rep(c(\"cn\", \"tr\"), each = n)\n  for(iter in 1:n_iter){\n    sample_1 &lt;- rnorm(n, mean = 10, sd = 1)\n    sample_2 &lt;- rnorm(n, mean = 10, sd = 1)\n    y &lt;- c(sample_1, sample_2)\n    m1 &lt;- lm(y ~ treatment) # no data statement necessary because both variables in workspace\n    p_t[iter] &lt;- coef(summary(m1))[\"treatmenttr\", \"Pr(&gt;|t|)\"]\n    \n    m2 &lt;- lmp(y ~ treatment,\n              perm = \"Prob\",\n              settings = FALSE)\n    p_perm[iter] &lt;- coef(summary(m2))[\"treatment1\", \"Pr(Prob)\"]\n  }\n  p_data &lt;- data.table(\n    t = p_t,\n    perm = p_perm\n  )\n  saveRDS(p_data, out_path)\n}\n\np_data &lt;- readRDS(out_path)\np_t &lt;- p_data[, t]\np_perm &lt;- p_data[, perm]\nsize_t &lt;- sum(p_t &lt; 0.05)/n_iter\nsize_perm &lt;- sum(p_perm &lt; 0.05)/n_iter\nsize_table &lt;- data.table(Method = c(\"t-test\", \"Permutation test\"),\n                         Size = c(size_t, size_perm))\nsize_table |&gt;\n  kable(digits = 4) |&gt;\n  kable_styling(full_width = FALSE)\n\n\n\n\nMethod\nSize\n\n\n\n\nt-test\n0.0489\n\n\nPermutation test\n0.0488\n\n\n\n\n\n\n\n\n\n7.1.1.2 Size example 2: the size of a t-test vs. a permutation test, when the data have a right skewed distribution\n\nout_path &lt;- here(\"output\", \"chap_infer_error-p_data_2.Rds\")\nn_iter &lt;- 10000\ndo_it &lt;- FALSE # set this to TRUE to run and save to the output folder\nif(do_it){\n  set.seed(1)\n  n &lt;- 10\n  p_t &lt;- numeric(n_iter)\n  p_perm &lt;- numeric(n_iter)\n  \n  treatment &lt;- rep(c(\"cn\", \"tr\"), each = n)\n  for(iter in 1:n_iter){\n    #  qplot(rnegbin(n = 10^4, mu = 100, theta = 1))\n    sample_1 &lt;- rnegbin(n, mu = 100, theta = 1)\n    sample_2 &lt;- rnegbin(n, mu = 100, theta = 1)\n    y &lt;- c(sample_1, sample_2)\n    # qplot(x=treatment, y = y)\n    m1 &lt;- lm(y ~ treatment) # no data statement necessary because both variables in workspace\n    p_t[iter] &lt;- coef(summary(m1))[\"treatmenttr\", \"Pr(&gt;|t|)\"]\n    \n    m2 &lt;- lmp(y ~ treatment,\n              perm = \"Prob\",\n              settings = FALSE)\n    p_perm[iter] &lt;- coef(summary(m2))[\"treatment1\", \"Pr(Prob)\"]\n  }\n  p_data &lt;- data.table(\n    t = p_t,\n    perm = p_perm\n  )\n  saveRDS(p_data, out_path)\n}\n\np_data &lt;- readRDS(out_path)\np_t &lt;- p_data[, t]\np_perm &lt;- p_data[, perm]\nsize_t &lt;- sum(p_t &lt; 0.05)/n_iter\nsize_perm &lt;- sum(p_perm &lt; 0.05)/n_iter\nsize_table &lt;- data.table(Method = c(\"t-test\", \"Permutation test\"),\n                         Size = c(size_t, size_perm))\nsize_table |&gt;\n  kable(digits = 4) |&gt;\n  kable_styling(full_width = FALSE)\n\n\n\n\nMethod\nSize\n\n\n\n\nt-test\n0.0438\n\n\nPermutation test\n0.0504\n\n\n\n\n\n\n\n\n\n7.1.1.3 Size example 3: the size of a t-test vs. a permutation test, when the data have heterogenous variance and the sample size is unequal\n\nout_path &lt;- here(\"output\", \"chap_infer_error-p_data_3.Rds\")\nn_iter &lt;- 10000\nset.seed(1)\ndo_it &lt;- FALSE # set this to TRUE to run and save to the output folder\nif(do_it){\n  n1 &lt;- 10\n  n2 &lt;- n1/2\n  n_perm &lt;- 1000 # for the manual permutation test, otherwise, not used\n  p_t &lt;- numeric(n_iter)\n  p_perm &lt;- numeric(n_iter)\n  t_perm &lt;- numeric(n_perm)\n  treatment &lt;- rep(c(\"cn\", \"tr\"), times = c(n1, n2))\n  for(iter in 1:n_iter){\n    #  qplot(rnegbin(n = 10^4, mu = 100, theta = 1))\n    sample_1 &lt;- rnorm(n1, mean = 10, sd = 0.5)\n    sample_2 &lt;- rnorm(n2, mean = 10, sd = 1)\n    y &lt;- c(sample_1, sample_2)\n    # qplot(x=treatment, y = y)\n    m1 &lt;- lm(y ~ treatment) # no data statement necessary because both variables in workspace\n    p_t[iter] &lt;- coef(summary(m1))[\"treatmenttr\", \"Pr(&gt;|t|)\"]\n    \n    # manual permutation test -- this is slooooo!\n    # y_samp &lt;- y\n    # for(perm_iter in 1:n_perm){\n    #   m2 &lt;- lm(y_samp ~ treatment)\n    #   t_perm[perm_iter] &lt;- coef(summary(m2))[2, \"t value\"]\n    #   y_samp &lt;- sample(y_samp, )\n    # }\n    # p_perm[iter] &lt;- sum(abs(t_perm) &gt;= abs(t_perm[1]))/n_perm\n    \n    m2 &lt;- lmp(y ~ treatment,\n              perm = \"Prob\",\n              settings = FALSE)\n    p_perm[iter] &lt;- coef(summary(m2))[\"treatment1\", \"Pr(Prob)\"]\n  }\n  p_data &lt;- data.table(\n    t = p_t,\n    perm = p_perm\n  )\n  saveRDS(p_data, out_path)\n}\n\np_data &lt;- readRDS(out_path)\np_t &lt;- p_data[, t]\np_perm &lt;- p_data[, perm]\nsize_t &lt;- sum(p_t &lt; 0.05)/n_iter\nsize_perm &lt;- sum(p_perm &lt; 0.05)/n_iter\nsize_table &lt;- data.table(Method = c(\"t-test\", \"Permutation test\"),\n                         Size = c(size_t, size_perm))\nsize_table |&gt;\n  kable(digits = 4) |&gt;\n  kable_styling(full_width = FALSE)\n\n\n\n\nMethod\nSize\n\n\n\n\nt-test\n0.1150\n\n\nPermutation test\n0.1211\n\n\n\n\n\n\n\n\n\n\n7.1.2 Power\nIn classical Neyman-Pearson hypothesis testing, an important property of a hypothesis test is the power of a test. “Power” is the probability of rejecting the null when the null is false. A common way to think about power is, power is a test’s ability to “detect” an effect if it exists. This makes sense using Neyman-Pearson but not Fisher (Using Fisher, a p-value is not a detector of an effect – a reasoning brain is). Using Fisher, we could say that power is the sensitivity of a test (it takes less sample to provide the same signal).\n\n7.1.2.1 Power example 1: the power of a t-test vs. a permutation test, when the data meet the assumptions\n\nset.seed(1)\nn &lt;- 10\nn_iter &lt;- 10000\np_t &lt;- numeric(n_iter)\np_perm &lt;- numeric(n_iter)\n\ntreatment &lt;- rep(c(\"cn\", \"tr\"), each = n)\nfor(iter in 1:n_iter){\n  sample_1 &lt;- rnorm(n, mean = 10, sd = 1)\n  sample_2 &lt;- rnorm(n, mean = 11, sd = 1)\n  y &lt;- c(sample_1, sample_2)\n  m1 &lt;- lm(y ~ treatment) # no data statement necessary because both variables in workspace\n  p_t[iter] &lt;- coef(summary(m1))[\"treatmenttr\", \"Pr(&gt;|t|)\"]\n  \n  m2 &lt;- lmp(y ~ treatment,\n            perm = \"Prob\",\n            settings = FALSE)\n  p_perm[iter] &lt;- coef(summary(m2))[\"treatment1\", \"Pr(Prob)\"]\n}\npower_t &lt;- sum(p_t &lt; 0.05)/n_iter\npower_perm &lt;- sum(p_perm &lt; 0.05)/n_iter\npower_table_normal &lt;- data.table(Method = c(\"lm\", \"perm\"),\n                         Power = c(power_t, power_perm))\nknitr::kable(power_table_normal, digits = 3)\n\n\n\n\nMethod\nPower\n\n\n\n\nlm\n0.554\n\n\nperm\n0.554\n\n\n\n\n\n\n\n7.1.2.2 Power example 2: the power of a t-test vs. a permutation test, when the data look like typical count data\n\nset.seed(1)\nn &lt;- 10\nn_iter &lt;- 10000\np_t &lt;- numeric(n_iter)\np_perm &lt;- numeric(n_iter)\n\ntreatment &lt;- rep(c(\"cn\", \"tr\"), each = n)\n\nfor(iter in 1:n_iter){\n  #  qplot(rnegbin(n = 10^4, mu = 100, theta = 1))\n  sample_1 &lt;- rnegbin(n, mu = 100, theta = 1)\n  sample_2 &lt;- rnegbin(n, mu = 300, theta = 1)\n  y &lt;- c(sample_1, sample_2)\n  # qplot(x=treatment, y = y)\n  m1 &lt;- lm(y ~ treatment) # no data statement necessary because both variables in workspace\n  p_t[iter] &lt;- coef(summary(m1))[\"treatmenttr\", \"Pr(&gt;|t|)\"]\n  \n  m2 &lt;- lmp(y ~ treatment,\n            perm = \"Prob\",\n            settings = FALSE)\n  p_perm[iter] &lt;- coef(summary(m2))[\"treatment1\", \"Pr(Prob)\"]\n}\npower_t &lt;- sum(p_t &lt; 0.05)/n_iter\npower_perm &lt;- sum(p_perm &lt; 0.05)/n_iter\npower_table_count &lt;- data.table(Method = c(\"lm\", \"perm\"),\n                         Power = c(power_t, power_perm))\nknitr::kable(power_table_count, digits = 3)\n\n\n\n\nMethod\nPower\n\n\n\n\nlm\n0.508\n\n\nperm\n0.578",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Errors in inference</span>"
    ]
  },
  {
    "objectID": "chapters/inference-errors.html#a-non-neyman-pearson-concept-of-power",
    "href": "chapters/inference-errors.html#a-non-neyman-pearson-concept-of-power",
    "title": "7  Errors in inference",
    "section": "7.2 A non-Neyman-Pearson concept of power",
    "text": "7.2 A non-Neyman-Pearson concept of power\nSize and power are concepts specific to the Neyman-Pearson hypothesis testing framework. Size and power also have limited (or no) use in a research program in which the null hypothesis is never (or rarely) strictly true. That said, the concept of size and power are useful. For example, what if we framed power as the distribution of p-values instead of the frequency of p-values less than \\(\\alpha\\).\nTable @ref(tab:error-power-dis) shows the p-value at the 10th, 25th, 50th, 75th, and 90th percentile of the set of p-values computed in Power Example 2 above (count data). The nth percentile is the value in an ordered set of numbers in which n % are less than the value and 100 - n% are greater than the value. The 50th percentile is the median. The table shows that at all percentiles except the 90th, the permutation p-value is smaller than the t-test p-value. And, importantly, the value at 75% for both is ~ 0.12. This means that for experiments that generate data something like the fake data generated in Power Example 2, the permutation test is more sensistive to the incompatibility between the null model and the data than the t-test, except in the random samples when both methods fail.\n\nquantile_list &lt;- c(0.1, 0.25, 0.5, 0.75, 0.9)\npercentiles_t &lt;- quantile(p_t, quantile_list)\npercentiles_perm &lt;- quantile(p_perm, quantile_list)\n\nalt_power_table &lt;- data.table(method = c(\"t-test\", \"permutation\"),\n                              (rbind(percentiles_t,\n                                     percentiles_perm)))\nknitr::kable(alt_power_table, digits = c(1, 4, 3, 3, 2, 2))\n\n\n\n\nmethod\n10%\n25%\n50%\n75%\n90%\n\n\n\n\nt-test\n0.0048\n0.016\n0.048\n0.13\n0.28\n\n\npermutation\n0.0012\n0.007\n0.034\n0.12\n0.33\n\n\n\n\n\n\n7.2.1 Estimation error\n\n\n7.2.2 Coverage\nThis text advocates reporting a confidence interval with each reported effect size. An important property of an estimator is coverage probability, often shortened to “coverage”.\n\n\n7.2.3 Type S error\nInstead of framing the “size” concept as the rate of Type I error, what if we framed this as the rate that an estimate is in the correct direction (meaning, the sign of an effect is the same as the true value). And,\n\n\n7.2.4 Type M error",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Errors in inference</span>"
    ]
  },
  {
    "objectID": "chapters/intro-stat-modeling.html",
    "href": "chapters/intro-stat-modeling.html",
    "title": "8  An introduction to linear models",
    "section": "",
    "text": "8.1 Two specifications of a linear model",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>An introduction to linear models</span>"
    ]
  },
  {
    "objectID": "chapters/intro-stat-modeling.html#lm-specifications",
    "href": "chapters/intro-stat-modeling.html#lm-specifications",
    "title": "8  An introduction to linear models",
    "section": "",
    "text": "8.1.1 The “error draw” specification\nIn introductory textbooks, a linear model is typically specified using an error-draw scheme.\n\\[\n\\begin{align}\nY &= \\beta_0 + \\beta_1 X + \\varepsilon \\\\\n\\varepsilon &\\sim N(0, \\sigma^2)\n\\end{align}\n\\tag{8.2}\\]\nThe first line of this specification has two components: the linear predictor \\(Y = \\beta_0 + \\beta_1 X\\) and the error \\(\\varepsilon\\). The linear predictor component looks like the equation for a line except that 1) \\(\\beta_0\\) is used for the intercept and \\(\\beta_1\\) for the slope and 2) the intercept term precedes the slope term. This re-labeling and re-arrangement make the notation for a linear model more flexible for more complicated linear models. For example \\(Y = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\varepsilon\\) is a model where \\(Y\\) is a function of two \\(X\\) variables.\nThe linear predictor is the deterministic or systematic part of the specification. As with the equation for a line, the linear predictor component of a linear model is a function that maps a specific value of \\(X\\) to a unique value of \\(Y\\). This mapped value is the expected value, or expectation, given a specific input value of \\(X\\). The expectation is often written as \\(\\mathrm{E}[Y|X]\\), which is read as “the expected value of \\(Y\\) given \\(X\\)”, where “given X” means a specific value of X. This text will often use the word conditional in place of “given”. For example, I would read \\(\\mathrm{E}[Y|X]\\) as “the expected value of \\(Y\\) conditional on \\(X\\)”. It is important to recognize that \\(\\mathrm{E}[Y|X]\\) is a conditional mean – it is the mean value of \\(Y\\) when we observe that \\(X\\) has some specific value \\(x\\) (that is \\(X = x\\)).\nThe second line of the specification (Equation 8.2) is read as “epsilon is distributed as Normal with mean zero and variance sigma squared”. This line explicitly specifies the distribution of the error component of line 1. The error component of a linear model is a random “draw” from a normal distribution with mean zero and variance \\(\\sigma^2\\). The second line shows that the error component of the first line is stochastic. Using the error-model specification, we can think of any measurement of \\(Y\\) as an expected value plus some random value sampled from a normal distribution with a specified variance. Because the stochastic part of this specification draws an “error” from a population, I refer to this as the error-draw specification of the linear model.\n\n\n8.1.2 The “conditional draw” specification\nA second way of specifying a linear model is using a conditional-draw scheme.\n\\[\n\\begin{align}\ny_i &\\sim N(\\mu_i, \\sigma^2)\\\\\n\\mathrm{E}(Y|X) &= \\mu\\\\\n\\mu_i &= \\beta_0 + \\beta_1 x_i\n\\end{align}\n\\tag{8.3}\\]\nThe first line states that the response variable \\(Y\\) is a random variable independently drawn from a normal distribution with mean \\(\\mu\\) and variance \\(\\sigma^2\\). This first line is the stochastic part of the statistical model. The second line simply states that \\(\\mu\\) (the greek letter “mu”) from the first line is the conditional mean (or expectation). The third line is the liner predictor, which states how \\(\\mu_i\\) is generated given that \\(X=x_i\\). Again, the linear predictor is the systematic (or deterministic) part of the statistical model. It is systematic because the same value of \\(x_i\\) will always generate the same \\(\\mu_i\\). With the conditional-draw specification, we can think of a measurement (\\(y_i\\)) as a random draw from the specified distribution. Because it is \\(Y\\) and not some “error” that is drawn from the specified distribution, I refer to this as the conditional-draw specification of the linear model.\n\n\n8.1.3 Comparing the error-draw and conditional-draw ways of specifying the linear model\nThese two ways of specifying the model encourage slightly different ways of thinking about how the data (the response varible \\(Y\\)) were generated. The error-draw specification “generates” data by 1) constructing what \\(y_i\\) “should be” given \\(x_i\\) (this is the conditional expection), then 2) adding some error \\(e_i\\) drawn from a normal distribution with mean zero and some specified variance. The conditional-draw specification “generates” data by 1) constructing what \\(y_i\\) “should be” given \\(x_i\\), then 2) drawing a random variable from some specified distribution whose mean is this expectation. This random draw is not “error” but the measured value \\(y_i\\). For the error draw generation, we need only one hat of random numbers, but for the conditional draw generation, we need a hat for each value of \\(x_i\\).\nHere is a short script that generates data by implementing both the error-draw and conditional-draw specifications. See if you can follow the logic of the code and match it to the meaning of these two ways of specifying a linear model.\n\nn &lt;- 5\nb_0 &lt;- 10.0\nb_1 &lt;- 1.2\nsigma &lt;- 0.4\nx &lt;- 1:n\ny_expected &lt;- b_0 + b_1*x\n\n# error-draw. Note that the n draws are all from the same distribution\nset.seed(1)\ny_error_draw &lt;- y_expected + rnorm(n, mean = 0, sd = sigma)\n\n# conditional-draw. Note that the n draws are each from a different\n# distribution because each has a different mean.\nset.seed(1)\ny_conditional_draw &lt;- rnorm(n, mean = y_expected, sd = sigma)\n\ndata.table(X = x,\n           \"Y (error draw)\" = y_error_draw,\n           \"Y (conditional draw)\" = y_conditional_draw)\n\n       X Y (error draw) Y (conditional draw)\n   &lt;int&gt;          &lt;num&gt;                &lt;num&gt;\n1:     1       10.94942             10.94942\n2:     2       12.47346             12.47346\n3:     3       13.26575             13.26575\n4:     4       15.43811             15.43811\n5:     5       16.13180             16.13180\n\n\n\n\n\n\n\n\nPseudorandom\n\n\n\nrnorm() is a pseudorandom number generator that simulates random draws from a normal distribution with the specified mean and variance. The algorithm to generate the numbers is entirely deterministic – the numbers are not truly random but are “pseudorandom”. The list of numbers returned closely approximates a set of true, random numbers. The sequence of numbers returned is determined by the “seed”, which can be set with the set.seed() function (R will use an internal seed if not set by the user).\n\n\nThe error-draw specification is not very useful for thinking about data generation for data analyzed by generalized linear models, which are models that allow one to specify distribution families other than Normal (such as the binomial, Poisson, and Gamma families). In fact, thinking about a model as a predictor plus error can lead to the misconception that, in a generalized linear model, the error (or residuals from the fit) has a distribution from the non-Normal distribution modeled. This cannot be true because the distributions modeled using generalized linear models (other than the Normal) do not have negative values (some residuals must have negative values since the mean of the residuals is zero). Introductory biostatistics textbooks typically only introduce the error-draw specification because introductory textbooks recommend data transformation or non-parametric tests if the data are not approximately normal. This is unfortunate because generalized linear models are extremely useful for real biological data.\nAlthough a linear model (or statistical model more generally) is a model of a data-generating process, linear models are not typically used to actually generate any data. Instead, when we use a linear model to understand something about a real dataset, we think of our data as one realization of a process that generates data like ours. A linear model is a model of that process. That said, it is incredibly useful to use linear models to create fake datasets for at least two reasons: to probe our understanding of statistical modeling generally and, more specifically, to check that a model actually creates data like that in the real dataset that we are analyzing.\n\n\n8.1.4 ANOVA notation of a linear model\nMany textbooks treat ANOVA differently from regression and express a linear model as an ANOVA model (and generally do not use the phrase “linear model”). ANOVA models are all variations of\n\\[\n\\begin{equation}\ny_{ij} = \\mu + \\tau_{i} + \\varepsilon_{ij}\n\\end{equation}\n\\tag{8.4}\\]\nUnlike the error and conditional draw specifications above, the ANOVA model doesn’t have a linear predictor in the form of a regression equation (or the equation for a line) – that is, there are neither \\(X\\) variables nor coefficients (\\(\\beta\\)). Instead, the ANOVA model is made up of a linear combination of means and deviations from means. In (Equation 8.4), \\(\\mu\\) is the grand mean (the mean of the means of the groups), \\(\\tau_i\\) is the deviation of the mean of group \\(i\\) from the grand mean (these are the effects), and \\(\\varepsilon_{ij}\\) is the deviation (or error) of individual \\(j\\) from the mean of group \\(i\\). Traditional ANOVA computes effects and the statistics for inference by computing means and deviations from means. Modern linear models compute effects and the statistics for inference by solving for the coefficients of a regression model.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>An introduction to linear models</span>"
    ]
  },
  {
    "objectID": "chapters/intro-stat-modeling.html#a-linear-model-can-be-fit-to-data-with-continuous-discrete-or-categorical-x-variables",
    "href": "chapters/intro-stat-modeling.html#a-linear-model-can-be-fit-to-data-with-continuous-discrete-or-categorical-x-variables",
    "title": "8  An introduction to linear models",
    "section": "8.2 A linear model can be fit to data with continuous, discrete, or categorical \\(X\\) variables",
    "text": "8.2 A linear model can be fit to data with continuous, discrete, or categorical \\(X\\) variables\nIn the linear model fit to the data in Figure (Figure 8.1)B, the \\(X\\) variable is continuous, which can take any real number between the minimum \\(X\\) and maximum \\(X\\) in the data. For biological data, most variables that are continuous are positive, real numbers (a zero is not physically possible but could be recorded in the data if the true value is less than the minimum measurable amount). One exception is a composition (the fraction of a total), which can be zero. Negative values can occur with variables in which negative represent a direction (work, electrical potential) or a rate. Discrete variables are numeric but limited to certain real numbers. Most biological variables that are discrete are counts, and can be zero, but not negative. Categorical variables are non-numeric descriptions of a measure. Many of the categorical variables in this text will be the experimentally controlled treatment variable of interest (the variable \\(treatment\\) containing the values “wild type” and “knockout”) but some are measured covariates (the variable \\(sex\\) containing the values “female” and “male”).\n\n8.2.1 Fitting linear models to experimental data in which the \\(X\\) variable is continuous or discrete\nA linear model fit to data with a numeric (continous or discrete) \\(X\\) is classical regression and the result is typically communicated by a regression line. The experiment introduced in (linear-models-with-a-single-continuous-x?, Linear models with a single, continuous X) is a good example. In this experiment, the researchers designed an experiment to measure the effect of warming on the timing of photosynthetic activity. Temperature was experimentally controlled at one of five settings (0, 2.25, 4.5, 6.75, or 9 °C above ambient temperature) within twelve, large enclosures. The response variable in the illustrated example is Autumn “green-down”, which is the day of year (DOY) of the transition to loss of photosynthesis. The intercept and slope parameters of the regression line (Figure (Figure 8.2)) are the coefficients of the linear model. The slope (4.98 days per 1 °C added warmth) estimates the effect of warming on green-down DOY. What is not often appreciated at the introductory biostatistics level is that the slope is a difference in conditional means. Any point on a regression line is the expected value of \\(Y\\) at a specified value of \\(X\\), that is, the conditional mean \\(\\mathrm{E}(Y|X)\\). The slope is the difference in expected values for a pair of points that differ in \\(X\\) by one unit.\n\\[\nb_1 = \\mathrm{E}(Y|X=x+1) - \\mathrm{E}(Y|X=x+1)\n\\]\nI show this in Figure 8.2 using the points on the regression line at \\(x = 5\\) and \\(x = 6\\). Thinking about a regression coefficient as a difference in conditional means is especially useful for understanding the coefficients of a categorical \\(X\\) variable, as described below.\n\n\n\n\n\n\nFigure 8.2: Illustration of the slope in a linear model with a numeric X. The slope (the coefficient of X) is the difference in expected value for any two X that are one unit apart. This is illustrated for the points on the line at x = 5 and x = 6.\n\n\n\n\n\n8.2.2 Fitting linear models to experimental data in which the \\(X\\) variable is categorical\nLinear models can be fit to experimental data in which the \\(X\\) variable is categorical – this is the focus of this text! For the model fit to the data in Figure 8.1 B, the coefficient of \\(X\\) is the slope of the line. Perhaps surprisingly, 1) we can fit a model like Equation 8.2 to data in which the \\(X\\) variable is categorical and 2) the coefficient of \\(X\\) is a slope. How is this possible? The slope of a line is \\(\\frac{y_2 - y_1}{x_2 - x_1}\\) where \\((x_1, y_1)\\) and \\((x_2, y_2)\\) are the graph coordinates of any two points on the line. What is the denominator of the slope function \\((x_2 - x_1)\\) when \\(X\\) is categorical?\nThe solution to using a linear model with categorical \\(X\\) is to recode the factor levels into numbers. An example of this was outlined in Chapter Chapter 1. The value of \\(X\\) for individual mouse i is a number that indicates the treatment assignment – a value of 0 is given to mice with a functional ASK1 gene and a value of 1 is given to mice with a knocked out gene. The regression line goes through the two group means (Figure 8.3). With the (0, 1) coding, \\(\\overline{x}_{ASK1Δadipo} - \\overline{x}_{ASK1F/F} = 1\\), so the denominator of the slope is equal to one and the slope is simply equal to the numerator \\(\\overline{y}_{ASK1Δadipo} - \\overline{y}_{ASK1F/F}\\). The coefficient (which is a slope!) is the difference in conditional means.\n\n\n\n\n\n\nFigure 8.3: Illustration of the slope in a linear model with categorical X. The slope (the coefficient of X) is the difference in conditional means.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>An introduction to linear models</span>"
    ]
  },
  {
    "objectID": "chapters/intro-stat-modeling.html#statistical-models-are-used-for-prediction-explanation-and-description",
    "href": "chapters/intro-stat-modeling.html#statistical-models-are-used-for-prediction-explanation-and-description",
    "title": "8  An introduction to linear models",
    "section": "8.3 Statistical models are used for prediction, explanation, and description",
    "text": "8.3 Statistical models are used for prediction, explanation, and description\nResearchers typically use statistical models to understand relationships between one or more \\(Y\\) variables and one or more \\(X\\) variables. These relationships include\n\nDescriptive modeling. Sometimes a researcher merely wants to describe the relationship between \\(Y\\) and a set of \\(X\\) variables, perhaps to discover patterns. For example, the arrival of a spring migrant bird (\\(Y\\)) as a function of sex (\\(X_1\\)) and age (\\(X_2\\)) might show that males and younger individuals arrive earlier. Importantly, if another \\(X\\) variable is added to the model (or one dropped), the coefficients, and therefore, the precise description, will change. That is, the interpretation of a coefficient as a descriptor is conditional on the other covariates (\\(X\\) variables) in the model. In a descriptive model, there is no implication of causal effects and the goal is not prediction. Nevertheless, it is very hard for humans to discuss a descriptive model without using causal language, which probably means that it is hard for us to think of these models as mere description. Like natural history, descriptive models are useful as patterns in want of an explanation, using more explicit causal models including experiments.\nPredictive modeling. Predictive modeling is very common in applied research. For example, fisheries researchers might model the relationship between population density and habitat variables to predict which subset of ponds in a region are most suitable for brook trout (Salvelinus fontinalis) reintroduction. The goal is to build a model with minimal prediction error, which is the error between predicted and actual values for a future sample. In predictive modeling, the \\(X\\) (“predictor”) variables are largely instrumental – how these are related to \\(Y\\) is not a goal of the modeling, although sometimes an investigator may be interested in the relative importance among the \\(X\\) for predicting \\(Y\\) (for example, collecting the data may be time consuming, or expensive, or enviromentally destructive, so know which subset of \\(X\\) are most important for predicting \\(Y\\) is a useful strategy).\nExplanatory (causal) modeling. Very often, researchers are explicitly interested in how the \\(X\\) variables are causally related to \\(Y\\). The fisheries researchers that want to reintroduce trout may want to develop and manage a set of ponds to maintain healthy trout populations. This active management requires intervention to change habitat traits in a direction, and with a magnitude, to cause the desired response. This model is predictive – a specific change in \\(X\\) predicts a specific response in \\(Y\\) – because the coefficients of the model provide knowledge on how the system functions – how changes in the inputs cause change in the output. Causal interpretation of model coefficients requires a set of strong assumptions about the \\(X\\) variables in the model. These assumptions are typically met in experimental designs but not observational designs.\n\nWith observational designs, biologists are often not very explicit about which of these is the goal of the modeling and use a combination of descriptive, predictive, and causal language to describe and discuss results. Many papers read as if the researchers intend explanatory inference but because of norms within the biology community, mask this intention with “predictive” language. Here, I advocate embracing explicit, explanatory modeling by being very transparent about the model’s goal and assumptions.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>An introduction to linear models</span>"
    ]
  },
  {
    "objectID": "chapters/intro-stat-modeling.html#what-is-the-interpretation-of-a-regression-coefficient",
    "href": "chapters/intro-stat-modeling.html#what-is-the-interpretation-of-a-regression-coefficient",
    "title": "8  An introduction to linear models",
    "section": "8.4 What is the interpretation of a regression coefficient?",
    "text": "8.4 What is the interpretation of a regression coefficient?\nA regression coefficient is the difference in \\(Y\\) that we expect to see if we see a one unit difference in X, but we see no difference in any other covariate (the other X).",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>An introduction to linear models</span>"
    ]
  },
  {
    "objectID": "chapters/intro-stat-modeling.html#what-do-we-call-the-x-and-y-variables",
    "href": "chapters/intro-stat-modeling.html#what-do-we-call-the-x-and-y-variables",
    "title": "8  An introduction to linear models",
    "section": "8.5 What do we call the \\(X\\) and \\(Y\\) variables?",
    "text": "8.5 What do we call the \\(X\\) and \\(Y\\) variables?\nThe inputs to a linear model (the \\(X\\) variables) have many names. In this text, the \\(X\\) variables are typically\n\ntreatment variables – this term makes sense only for categorical variables and is often used for variables that are a factor containing the treatment assignment (for example “control” and “knockout”)\nfactor variables (or simply, factors) – again, this term makes sense only for categorical variables\ncovariates – this term is usually used for the non-focal \\(X\\) variables in a statistical model.\n\nA linear model is a regression model and in regression modeling, the \\(X\\) variables are typically called\n\nindependent variables (often shortened to IV) – “independent” in the sense that in a statistical model at least, the \\(X\\) are not a function of \\(Y\\).\npredictor variables (or simply, “predictors”) – this makes the most sense in prediction models.\nexplanatory variables – this term is usually applied in observational designs and is best used if the explicit goal is causal modeling.\n\nIn this text, the output of a linear model (the \\(Y\\) variable or variables if the model is multivariate) will most often be calle either of\n\nresponse variable (or simply, “response”)\noutcome variable (or simply, “outcome”)\n\nThese terms have a causal connotation in everyday english. These terms are often used in regression modeling with observational data, even if the model is not explicitly causal. One other term, common in introductory textbooks, is\n\ndependent variable – “dependent” in the sense that in a statistical model at least, the \\(Y\\) is a function of the \\(X\\).",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>An introduction to linear models</span>"
    ]
  },
  {
    "objectID": "chapters/intro-stat-modeling.html#modeling-strategy",
    "href": "chapters/intro-stat-modeling.html#modeling-strategy",
    "title": "8  An introduction to linear models",
    "section": "8.6 Modeling strategy",
    "text": "8.6 Modeling strategy\nA “best practice” sequence of steps used throughout this text to analyze experimental data is\n\nexamine the data using exploratory plots to\n\n\nexamine individual points and identify outliers that are likely due to data transcription errors or measurement blunders\nexamine outlier points that are biologically plausible, but raise ref flags about undue influe on fit models. This information is used to inform the researcher on the strategy to handle outliers in the statistical analysis, including algorithms for excluding data or implementation of robust methods.\nprovide useful information for initial model filtering (narrowing the list of potential models that are relevant to the question and data). Statistical modeling includes a diverse array of models, yet almost all methods used by researchers in biology, and all models in this book, are generalizations of the linear model specified in (Equation 8.3). For some experiments, there may be multiple models that are relevant to the question and data. Model checking (step 3) can help decide which model to ultimately use.\n\n\nfit the model, in order to estimate the model parameters and the uncertainty in these estimates.\ncheck the model, which means to use a series of diagnostic plots and computations of model output to check that the fit model reasonably approximates the data. If the diagnostic plots suggest a poor approximation, then choose a different model and go back to step 2.\ninference from the model, which means to use the fit parameters to learn, with uncertainty, about the system, or to predict future observations, with uncertainty.\nplot the model, which means to plot the data, which may be adjusted, and the estimated parameters (or other results dervived from the estimates) with their uncertainty.\n\nNote that step 1 (exploratory plots) is not data mining, or exploring the data for patterns to test.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>An introduction to linear models</span>"
    ]
  },
  {
    "objectID": "chapters/intro-stat-modeling.html#predictions-from-the-model",
    "href": "chapters/intro-stat-modeling.html#predictions-from-the-model",
    "title": "8  An introduction to linear models",
    "section": "8.7 Predictions from the model",
    "text": "8.7 Predictions from the model\nFor the linear model specified in Model (Equation 8.2), the fit model is\n\\[\ny_i = b_0 + b_1 x_i + e_i\n\\tag{8.5}\\]\nwhere \\(b_0\\) and \\(b_1\\) are the coefficients of the fit model and the \\(e_i\\) are the residuals of the fit model. We can use the coefficients and residuals to recover the \\(y_i\\), although this would rarely be done. More commonly, we could use the coefficients to calculate conditional means (the mean conditional on a specified value of \\(X\\)).\n\\[\n\\hat{y}_i = b_0 + b_1 x_i\n\\tag{8.6}\\]\nThe conditional means are typically called fitted values, if the \\(X\\) are the \\(X\\) used to fit the model, or predicted values, if the \\(X\\) are new. “Predicted values” is often shortened to “the prediction”.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>An introduction to linear models</span>"
    ]
  },
  {
    "objectID": "chapters/intro-stat-modeling.html#inference-from-the-model",
    "href": "chapters/intro-stat-modeling.html#inference-from-the-model",
    "title": "8  An introduction to linear models",
    "section": "8.8 Inference from the model",
    "text": "8.8 Inference from the model\nIf our goal is inference, we want to use the fit parameters to learn, with uncertainty, about the system. Using Equation 8.5, the coefficients \\(b_0\\) and \\(b_1\\) are point estimates of the true, generating parameters \\(\\beta_0\\) and \\(\\beta_1\\), the \\(e_i\\) are estimates of \\(\\varepsilon_i\\) (the true, biological “noise”), and \\(\\frac{\\sum{e_i^2}}{N-2}\\) is an estimate of the true, population variance \\(\\sigma^2\\) (this will be covered more in Chapter 10 but you may recognize that \\(\\frac{\\sum{e_i^2}}{N-2}\\) is the formula for a variance). And, using Equation 8.6, \\(\\hat{y}_i\\) is the point estimate of the parameter \\(\\mu_i\\) (the true mean conditional on \\(X=x_i\\)). Throughout this text, Greek letters refer to a theoretical parameter and Roman letters refer to point estimates.\nOur uncertainty in the estimates of the parameters due to sampling is the standard error of the estimate. It is routine to report standard errors of means and coefficients of the model. While a standard error of the estimate of \\(\\sigma\\) is available, this is effectively never reported, at least in the experimental biology literature, presumably because the variance is thought of as a nuisance parameter (noise) and not something worthy of study. This is a pity. Certainly treatments can effect the variance in addition to the mean.\nParametric inference assumes that the response is drawn from some probability distribution (Normal, or Poisson, or Bernouli, etc.). Throughout this text, I emphasize reporting and interpreting point estimates and interval estimates of the point estimate. A confidence interval is a type of interval estimate. A confidence interval of a parameter is a measure of the uncertainty in the estimate. A 95% confidence interval has a 95% probability (in the sense of long-run frequency) of containing the parameter. This probability is a property of the population of intervals that could be computed using the same sampling and measuring procedure. It is not correct, without further assumptions, to state that there is a 95% probability that the parameter lies within the interval. Perhaps a more useful interpretation is that the interval is a compatability interval in that it contains the range of estimates that are compatible with the data, in the sense that a \\(t\\)-test would not reject the null hypothesis of a difference between the estimate and any value within the interval (this interpretation does not imply anything about the true value).\nAnother kind of inference is a significance test, which is the computation of the probability of “seeing the data” or something more extreme than the data, given a specified null hypothesis. This probability is the p-value, which can be reported with the point estimate and confidence interval. There are some reasonable arguments made by very influential statisticians that p-values are not useful and lead researchers into a quagmire of misconceptions that impede good science. Nevertheless, the current methodology in most fields of Biology have developed in a way to become completely dependent on p-values. I think at this point, a p-value can be a useful, if imperfect tool in inference, and will show how to compute p-values throughout this text.\nSomewhat related to a significance test is a hypothesis test, or a Null-Hypothesis Signficance Test (NHST), in which the \\(p\\)-value from a significance test is compared to a pre-specified error rate called \\(\\alpha\\). Hypothesis testing was developed as a formal means of decision making but this is rarely the use of NHST in experimental biology. For almost all applications of p-values that I see in the literature that I read in ecology, evolution, physiology, and wet-bench biology, comparing a \\(p\\)-value to \\(\\alpha\\) adds no value to the communication of the results.\n\n8.8.1 Assumptions for inference with a statistical model\n\nThe data were generated by a process that is “linear in the parameters”, which means that the different components of the model are added together. This additive part of the model containing the parameters is the linear predictor in specifications (Equation 8.2) and (Equation 8.3) above. For example, a cubic polynomial model\n\n\\[\n\\mathrm{E}(Y|X) = \\beta_0 + \\beta_1 X + \\beta_2 X^2 + \\beta_3 X^3\n\\]\nis a linear model, even though the function is non-linear, because the different components are added. Because a linear predictor is additive, it can be compactly defined using matrix algebra\n\\[\n\\mathrm{E}(Y|X) = \\mathbf{X}\\boldsymbol{\\beta}\n\\]\nwhere \\(mathbf{X}\\) is the model matrix and \\(\\boldsymbol{\\beta}\\) is the vector of parameters.\nA Generalized Linear Model (GLM) has the form \\(g(\\mu_i) = \\eta_i\\) where \\(\\eta\\) (the Greek letter “eta”) is the linear predictor\n\\[\\begin{equation}\n\\eta = \\mathbf{X}\\boldsymbol{\\beta}\n\\end{equation}\\]\nGLMs are extensions of linear models. There are non-linear models that are not linear in the parameters, that is, the predictor is not a simple dot product of the model matrix and a vector of parameters. For example, the Michaelis-Menten model is a non-linear model\n\\[\n\\mathrm{E}(Y|X)  = \\frac{\\beta_1 X}{\\beta_2 + X}\n\\]\nthat is non-linear in the parameters because the parts are not added together. This text covers linear models and generalized linear models, but not non-linear models that are also non-linear in the parameters.\n\nThe draws from the probability distribution are independent. Independence implies uncorrelated \\(Y\\) conditional on the \\(X\\), that is, for any two \\(Y\\) with the same value of \\(X\\), we cannot predict the value of one given the value of the other. For example, in the ASK1 data above, “uncorrelated” implies that we cannot predict the glucose level of one mouse within a specific treatment combination given the glucose level of another mouse in that combination. For linear models, this assumption is often stated as “independent errors” (the \\(\\varepsilon\\) in model (Equation 8.2)) instead of independent observations.\n\nThere are lots of reasons that conditional responses might be correlated. In the mouse example, correlation within treatment group could arise if subsets of mice in a treatment group are siblings or are housed in the same cage. More generally, if there are measures both within and among experimental units (field sites or humans or rats) then we’d expect the measures within the same unit to err from the model in the same direction. Multiple measures within experimental units (a site or individual) creates “clustered” observations. Lack of independence or clustered observations can be modeled using models with random effects. These models go by many names including linear mixed models (common in Ecology), hierarchical models, multilevel models, and random effects models. A linear mixed model is a variation of model Equation 8.2. This text introduces linear mixed models in Chapter 14.\nMeasures that are taken from sites that are closer together or measures taken closer in time or measures from more closely related biological species will tend to have more similar values than measures taken from sites that are further apart or from times that are further apart or from species that are less closely related. Space and time and phylogeny create spatial and temporal and phylogenetic autocorrelation. Correlated error due to space or time or phylogeny can be modeled with Generalized Least Squares (GLS) models. A GLS model is a variation of model (Equation 8.2).\n\n\n8.8.2 Specific assumptions for inference with a linear model\n\nConstant variance or homoskedasticity. The most common way of thinking about this is the error term \\(\\varepsilon\\) has constant variance, which is a short way of saying that random draws of \\(\\varepsilon\\) in model (Equation 8.2) are all from the same (or identical) distribution. This is explicitly stated in the second line of model specification (Equation 8.2). If we were to think about this using model specification (Equation 8.3), then homoskedasticity means that \\(\\sigma\\) in \\(N(\\mu, \\sigma)\\) is constant for all observations (or that the conditional probability distributions are identical, where conditional would mean adjusted for \\(\\mu\\))\n\nMany biological processes generate data in which the error is a function of the mean. For example, measures of biological variables that grow, such as lengths of body parts or population size, have variances that “grow” with the mean. Or, measures of counts, such as the number of cells damaged by toxin, the number of eggs in a nest, or the number of mRNA transcripts per cell have variances that are a function of the mean. Heteroskedastic error can be modeled with Generalized Least Squares, a generalization of the linear model, and with Generalized Linear Models (GLM), which are “extensions” of the classical linear model.\n\nNormal or Gaussian probability distribution. As above, the most common way of thinking about this is the error term \\(\\varepsilon\\) is Normal. Using model specification (Equation 8.3), we’d say the conditional probablity distribution of the response is normal. A normal probability distribution implies that 1) the response is continuous and 2) the conditional probability is symmetric around \\(mu_i\\). If the conditional probability distribution has a long left or right tail it is skewed left or right. Counts (number of cells, number of eggs, number of mRNA transcripts) and binary responses (sucessful escape or sucessful infestation of host) are not continuous and often often have asymmetric probablity distributions that are skewed to the right and while sometimes both can be reasonably modeled using a linear model they are more often modeled using generalized linear models, which, again, is an extension of the linear model in equation (Equation 8.3). A classical linear model is a specific case of a GLM.\n\nA common misconception is that inference from a linear model assumes that the raw response variable is normally distributed. Both the error-draw and conditional-draw specifications of a linear model show precisely why this conception is wrong. Model (?eq-lm) states explicitly that it is the error that has the normal distribution – the distribution of \\(Y\\) is a mix of the distribution of \\(X\\) and the error. Model (Equation 8.3) states that the conditional outcome has a normal distribution, that is, the distribution after adjusting for variation in \\(X\\).",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>An introduction to linear models</span>"
    ]
  },
  {
    "objectID": "chapters/intro-stat-modeling.html#linear-modelregression-model-or-statistical-model",
    "href": "chapters/intro-stat-modeling.html#linear-modelregression-model-or-statistical-model",
    "title": "8  An introduction to linear models",
    "section": "8.9 “linear model,”regression model”, or “statistical model”?",
    "text": "8.9 “linear model,”regression model”, or “statistical model”?\nStatistical modeling terminology can be confusing. The \\(X\\) variables in a statistical model may be quantitative (continuous or integers) or categorical (names or qualitative amounts) or some mix of the two. Linear models with all quantitative independent variables are often called “regression models.” Linear models with all categorical independent variables are often called “ANOVA models.” Linear models with a mix of quantitative and categorical variables are often called “ANCOVA models” if the focus is on one of the categorical \\(X\\) or “regression models” if there tend to be many independent variables.\nThis confusion partly results from the history of the development of regression for the analysis of observational data and ANOVA for the analysis of experimental data. The math underneath classical regression (without categorical variables) is the linear model. The math underneath classical ANOVA is the computation of sums of squared deviations from a group mean, or “sums of squares”. The basic output from a regression is a table of coefficients with standard errors. The basic ouput from ANOVA is an ANOVA table, containing the sums of squares along with mean-squares, F-ratios, and p-values. Because of these historical differences in usage, underlying math, and output, many textbooks in biostatistics are organized around regression “vs.” ANOVA, presenting regression as if it is “for” observational studies and ANOVA as if it is “for” experiments.\nIt has been recognized for many decades that experiments can be analyzed using the technique of classical regression if the categorical variables are coded as numbers (again, this will be explained later) and that both regression and ANOVA are variations of a more general, linear model. Despite this, the “regression vs. ANOVA” way-of-thinking dominates the teaching of biostatistics.\nTo avoid misconceptions that arise from thinking of statistical analysis as “regression vs. ANOVA”, I will use the term “linear model” as the general, umbrella term to cover everything in this book. By linear model, I mean any model that is linear in the parameters, including classical regression models, marginal models, linear mixed models, and generalized linear models. To avoid repetition, I’ll also use “statistical model”.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>An introduction to linear models</span>"
    ]
  },
  {
    "objectID": "chapters/continuous_x.html",
    "href": "chapters/continuous_x.html",
    "title": "9  Linear models with a single, continuous X (“linear regression”)",
    "section": "",
    "text": "9.1 Stats 101 What is classical linear regression?\nThe figure above is a scatterplot of the values of relative hepatic slc16a13 (a membrane transporter protein) expression against Hepatic TAG (triacylglycerol) from a sample of 38 humans. The plot shows a trend: humans with higher levels of TAG tend to have higher slc16a13 expression levels and humans with low levels of TAG tend to have lower slc16a13 expression levels. “Trend” and “Tend to” means the relationship isn’t perfect, that there are exceptions. Classical linear regression is a method for measuring this trend. The line through the scatter of points is the linear regression model fit to the data, or simply the “regression line”. It is the line of expected values of \\(Y\\) (slc16a13) given a specific value of \\(X\\) (TAG). The slope of the model is the magnitude of the trend.\nA correlation is a measure of the closeness of the scatter of points to the line and ranges between -1 and 1. If the trend is positive (a regression line that slopes up when moving to the right), the correlation is positive. If the trend is negative (a regression line that slopes down when moving to the right), the correlation is negative. If there is no slope, the correlation is zero. The direction and magnitude of a correlation is denoted by \\(r\\).\nThe p-value in the plot is the probability of randomly sampling 38 pairs of numbers with a slope and correlation as high or higher than the observed slope and correlation, if the true slope and correlation in the sampled population are zero.\nIn most uses of classical linear regression that I see in the experimental bench biology literature, the researchers don’t care about the magnitude of the slope – they simply want to test if two variables are associated (a non-zero slope and correlation) because an association suggests some causal link between the two. And that’s the goal of experimental bench biology, to sleuth out the details of causal links between measured sets of variables.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Linear models with a single, continuous *X* (\"linear regression\")</span>"
    ]
  },
  {
    "objectID": "chapters/continuous_x.html#a-linear-model-with-a-single-continuous-x-is-classical-regression",
    "href": "chapters/continuous_x.html#a-linear-model-with-a-single-continuous-x-is-classical-regression",
    "title": "9  Linear models with a single, continuous X (“linear regression”)",
    "section": "9.2 A linear model with a single, continuous X is classical “regression”",
    "text": "9.2 A linear model with a single, continuous X is classical “regression”\n\n9.2.1 Analysis of “green-down” data\nTo introduce some principles of modeling with a single continuous \\(X\\) variable, I’ll use a dataset from\nRichardson, A.D., Hufkens, K., Milliman, T. et al. Ecosystem warming extends vegetation activity but heightens vulnerability to cold temperatures. Nature 560, 368–371 (2018).\nwhich has nothing to do with experimental bench biology using mouse models but does have a really really nice dataset for understanding classical linear regression.\nSource data\nThe data are from a long-term experiment on the effects of warming and CO2 on a high-carbon northern temperate peatland and is the focal dataset of the study. The experiment involves 10 large, temperature and CO2 controlled enclosures. CO2 is set to 400 ppm in five enclosures and 900 ppm in five enclosures. Temperature of the five enclosures within each CO2 level is set to 0, 2.25, 4.5, 6.75, or 9 °C above ambient temperature. The multiple temperature levels is a regression design, which allows a researcher to measure non-linear effects. Read more about the experimental design and the beautiful implementation.\nThe question pursued is in this study is, what is the causal effect of warming on the timing (or phenology) of the transition into photosynthetic activity (“green-up”) in the spring and of the transition out of photosynthetic activity (“green-down”) in the fall? The researchers measured these transition dates, or Day of Year (DOY), using foliage color. Here, we focus on the transition out of photosynthesis or “green-down” DOY.\nImport the data\n\nExamine the data\n\n\ngg1 &lt;- qplot(x = temperature,\n      y = transition_date,\n      data = fig2c) +\n  geom_smooth(method = \"lm\")\n\nWarning: `qplot()` was deprecated in ggplot2 3.4.0.\n\ngg2 &lt;- qplot(x = temperature,\n      y = transition_date,\n      data = fig2c) +\n  geom_smooth(method = \"lm\", formula = y ~ poly(x, 2))\ngg3 &lt;- qplot(x = temperature,\n      y = transition_date,\n      data = fig2c) +\n  geom_smooth()\nplot_grid(gg1, gg2, gg3, ncol=3)\n\n\n\n\n\n\n\n\nNo plot shows any obvious outlier that might be due to measurement blunders or curation error. The linear regression in the left-most plot clearly shows that a linear response is sufficient to capture the effect of temperature on day of green-down.\n\nchoose a model. Because the \\(X\\) variable (\\(temperature\\)) was experimentally set to five levels, the data could reasonably be modeled using either a linear model with a categorical \\(X\\) or a linear model with a continuous \\(X\\). The advantage of modeling \\(temperature\\) as a continuous variable is that there is only one effect, the slope of the regression line. If modeled as a categorical factor with five levels, there are, at a minimum, four interesting effects (the difference in means between each non-reference level and reference (temperature = 0) level). Also, for inference, modeling \\(temperature\\) as a continuous variable increases power for hypothesis tests.\nfit the model\n\n\n# Step 1: fit the model\nm1 &lt;- lm(transition_date ~ temperature, data = fig2c)\n\n\ncheck the model\n\n\n# check normality assumption\nset.seed(1)\nqqPlot(m1, id=FALSE)\n\n\n\n\n\n\n\n\nThe Q-Q plot indicates the distribution of residuals is well within that expected for a normal sample and there is no cause for concern with inference.\n\n# check homogeneity assumption\nspreadLevelPlot(m1, id=FALSE)\n\n\n\n\n\n\n\n\n\nSuggested power transformation:  0.6721303 \n\n\nThe spread-location plot shows no conspicuous trend in how the spread changes with the conditonal mean. There is no cause for concern with inference.\n\ninference from the model\n\n\nm1_coeff &lt;- summary(m1) |&gt;\n  coef()\nm1_confint &lt;- confint(m1)\nm1_coeff &lt;- cbind(m1_coeff, m1_confint)\nm1_coeff\n\n              Estimate Std. Error   t value     Pr(&gt;|t|)      2.5 %     97.5 %\n(Intercept) 289.458750  3.0593949 94.613071 1.738650e-13 282.403773 296.513728\ntemperature   4.982745  0.5541962  8.990941 1.866888e-05   3.704767   6.260724\n\n\nThe effect of added temperature on the day of green-down is 4.98 d per 1 °C (95% CI: 3.7, 6.3; p &lt; 0.001).\n\nplot the model\n\n\n\n\n\n\nModification of the published Figure 2c showing the experimental effect of warming on the date of autumn green-down (the transition to fall foliage color) in a mixed shrub community. The bottom panel is a scatterplot. The regression line shows the expected value of Y (transition day of year) given a value of X (added temperature). The slope of the regression line is the estimate of the effect. The estimate and 95% confidence interval of the estimate are given in the top panel.\n\n\n\n\n\nReport the results\n\nThe modeled effect of added temperature is Slope: 4.98 (3.7, 6.26) d per 1 °C (@ref(fig:continuous-x-plot)).\n\n\n9.2.2 Learning from the green-down example\nFigure @ref(fig:continuous-x-plot) is a scatterplot with the green-down DOY for the mixed-shrub community on the \\(Y\\) axis and added temperature on the \\(X\\) axis. The line through the data is a regression line, which is the expected value of Y (green-down DOY) given a specific value of X (added temperature). The slope of the line is the effect of added temperature on timing of green-down. The intercept of the regression line is the value of the response (day of green-down) when \\(X\\) is equal to zero. Very often, this value is not of interest although the value should be reported to allow predictions from the model. Also very often, the value of the intercept is not meaningful because a value of \\(X = 0\\) is far outside the range of measured \\(X\\), or the value is absurd because it is impossible (for example, if investigating the effect of body weight on metabolic rate, the value \\(weight = 0\\) is impossible).\nThe intercept and slope are the coefficients of the model fit to the data, which is\n\\[\\begin{equation}\nday_i = b_0 + b_1 temperature_i + e_i\n(\\#eq:continuous-x-fit)\n\\end{equation}\\]\nwhere day is the day of green-down, temperature is the added temperature, and i refers (or “indexes”) the ith enclosure. This model completely reconstructs the day of green-down for all ten enclosures. For example, the day of green-down for enclosure 8 is\n\\[\\begin{equation}\n332 = 289.458750 + 4.982745 \\times 6.73 + 9.00737\n\\end{equation}\\]\nThe coefficients in the model are estimates of the parameters of the generating model fit to the data\n\\[\\begin{align}\nday &= \\beta_0 + \\beta_1 temperature + \\varepsilon\\\\\n\\varepsilon &\\sim N(0, \\sigma^2)\n(\\#eq:continuous-x-m1)\n\\end{align}\\]\nA generating model of the data is used to make inference, for example, a measure of uncertainty in a prediction in the timing of green-down with future warming, or a measure of uncertainty about the effect of temperature on green-down.\n\n\n9.2.3 Using a regression model for “explanation” – causal models\nIn this text, “explanatory” means “causal” and a goal of explanatory modeling is the estimate of causal effects using a causal interpretation of the linear model (=regression) coefficients. “What what? I learned in my stats 101 course that we cannot interpret regression coefficients causally”.\nStatisticians (and statistics textbooks) have been quite rigid that a regression coefficient has a descriptive (or “observational”, see below) interpretation but not a causal interpretation. At the same time, statisticians (and statistics textbooks) do not seem to have any issue with interpeting the modeled effects from an experiment causally, since this is how they are interpreted. But if the modeled effects are simply coefficients from a linear (= regression) model, then this historical practice is muddled.\nPart of this muddled history arises from the use of “regression” for models fit to observational data with one or more continuous \\(X\\) variables and the use of “ANOVA” for models fit to experimental data with one or more categorical \\(X\\). This separation seems to have blinded statisticians from working on the formal probabilistic statements underlying causal interpretations of effect estimates in ANOVA and the synthesis of these statements with the probabilistic statements in regression modeling. Two major approaches to developing formal, probabilistic statements of causal modeling in statistics are the Rubin causal model and the do-operator of Pearl. Despite the gigantic progress in these approaches, little to none of this has found its way into biostatistics textbooks.\n\n9.2.3.1 What a regression coefficient means\n\nDescription: when we saw a one unit larger value in X, we saw, on average, a difference of b in Y.\nPrediction: when we see a one unit larger value in X, we expect to see a difference of b in Y.\nExplanation (causation): when we set X to have one unit larger value, we expect Y to change by b.\n\nA linear (“regression”) model coefficient, such as the coefficient for temperature, \\(\\beta_1\\), has three interpretations, an observational interpretation, a predictive interpretation, and a causal interpretation. This text is about causal interpretations, but to explain what is meant by this, I need to clarify the differences between causal and observational interpretations.\nTo clarify the differences, it’s useful to remember that an expected value from a regression model is a conditional mean\n\\[\\begin{equation}\n\\textrm{E}[day|temperature] = \\beta_0 + \\beta_1 temperature\n(\\#eq:continuous-x-conditional-mean)\n\\end{equation}\\]\nIn words “the expected value of day conditional on temperature is beta-knot plus beta-one times temperature”. An expected value is a long run average – if we had an infinite number of enclosures with \\(temperature=x\\) (where \\(x\\) is a specific value of added temperature, say 2.5 °C), the average \\(day\\) of these enclosures is \\(\\beta_0 + \\beta_1 x\\).\nThe parameter \\(\\beta_1\\) is a difference in conditional means.\n\\[\\begin{equation}\n\\beta_1 = \\textrm{E}[day|temperature = x+1] - \\textrm{E}[day|temperature = x]\n(\\#eq:beta)\n\\end{equation}\\]\nIn words, “beta-one is the expected value of day of green-down when the temperature equals x + 1 minus the expected value of day of green-down when the temperature equals x.” A very short way to state this is “beta-one is a difference in conditional means”.\n\ntl;dr. Note that the “+ 1” in this definition is mere convenience. Since the slope of a line is \\(\\frac{y_2 - y_1}{x_2 - x_1}\\), where (\\(x_1\\), \\(y_1\\)) and (\\(x_2\\), \\(y_2\\)) are the coordinates of any two points on the line, it is convenient to choose two points that differ in \\(X\\) by one unit, which makes the fraction equal to the numerator only. The numerator is a difference in conditional means. It is also why the units of a regression coefficient are “per unit of \\(X\\) even if defined as a difference in two \\(Y\\) values.\n\nThe difference between observational and causal interpretations of \\(\\beta_1\\) depends on the “event” conditioned on in \\(\\textrm{E}[day|temperature]\\). Let’s start with the causal interpretation, which is how we should think about the regression coefficients in the green-down experiment.\n\n\n9.2.3.2 Causal interpretation – conditional on “doing” X = x\nIn the causal interpretation of regression, \\(\\textrm{E}[day|temperature]\\) is conditioned on “doing” a real or hypothetical intervention in the system where we set the value of \\(temperature\\) to a specific value \\(x\\) (“\\(temperature=x\\)), while keeping everything else about the system the same. This can be stated explicitly as \\(\\textrm{E}[day|\\;do\\;temperature = x]\\). Using the do-operator, we can interpret \\(\\beta_1\\) as an effect coefficient.\n\\[\\begin{equation}\n\\beta_1 = \\textrm{E}[day|\\;do\\;temperature = x+1] - \\textrm{E}[day|\\;do\\;temperature = x]\n\\end{equation}\\]\nIn words, “beta-one is the expected value of day of green-down were we to set the temperature to x + 1, minus the expected value of day of green-down were we to set the temperature to x.”\n\ntl;dr. In the green-down experiment, the researchers didn’t set the temperature in the intervened enclosures to the ambient temperature + 1 but to ambient + 2.25, ambient + 4.5, ambient + 6.75, and ambient + 9.0. Again (see tl;dr above), the + 1 is mere convenience in the definition.\n\n\n\n9.2.3.3 Observational interpretation – conditional on “seeing” X = x.\nIn the observational interpretation of regression, \\(\\textrm{E}[day|temperature]\\) is conditioned on sampling data and “seeing” a value of \\(temperature\\). We can state this explicitly as \\(\\textrm{E}[day|\\;see\\;temperature]\\). From this, we can interpret \\(\\beta_1\\) as an observational coefficient\n\\[\\begin{equation}\n\\beta_1 = \\textrm{E}[day|\\;see\\;temperature = x+1] - \\textrm{E}[day|\\;see \\;temperature = x]\n\\end{equation}\\]\nIn words, “beta-one is the expected value of day of green-down when see that temperature equals x + 1 minus the expected value of day of green-down when we see that temperature equals x.” To understand what I mean by “observational”, let’s imagine that the green-down data do not come from an experiment in which the researchers intervened and set the added temperature to a specifc value but from ten sites that naturally vary in mean annual temperature. Or from a single site with 10 years of data, with some years warmer and some years colder. Data from this kind of study is observational – the researcher didn’t intervene and set the \\(X\\) values but merely observed the \\(X\\) values.\nIf we sample (or “see”) a site with a mean annual temperature that is 2.5 °C above a reference value, the expected day of green-down is \\(\\textrm{E}[day|temperature = 2.5 °C]\\). That is, values near \\(\\textrm{E}[day|temperature = 2.5 °C]\\) are more probable than values away from \\(\\textrm{E}[day|temperature = 2.5 °C]\\). Or, if the only information that we have about this site is a mean annual temperature that is 2.5 °C above some reference, then our best prediction of the day of green-down would be \\(\\textrm{E}[day|temperature = 2.5 °C]\\). We do not claim that the 4.98 day delay in green-down is caused by the warmer temperature, only that this is the expected delay relative to the reference having seen the data.\nThe seeing interpretation of a regression coefficient is descriptive– it is a description of a mathematical relationship. In this interpretation, the coefficient is not causal in the sense of what the expected response in \\(Y\\) would be were we to intervene in the system and change \\(X\\) from \\(x\\) to \\(x+1\\).\n\n\n9.2.3.4 Omitted variable bias\nWhat is the consequence of interpreting a regression coefficient causally instead of observationally?\n\n\n\n\n\na Directed Acyclic (or causal) Graph of a hypothetical world where the day of green-down is caused by two, correlated environmental variables, temperature and moisture, and to a noise factor (U) that represents an unspecified set of additional variables that are not correlated to either temperature or moisture.\n\n\n\n\nLet’s expand the thought experiment where we have an observational data set of green down dates. In this thought experiment, only two variables systematically affect green-down DOY. The first is the temperature that the plants experience; the effect of \\(temperature\\) is \\(\\beta_1\\). The second is the soil moisture that the plants experience; the effect of \\(moisture\\) is \\(\\beta_2\\). \\(temperature\\) and \\(moisture\\) are correlated with a value \\(\\rho\\). This causal model is graphically represented by the causal graph above.\nLets fit two linear models.\n\\[\\begin{align}\n(\\mathcal{M}_1)\\; day &= b_0 + b_1 \\; temperature + b_2 \\; moisture + \\varepsilon\\\\\n(\\mathcal{M}_2)\\; day &= b_0 + b_1 \\; temperature + \\varepsilon\n\\end{align}\\]\n\\(\\mathcal{M}_1\\) the linear model including both \\(temperature\\) and \\(moisture\\) as \\(X\\) variables and \\(\\mathcal{M}_2\\) the linear model including only \\(temperature\\) as an \\(X\\) variable. Given the true causal model above, \\(b_1\\) is an unbiased estimate of the true causal effect of temperature \\(\\beta_1\\) in \\(\\mathcal{M}_1\\) because the expected value of \\(b_1\\) is \\(\\beta_1\\). By contrast, \\(b_1\\) is a biased estimate of the true causal effect of temperature \\(\\beta_1\\) in \\(\\mathcal{M}_2\\). The expected value of \\(b_1\\) in \\(\\mathcal{M}_2\\) is the true, causal effect plus a bias term.\n\\[\\begin{equation}\n\\mathrm{E}(b_1|\\mathcal{M}_1) = \\beta + \\rho \\alpha \\frac{\\sigma_{moisture}}{\\sigma_{temperature}}\n\\end{equation}\\]\nThis bias (\\(\\rho \\alpha \\frac{\\sigma_{moisture}}{\\sigma_{temperature}}\\)) is omitted variable bias. The omitted variable \\(moisture\\) is an unmeasured, confounding variable. A variable \\(X_2\\) is a confounder for variable \\(X_1\\) if \\(X_2\\) has both a correlation with \\(X_1\\) and a path to the response \\(Y\\) that is not through \\(X_1\\). With ommitted variable bias, as we sample more and more data, our estimate of the effect doesn’t converge on the truth but the truth plus some bias.\n\n\n9.2.3.5 Causal modeling with experimental versus observational data\nCausal interpretation requires conditioning on “doing X=x”. For the green-down data, “doing X=x” is achieved by the random treatment assignment of the enclosures. How does random treatment assignment achieve this? Look again at Figure @ref(fig:continuous-x-dag). If the values of \\(treatment\\) are randomly assigned, then, by definition, the expected value of the correlation between \\(treatment\\) and \\(moisture\\) is zero. In fact, the expected value of the correlation between \\(treatment\\) and any measurable variable at the study site is zero. Given, this, the expected value of the regression coefficient for \\(temperature\\) (\\(b_1\\)) is \\(\\beta\\) because \\(\\rho=0\\). That is, the estimate of the true effect is unbiased. It doesn’t matter if \\(moisture\\), or any other variable, is excluded from the model. (That said, we may want to include moisture or other variables in the model to increase precision of the causal effect. This is addressed in the chapter “Models with Covariates”). This is what an experiment does and why experiments are used to answer causal questions.\nCan we use observational data for causal modeling? Yes, but the methods for this are outside of the scope of this text. The mathematical foundation for this is known as path analysis, which was developed by geneticist and evolutionary biologist Sewell Wright (I include this because this work has inspired much of how I think about biology and because he is both my academic grandfather and great-grandfather). Causal analysis of observational data in biology is rare outside of ecology and epidemiology. Good starting points for the modern development of causal analysis are Hernán MA and Robins JM (2020) and Burgess et al. (2018). A gentle introduction is Pearl and Mackenzie (2018).\n\n\n\n9.2.4 Using a regression model for prediction – prediction models\nThe researchers in the green-down study also presented estimates of the effect of temperature on green-down using two observational datasets. Let’s use the one in Extended Data Figure 3d to explore using a regression model for prediction. The data are taken from measurements of the day of green-down over an 86 year period at a single site. The response variable is \\(green\\_down\\_anomaly\\) (the difference between the day of green down for the year and the mean of these days over the 86 years). The predictor variable is \\(autumn\\_temp\\_anomaly\\) (the difference between the mean temperature over the year and the mean of these means).\n\n\nWarning: The dot-dot notation (`..rr.label..`) was deprecated in ggplot2 3.4.0.\nℹ Please use `after_stat(rr.label)` instead.\nℹ The deprecated feature was likely used in the ggpubr package.\n  Please report the issue at &lt;https://github.com/kassambara/ggpubr/issues&gt;.\n\n\n\n\n\n\n\n\n\nThe plot in figure @ref(fig:continuous-x-pred-fig) shows the regression line of a linear model fit to the data. Two statistics are given in the plot.\n\nThe p-value of the slope (the coefficient \\(b_1\\) of \\(autumn\\_temp\\_anomaly\\))\nThe \\(R^2\\) of the model fit.\n\nIn addition, two intervals are shown.\n\n95% confidence interval of the expected values in blue. The width of this band is a measure of the precision of the estimates of expected values.\n95% prediction interval of the predictions in gray. The width of this band is a measure of how well the model predicts.\n\nIt is important that researchers knows what each of these statistics and bands are, when to compute them and when to ignore them them, and what to say about each when communicating the results.\n\n9.2.4.1 95% CI and p-value\nBoth the 95% confidence interval of the expected values and the p-value are a function of the standard error of the slope coefficient \\(b_1\\) and so are complimentary statistics. The p-value is the probability of sampling the null that results in a \\(t\\)-value as or more extreme than the observed t for the coefficient \\(b_1\\). This null includes the hypothesis \\(\\beta_1 = 0\\). The 95% confidence interval of the expected values is the band containing expected values (mean \\(green\\_down\\_anomaly\\) conditional on \\(autumn\\_temp\\_anomaly\\)) that are compatible with the data. There are a couple of useful ways of thinking about this band.\n\nThe band captures an infinite number of regression lines that are compatible with the data. Some are more steep and predict smaller expected values at the low end of \\(autumn\\_temp\\_anomaly\\) and higher expected values at the high end of \\(autumn\\_temp\\_anomaly\\). Others are less steep and predict higher expected values at the low end of \\(autumn\\_temp\\_anomaly\\) and lower expected values at the high end of \\(autumn\\_temp\\_anomaly\\).\nThe band captures the 95% CI of the conditional mean at every value of \\(X\\). Consider the 95% CI of the conditional mean at the mean value of \\(X\\). As we move away from this mean value (lower to higher \\(X\\)), the 95% CI of the conditional mean increases.\n\nA 95% CI and p-value are useful statistics to report if the purpose is causal modeling, as in the example above using the experimental green-down data (where the 95% CI was not presented as a confidence band of the expected values but a CI of the estimate of the effect of added temperature). A 95% CI and p-value are also useful statistics to report if the purpose is descriptive modeling, simply wanting to know how the conditional mean of the response is related to an \\(X\\) variable.\n\n\n9.2.4.2 95% prediction interval and \\(R^2\\)\nBoth the \\(R^2\\) and the 95% prediction interval are a function of the population variability of \\(green\\_down\\_anomaly\\) conditional on \\(autumn\\_temp\\_anomaly\\) (the spread of points along a vertical axis about the regression line) and are complimentary statistics. Briefly,\n\nthe \\(R^2\\) is a measure of the fraction of the variance in the response that is accounted by the model (some sources say “explained by the model” but this is an odd use of “explain”). It is a number between 0 and 1 and is a measure of “predictability” if the goal is prediction modeling.\nThe 95% prediction interval will contain a new observation 95% of the time. It provides bounds on a prediction – given a new observation, there is a 95% probability that the interval at \\(x_{new}\\) will contain \\(y_{new}\\).\n\nTo understand \\(R^2\\) and the 95% prediction interval a bit better, let’s back up.\n\\[\\begin{equation}\ngreen\\_down\\_anomaly_i = b_0 + b_1 autumn\\_temp\\_anomaly_i + e_i\n(\\#eq:doy_fit)\n\\end{equation}\\]\nModel @ref(eq:eq:doy_fit) recovers the measured value of \\(green\\_down\\_anomaly\\) for any year, given the \\(autumn\\_temp\\_anomaly\\) for the year. The equation includes the linear predictor (\\(b_0 + b_1 autumn\\_temp\\_anomaly_i\\)) and the residual from the predictor (\\(e_i\\)). The predictor part of @ref(eq:doy_fit) is used to compute \\(\\hat{y}\\) (“y hat”).\n\\[\\begin{equation}\n\\hat{y}_i = b_0 + b_1 autumn\\_temp\\_anomaly_i\n(\\#eq:doy_hat)\n\\end{equation}\\]\nThe \\(\\hat{y}\\) are fitted values, if the values are computed from the data that were used for the fit, or predicted values (or simply the prediction), if the values are computed from values of the predictor variables outside the set used to fit the model. For the purpose of plotting, generally, with models with categorical factors as \\(X\\), I prefer either modeled values or conditional means to fitted values.\n\n\n9.2.4.3 \\(R^2\\)\nA good model accounts for a sizable fraction of the total variance in the response. This fraction is the \\(R^2\\) value, which is given in summary(m1) and accessed directly with\n\nsummary(m1)$r.squared\n\n[1] 0.1305754\n\n\n\\(R^2\\) is the fraction of the total variance of \\(Y\\) that is generated by the linear predictor.\n\\[\\begin{equation}\nR^2 = \\frac{\\mathrm{VAR}(\\hat{y})}{\\mathrm{VAR}(y)}\n\\end{equation}\\]\n\nyhat &lt;- fitted(m1)\ny &lt;- efig_3d[, green_down_anomaly]\nvar(yhat)/var(y)\n\n[1] 0.1305754\n\n\n\\(R^2\\) will vary from zero (the model accounts for nothing) to one (the model accounts for everything). \\(R^2\\) is often described as the fraction of total variation explained by the model” but the usage of “explain” is observational and not causal. Because of the ambiguous usage of “explain” in statistics, I prefer to avoid the word.\nIt can be useful sometimes to think of \\(R^2\\) in terms of residual error, which is the variance of the residuals from the model. The larger the residual error, the smaller the \\(R^2\\), or\n\\[\\begin{equation}\nR^2 = 1 - \\frac{\\mathrm{VAR}(e)}{\\mathrm{VAR}(y)}\n\\end{equation}\\]\n\ne &lt;- residuals(m1)\ny &lt;- efig_3d[, green_down_anomaly]\n1 - var(e)/var(y)\n\n[1] 0.1305754\n\n\nThe smaller the residuals, the higher the \\(R^2\\) and the closer the predicted values are to the measured values. The sum of the model variance and residual variance equals the total variance and, consequently, \\(R^2\\) is a signal to signal + noise ratio. The noise in \\(R^2\\) is the sampling variance of the individual measures. The noise in a t-value is the sampling variance of the parameter (for m1, this is the sampling variance of \\(b_1\\)). This is an important distinction because it means that t and \\(R^2\\) are not related 1:1. In a simple model with only a single \\(X\\), one might expect \\(R^2\\) to be big if the p-value for the slope is tiny, but this isn’t necessarily true because of the different meaning of noise in each. A study with a very large sample size \\(n\\) can have a tiny p-value and a small \\(R^2\\). A p-value is not a good indicator of predictability. \\(R^2\\) is. This is explored more below.\n\n\n9.2.4.4 Prediction interval\nA good prediction model has high predictability, meaning the range of predicted values is narrow. A 95% CI is a reasonable range to communicate. For any decision making using prediction, it is better to look at numbers than a band on a plot.\n\n\n\n\n\nAutumn Temp Anomaly (°C)\nExpected (days)\n2.5% (days)\n97.5% (days)\n\n\n\n\n0.5\n0.8\n-8.7\n10.3\n\n\n1.0\n1.6\n-7.9\n11.1\n\n\n1.5\n2.4\n-7.2\n12.0\n\n\n2.0\n3.2\n-6.4\n12.8\n\n\n2.5\n4.0\n-5.7\n13.7\n\n\n\n\n\nGiven these data and the fit model, if we see a 2 °C increase in mean fall temperature, we expect the autumn green-down to extend 3.2 days. This expectation is an average. We’d expect 95% of actual values to range between -6.4 days and 12.8 days. This is a lot of variability. Any prediction has a reasonable probability of being over a week early or late. This may seem surprising given the p-value of the fit, which is 0.0006. But the p-value is not a reliable indicator of predictability. It is a statistic related to the blue band, not the gray band.\nTo understand this prediction interval a bit more, and why a p-value is not a good indicator of predictability, it’s useful to understand what makes a prediction interval “wide”. The width of the prediction interval is a function of two kinds of variability\n\nThe variance of the expected value at a specific value of \\(X\\). This is the square of the standard error of \\(b_1\\). The blue band is communicating the CI based on this variance. The p-value is related to the wideth of this band.\nThe variance of \\(Y\\) at a specific value of \\(X\\). This variance is \\(\\sigma^2\\). It is useful for learning to think about the equation for the estimate of this variance.\n\n\\[\\begin{equation}\n\\hat\\sigma^2 = \\frac{\\sum{e_i^2}}{N-2}\n\\end{equation}\\]\nAgain, \\(e_i\\) is the residual for the ith case. The denominator (\\(N-2\\)) is the degrees of freedom of the model. Computing \\(\\hat\\sigma^2\\) manually helps to insure that we understand what is going on.\n\nsummary(m1)$sigma # R's calculation of sigma hat\n\n[1] 4.736643\n\ndf &lt;- summary(m1)$df[2] # R's calculation of df. Check that this is n-2!\nsqrt(sum(e^2)/df)\n\n[1] 4.736643\n\n\nRemember that an assumption of the linear models that we are working with at this point is, this variance is constant for all values of \\(X\\), so we have a single \\(\\sigma\\). Later, we will cover linear models that model heterogeneity in this variance. \\(\\sigma\\) is a function of variability in the population – it is the population standard deviation conditional on \\(X\\).\nImportantly, predictability is a function of both these components of variability. As a consequence, it is \\(R^2\\), and not the p-value, that is the indicator of predictability. In the observational green-down data, even if we had thousands of years of data, we would still have a pretty low \\(R^2\\) because of the population variability of \\(green\\_down\\_anomaly\\) at a given \\(autumn\\_temp\\_anomaly\\).\n\n\n9.2.4.5 Median Absolute Error and Root Mean Square Error are absolute measures of predictability\nA p-value is not at all a good guide to predictability. \\(R^2\\) is proportional to predictability but is not really useful in any absolute sense. If we want to predict the effect of warming on the day of green-down, I would like to have a measure of predictability in the units of green-down, which is days. The prediction interval gives this for any value of \\(X\\). But what about an overall measure of predictability?\nThree overall measures of predictability are\n\n\\(\\hat{\\sigma}\\), the estimate of \\(\\sigma\\). This is the standard deviation of the sample conditional on \\(X\\).\n\\(RMSE\\), the root mean squared error. This is\n\n\\[\\begin{align}\nSSE &= \\sum{(y_i - \\hat{y}_i)^2}\\\\\nMSE &= \\frac{SSE}{N}\\\\\nRMSE &= \\sqrt{MSE}\n\\end{align}\\]\n\\(SSE\\) (“sum of squared error”) is the sum of the squared residuals (\\(y_i - \\hat{y}_i\\)) of the model. \\(MSE\\) (“mean squared error”) is the mean of the squared errors. \\(RMSE\\) is the square root of the mean squared error. \\(RMSE\\) is almost equal to \\(\\hat{\\sigma}\\). The difference is the denominator, which is \\(N\\) in the computation of \\(RMSE\\) and \\(df\\) (the degrees of freedom of the model, which is \\(N\\) minus the number of fit parameters) in the computation of \\(\\hat{\\sigma}\\).\n\n\\(MAE\\), the mean absolute error. This is\n\n\\[\\begin{equation}\nMAE = \\frac{1}{N}\\sum{|y_i - \\hat{y}_i|}\n\\end{equation}\\]\n\n\n[1] 4.681241\n\n\n\n\n\nsigma\nRMSE\nMAE\n\n\n\n\n4.74\n4.68\n3.58\n\n\n\n\n\nIf the goal of an analysis is prediction, one of these statistics should be reported. For the model fit to the observational green-down data in Extended Figure 3d, these three statistics are given in Table @ref(table:continuous-x-average-error) above (to two decimal places simply to show numerical difference between \\(\\hat{\\sigma}\\) and \\(RMSE\\)). All of these are measures of an “average” prediction error in the units of the response. The average error is either 4.7 or 3.6 days, depending on which statistic we report. Why the difference? Both \\(\\hat{\\sigma}\\) and \\(RMSE\\) are functions of squared error and so big differences between predicted and actual value are emphasized. If an error of 6 days is more than twice as bad than an error of 3 days, report \\(RMSE\\). Why \\(RMSE\\) and not \\(\\hat{\\sigma}\\)? Simply because researchers using prediction models are more familiar with \\(RMSE\\). If an error of 6 days is not more than twice as bad than an error of 3 days, report \\(MAE\\).\n\n\n9.2.4.6 Prediction modeling is more sophisticated than presented here\nFor data where the response is a non-linear function of the predictor, or for data with many predictor variables, researchers will often build a model using a model selection method. Stepwise regression is a classical model selection method that is commonly taught in intro biostatistics and commonly used by researchers. Stepwise regression as a method of model selection has many well-known problems and should be avoided.\nSome excellent books that are useful for building models and model selection are\n\nThe Elements of Statistical Learning\nRegression and Other Stories\nRegression Modeling Strategies\n\n\n\n\n9.2.5 Using a regression model for creating a new response variable – comparing slopes of longitudinal data\nIn the study in this example, the researchers compared the growth rate of tumors in mice fed normal chow versus mice with a methionine restricted diet. Growth rate wasn’t actually compared. Instead, the researchers used a t-test to compare the size of the tumor measured at six different days. A problem with multiple t-tests for this dataset is that the errors (residuals from the model) on one day are correlated with the errors from another day because of the repeated measures on each mouse. This correlation will inflate Type I error rate.\nInstead of six t-tests, a better strategy for these data is to use a regression to calculate a tumor growth rate for each mouse. There are sixteen mice so this is sixteen fit models. Here I use a “for loop” to fit the model to the data from a single mouse and use the slope (\\(b_1\\)) as the estimate of the tumor growth rate for that mouse.\nUse a for-loop to estimate growth rate for each mouse. In each pass through the loop\n\nthe subset of fig1f (the data in long format) belonging to mouse i is created\nthe linear model volume ~ day is fit to the subset\nthe coefficient of day (the slope, \\(b_1\\)) is inserted in mouse i’s row in the column “growth” in the data.table “fig1f_wide”.\n\nAt the end of the loop, we have the data.table fig1f_wide which has one row for each mouse, a column for the treatment factor (diet) and a column called “growth” containing each mouse’s growth rate. There are also columns of tumor volume for each mouse on each day but these are ignored.\n\nN &lt;- nrow(fig1f_wide)\nid_list &lt;- fig1f_wide[, id]\nfor(i in 1:N){\n  mouse_i_data &lt;- fig1f[id == id_list[i]] # subset\n  fit &lt;- lm(volume ~ day, data = mouse_i_data)\n  fig1f_wide[id == id_list[i], growth := coef(fit)[2]]\n}\n# View(fig1f_wide)\n# qplot(x = treatment, y = growth, data = fig1f_wide)\n# qplot(x = day, y = volume, color = treatment, data = fig1f) + geom_smooth(aes(group = id), method = \"lm\", se = FALSE)\n\nStep 3. fit the model\n\nm1 &lt;- lm(growth ~ treatment, data = fig1f_wide)\n\nStep 5. inference from the model\n\nm1_coef &lt;- summary(m1) |&gt;\n  coef()\nm1_ci &lt;- confint(m1)\n(m1_coef_table &lt;- cbind(m1_coef, m1_ci))\n\n             Estimate Std. Error   t value     Pr(&gt;|t|)     2.5 %    97.5 %\n(Intercept)  52.63406   3.102096 16.967258 9.865155e-11  45.98072  59.28739\ntreatmentMR -23.57616   4.387026 -5.374065 9.809457e-05 -32.98540 -14.16693\n\n\nStep 6. plot the model\n\n\nWarning: A numeric `legend.position` argument in `theme()` was deprecated in ggplot2\n3.5.0.\nℹ Please use the `legend.position.inside` argument of `theme()` instead.\n\n\n\n\n\n\n\n\n\n\n\n\n\n9.2.6 Using a regression model for for calibration",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Linear models with a single, continuous *X* (\"linear regression\")</span>"
    ]
  },
  {
    "objectID": "chapters/continuous_x.html#working-in-r",
    "href": "chapters/continuous_x.html#working-in-r",
    "title": "9  Linear models with a single, continuous X (“linear regression”)",
    "section": "9.3 Working in R",
    "text": "9.3 Working in R\n\n9.3.1 Fitting the linear model\nA linear model is fit with the lm function, which is very flexible and will be a workhorse in much of this text.\n\nm1 &lt;- lm(transition_date ~ temperature, data = fig2c)\n\nm1 is an lm model object that contains many different kinds information, such as the model coefficients.\n\ncoef(m1)\n\n(Intercept) temperature \n 289.458750    4.982745 \n\n\nWe’ll return to others, but first, let’s explore some of the flexibility of the lm function. Two arguments were sent to the function\n\nthe model formula transition_date ~ temperature with the form Y ~ X, where Y and X are names of columns in the data.\n\n\nThe model formula itself can be assigned to a variable, which is useful when building functions. An example\n\n\ncoef_table &lt;- function(x, y, data){\n  m1_form &lt;- formula(paste(y, \"~\", x))\n  m1 &lt;- lm(m1_form, data = data)\n  return(coef(summary(m1)))\n}\n\ncoef_table(x = \"temperature\",\n           y = \"transition_date\",\n           data = fig2c)\n\n              Estimate Std. Error   t value     Pr(&gt;|t|)\n(Intercept) 289.458750  3.0593949 94.613071 1.738650e-13\ntemperature   4.982745  0.5541962  8.990941 1.866888e-05\n\n\n\nBoth Y and X can also be column names embedded within a function, for example\n\n\nm2 &lt;- lm(log(transition_date) ~ temperature, data = fig2c)\ncoef(m2)\n\n(Intercept) temperature \n  5.6690276   0.0160509 \n\n\nor\n\nm3 &lt;- lm(scale(transition_date) ~ scale(temperature), data = fig2c)\ncoef(m3)\n\n       (Intercept) scale(temperature) \n      4.171921e-16       9.539117e-01 \n\n\n\nThe data frame (remember that a data.table is a data frame) containing the columns with the variable names in the model formula. A data argument is not necessary but it is usually the better way (an exception is when a researcher wants to create a matrix of Y variables or to construct their own model matrix)\n\ntype ?lm into the console to see other arguments of the lm function.\n\nx &lt;- fig2[, temperature]\ny &lt;- fig2[, transition_date]\nm4 &lt;- lm(y ~ x)\ncoef(m4)\n\n(Intercept)           x \n204.8866185   0.4324755 \n\n\n\n\n9.3.2 Getting to know the linear model: the summary function\nThe lm function returns an lm object, which we’ve assigned to the name m1. m1 contains lots of information about our fit of the linear model to the data. Most of the information that we want for most purposes can be retrieved with the summary function, which is a general-purpose R command the works with many R objects.\n\nsummary(m1)\n\n\nCall:\nlm(formula = transition_date ~ temperature, data = fig2c)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-7.5062 -3.8536  0.6645  2.7537  9.0074 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 289.4588     3.0594  94.613 1.74e-13 ***\ntemperature   4.9827     0.5542   8.991 1.87e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.443 on 8 degrees of freedom\nMultiple R-squared:  0.9099,    Adjusted R-squared:  0.8987 \nF-statistic: 80.84 on 1 and 8 DF,  p-value: 1.867e-05\n\n\nWhat is here:\nCall. This is the model that was fit\nResiduals. This is a summary of the distribution of the residuals. From this one can get a sense of the distribution (for inference, the model assumes a normal distribution with mean zero). More useful ways to examine this distribution will be introduced later in this chapter.\nCoefficients table. This contains the linear model coefficients and their standard error and associated \\(t\\) and \\(p\\) values.\n\nThe column of values under “Estimate” are the coefficients of the fitted model (equation @ref(eq:continuous-x-fit)). Here, 289.4587503 is the intercept (\\(b_0\\)) and 4.9827453 is the effect of \\(temperature\\) (\\(b_1\\)).\nThe column of values under “Std. Error” are the standard errors of the coefficients.\nThe column of values under “t value” are the t-statistics for each coefficient. A t-value is a signal to noise ratio. The coefficient \\(b_1\\) is the “signal” and the SE is the noise. Get used to thinking about this ratio. A t-value greater than about 3 indicates a “pretty good” signal relative to noise, while one much below than 2 is not.\nThe column of values under “Pr(&gt;|t|)” is the p-value, which is the t-test of the estimate. What is the p-value a test of? The p-value tests the hypothesis “how probable are the data, or more extreme than than the data, if the true parameter is zero?”. Formally \\(p = \\mathrm{freq(t' \\ge t|H_o)}\\), where \\(t'\\) is the hypothetical t-value, t is the observed \\(t\\)-value, and \\(H_o\\) is the null hypothesis. We will return to p-values in Chapter xxx.\n\nSignif. codes. Significance codes are extremely common in the wet bench experimental biology literature but do not have much to recommend. I’ll return to these in the p-values chapter.\nBeneath the Signif. codes are some model statistics which are useful\nResidual standard error This is \\(\\sqrt{\\sum{e_i^2}/(n-2)}\\), where \\(e_i\\) are the residuals in the fitted model. “degrees of freedom” is the number of \\(e_i\\) that are “allowed to vary” after fitting the parameters, so is the total sample size (\\(n\\)) minus the number of parameters in the model. The fit model has two fit parameters (\\(b_0\\) and \\(b_1\\) so the df is \\(n-2\\). Note that this is the denominator in the residual standard error equation.\nMultiple R-squared. This is an important but imperfect summary measure of the whole model that effectively measures how much of the total variance in the response variable “is explained by” the model. Its value lies between zero and 1. It’s a good measure to report in a manuscript, especially for observational data.\nF-statistic and p-value. These are statistics for the whole model (not the individual coefficients) and I just don’t find these very useful.\n\n\n9.3.3 Inference – the coefficient table\nThere are different ways of\n\n# step by step\n\nm1_summary &lt;- summary(m1) # get summary\nm1_coef_p1 &lt;- coef(m1_summary) # extract coefficient table\nm1_coef_p2 &lt;- confint(m1) # get CIs\nm1_coef &lt;- cbind(m1_coef_p1, m1_coef_p2) # column bind the two\n\n# this can be shortened (but still readable) using\nm1_coef &lt;- cbind(coef(summary(m1)),\n                 confint(m1))\nm1_coef\n\n              Estimate Std. Error   t value     Pr(&gt;|t|)      2.5 %     97.5 %\n(Intercept) 289.458750  3.0593949 94.613071 1.738650e-13 282.403773 296.513728\ntemperature   4.982745  0.5541962  8.990941 1.866888e-05   3.704767   6.260724\n\n\nNote that the p-value for the coefficient for temperature is very small and we could conclude that the data are not compatible with a model of no temperature effect on day of green-down. But did we need a formal hypothesis test for this? We haven’t learned much if we have only learned that the slope is “not likely to be exactly zero” (Temperature effects everything in biology). A far more important question is not if there is a relationship between temperature and day of green-down, but what is the sign and magnitude of the effect and what is the uncertainty in our estimate of this effect. For this, we we want the coefficient and its SE or confidence interval, both of which are in this table. Remember, our working definition of a confidence interval:\nA confidence interval contains the range of parameter values that are compatible with the data, in the sense that a \\(t\\)-test would not reject the null hypothesis of a difference between the estimate and any value within the interval\nA more textbook way of defining a CI is: A 95% CI of a parameter has a 95% probability of including the true value of the parameter. It does not mean that there is a 95% probability that the true value lies in the interval. This is a subtle but important difference. Here is a way of thinking about the proper meaning of the textbook definition: we don’t know the true value of \\(\\beta_1\\) but we can 1) repeat the experiment or sampling, 2) re-estimate \\(\\beta_1\\), and 3) re-compute a 95% CI. If we do 1-3 many times, 95% of the CIs will include \\(\\beta_1\\) within the interval.\nConfidence intervals are often interpreted like \\(p\\)-values. That is, the researcher looks to see if the CI overlaps with zero and if it does, concludes there is “no effect”. First, this conclusion is not correct – the inability to find sufficient evidence for an effect does not mean there is no effect, it simply means there is insufficient evidence to conclude there is an effect!\nSecond, what we want to use the CI for is to guide us about how big or small the effect might reasonably be, given the data. Again, A CI is a measure of parameter values that are “compatible” with the data. If our biological interpretations at the small-end and at the big-end of the interval’s range radically differ, then we don’t have enough precision in our analysis to reach an unambiguous conclusion.\n\n\n9.3.4 How good is our model? – Model checking\nThere are two, quite different senses of what is meant by a good model.\n\nHow good is the model at predicting? Or, how much of the variance in the response (the stuff to be “explained”) is accounted for by the model? This was described above.\nHow well do the data look like a sample from the modeled distribution? If not well, we should consider alternative models. This is model checking.\n\nFor inference, a good model generates data that look like the real data. If this is true, our fit model will have well-behaved residuals. There are several aspects of “well-behaved” and each is checked with a diagnostic plot. This model checking is covered in more detail in chapter xxx.\nInference from model @ref(eq:continuous-x-m1) assumes the data were sampled from a normal distribution. To check this, use a quantile-quantile or Q-Q plot. The qqPlot function from the car package generates a more useful plot than that from Base R.\n\nset.seed(1)\nqqPlot(m1, id=FALSE)\n\n\n\n\n\n\n\n\nApproximately normal residuals will track the solid line and stay largely within the boundaries marked by the dashed lines. The residuals from m1 fit to the green-down data track the solid line and remain within the dashed lines, indicating adequate model fit.\n\nNote that a formal test of normality is often recommended. Formal tests do not add value above a diagnostic check. Robustness of inference (for example, a p-value) is a function of the type and degree of “non-normalness”, not of a p-value. For a small sample, there is not much power to test for normality, so samples from non-normal distributions pass the test (\\(p &gt; 0.05\\)) and are deemed “normal”. For large samples, samples from distributions that deviate slightly from normal fail the test (\\(p &lt; 0.05\\)) and are deemed “not normal”. Inference with many non-normal samples with large \\(n\\) are very robust (meaning infernece is not likely to fool you with randomness).\n\nInference from model @ref(eq:continuous-x-m1) assumes homogeneity of the response conditional on \\(X\\). For a continuous \\(X\\), this means the residuals should have approximately equal variance at low, mid, and high values of \\(X\\) (and everywhere in between). One can visually inspect the spread of points in the \\(Y\\) direction across the groups for categorical \\(X\\) or along the X-axis for continuous \\(X\\). A useful method for checking how residual variance changes (usually increases) with the conditional mean of \\(Y\\) is a spread-location plot. The spreadLevelPlot(m1) function from the car package is useful.\n\nspreadLevelPlot(m1)\n\n\n\n\n\n\n\n\n\nSuggested power transformation:  0.6721303 \n\n\nThe dashed blue line shows linear trends while the magenta line shows non-linear trends. For the green-down data, the dashed line is very flat while the magenta-line shows what looks like random fluctations. Taken together, the two lines indicate adequate model fit.\n\n\n9.3.5 Plotting models with continuous X\n\n9.3.5.1 Quick plot\nqplot from the ggplot package is useful for initial examinations of the data\n\nqplot(x = temperature, y = transition_date, data = fig2c) + geom_smooth()\n\n\n\n\n\n\n\n\nSome variations of this quick plot to explore include\n\nuse geom_smooth(method = \"lm\")\n\n\n\n9.3.5.2 ggpubr plots\nggpubr is a package with functions that automates the construction of publication-ready ggplots. ggscatter can generate a publishable plot with little effort.\n\nggscatter(data = fig2c,\n          x = \"temperature\",\n          y = \"transition_date\",\n          add = \"reg.line\",\n          xlab = \"Added Temperature (°C)\",\n          ylab = \"Day of Green-down (DOY)\")\n\n\n\n\n\n\n\n\nIt take only a little more effort to add useful modifications.\n\nggscatter(data = fig2c,\n          x = \"temperature\",\n          y = \"transition_date\",\n          color = \"black\",\n          fill = \"red\",\n          shape = 21,\n          size = 3,\n          add = \"reg.line\",\n          add.params = list(color = \"steelblue\",\n                            fill = \"lightgray\"),\n          conf.int = TRUE,\n          xlab = \"Added Temperature (°C)\",\n          ylab = \"Day of Green-down (DOY)\") +\n  \n  stat_regline_equation(size = 4,\n                        label.y = 340) +\n  stat_cor(aes(label = paste(..rr.label.., ..p.label.., sep = \"~`,`~\")),\n           size = 4,\n           label.y = 335) +\n\n  NULL\n\n\n\n\n\n\n\n\nNotes\n\nThe interval shown is the 95% confidence interval of the expected values, which is what we want to communicate with the experimental green-down data. Were we to want to use this as a prediction model, we would want the 95% prediction interval. I’m not sure that ggpubr can plot a prediction interval. To see how I plotted the prediction interval in Figure @ref(fig:continuous-x-pred-fig) above, see the Hidden Code section below.\nThe arguments of the ggscatter function are typed in explicitly (x = \"temperarture\" and not just \"temperature\"). Each argument starts on a new line to increase readability.\nThe + after the ggscatter function adds additional layers to the plot. Each additional component is started on a new line to increase readability.\nThe first line of the stat_cor function (everything within aes()) plots the \\(R^2\\) instead of the correlation coefficient \\(r\\). Copy and pasting this whole line just works.\nComment out the line of the ggplot script starting with stat_cor and re-run (comment out by inserting a # at the front of the line. A consistent way to do this is to triple-click the line to highlight the line and then type command-shift-c on Mac OS). The script runs without error because NULL has been added as the final plot component. Adding “NULL” is a useful trick.\n\n\nggscatter(data = efig_3d,\n          x = \"autumn_temp_anomaly\",\n          y = \"green_down_anomaly\",\n          color = \"black\",\n          fill = \"red\",\n          shape = 21,\n          size = 3,\n          add = \"reg.line\",\n          add.params = list(color = \"steelblue\",\n                            fill = \"lightgray\"),\n          conf.int = TRUE,\n          xlab = \"Added Temperature (°C)\",\n          ylab = \"Day of Green-down (DOY)\")\n\n\n\n\n\n\n\n\n\n\n9.3.5.3 ggplots\nUse ggplot from the ggplot2 package for full control of plots. See the Hidden Code below for how I generated the plots in Figure @ref(fig:continuous-x-plot) above.\n\n\n\n9.3.6 Creating a table of predicted values and 95% prediction intervals\nefig_3d is the data.table created from importing the data in Extended Data Figure 3d above. The fit model is\n\nefig3d_m1 &lt;- lm(green_down_anomaly ~ autumn_temp_anomaly, data = efig_3d)\n\nThe predict(efig3d_m1) function is used to compute either fitted or predicted values, and either the confidence or prediction interval of these values.\nUnless specified by newdata, the default x-values used to generate the y in predict(efig3d_m1) are the x-values in the data used to fit the model. The returned values are the expected value for each \\(x_i\\) in the data. The argument newdata passes a data.frame (remember a data.table is a data.frame!) with new x-values. Since these x-values were not in the data used to fit the model, the returned \\(y_{hat}\\) are predicted values.\nThe range of x-values in the data is\n\nrange(efig_3d[,autumn_temp_anomaly])\n\n[1] -2.673793  2.568045\n\n\nLet’s get predicted values for a value of \\(autumn\\_temp\\_anomaly = 2\\).\n\npredict(efig3d_m1, newdata = data.table(autumn_temp_anomaly = 2))\n\n       1 \n3.192646 \n\n\nAnd predicted values across the range of measured values\n\nnew_dt &lt;- data.table(autumn_temp_anomaly = c(-2, -1, 1, 2))\npredict(efig3d_m1, newdata = new_dt)\n\n        1         2         3         4 \n-3.192646 -1.596323  1.596323  3.192646 \n\n\nAdd 95% confidence intervals (this could be used to create the band for plotting)\n\npredict(efig3d_m1, \n        newdata = new_dt,\n        interval = \"confidence\",\n        se.fit = TRUE)$fit\n\n        fit        lwr        upr\n1 -3.192646 -5.2485713 -1.1367215\n2 -1.596323 -2.9492686 -0.2433778\n3  1.596323  0.2433778  2.9492686\n4  3.192646  1.1367215  5.2485713\n\n\nChange to the 95% prediction intervals\n\npredict(efig3d_m1, \n        newdata = new_dt,\n        interval = \"prediction\",\n        se.fit = TRUE)$fit\n\n        fit        lwr       upr\n1 -3.192646 -12.833739  6.448446\n2 -1.596323 -11.112325  7.919679\n3  1.596323  -7.919679 11.112325\n4  3.192646  -6.448446 12.833739\n\n\nPut this all together in a pretty table. I’ve used knitr’s kable function but there are table packages in R that allow extensive control of the output.\n\nefig3d_m1 &lt;- lm(green_down_anomaly ~ autumn_temp_anomaly, data = efig_3d)\nnew_dt &lt;- data.table(autumn_temp_anomaly = c(-2, -1, 1, 2))\nprediction_table &lt;- predict(efig3d_m1, \n                            newdata = new_dt,\n                            interval = \"prediction\",\n                            se.fit = TRUE)$fit\nprediction_table &lt;- data.table(new_dt, prediction_table)\npretty_names &lt;- c(\"Autumn Temp Anomaly\", \"Estimate\", \"2.5%\", \"97.5%\")\nsetnames(prediction_table, old = colnames(prediction_table), new = pretty_names)\nknitr::kable(prediction_table, digits = c(1, 1, 1, 1))\n\n\n\n\nAutumn Temp Anomaly\nEstimate\n2.5%\n97.5%\n\n\n\n\n-2\n-3.2\n-12.8\n6.4\n\n\n-1\n-1.6\n-11.1\n7.9\n\n\n1\n1.6\n-7.9\n11.1\n\n\n2\n3.2\n-6.4\n12.8",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Linear models with a single, continuous *X* (\"linear regression\")</span>"
    ]
  },
  {
    "objectID": "chapters/continuous_x.html#hidden-code",
    "href": "chapters/continuous_x.html#hidden-code",
    "title": "9  Linear models with a single, continuous X (“linear regression”)",
    "section": "9.4 Hidden code",
    "text": "9.4 Hidden code\n\n9.4.1 Import and plot of fig2c (ecosystem warming experimental) data\nImport\n\ndata_from &lt;- \"Ecosystem warming extends vegetation activity but heightens vulnerability to cold temperatures\"\nfile_name &lt;- \"41586_2018_399_MOESM3_ESM.xlsx\"\nfile_path &lt;- here(data_folder, data_from, file_name)\nfig2 &lt;- read_excel(file_path) |&gt; # import\n  clean_names() |&gt; # clean the column names\n  data.table() # convert to data.table\n# View(fig2)\n\nfig2c &lt;- fig2[panel == \"2c\",]\n# View(fig2c)\n\nCreating the response plot (the bottom component)\n\nm1_b &lt;- coef(m1)\nm1_ci &lt;- confint(m1)\nm1_b0_text &lt;- paste0(\"Intercept: \",\n                  round(m1_b[1],1),\n                  \" (\",\n                  round(m1_ci[1,1],1),\n                  \", \",\n                  round(m1_ci[1,2],1),\n                  \") d\")\nm1_b1_text &lt;- paste0(\"Slope: \",\n                  round(m1_b[2],2),\n                  \" (\",\n                  round(m1_ci[2,1],2),\n                  \", \",\n                  round(m1_ci[2,2],2),\n                  \") d per 1 °C\")\n\n# regression line\nm1_x &lt;- min(fig2c[, temperature])\nm1_xend &lt;- max(fig2c[, temperature])\nm1_y &lt;- m1_b[1] + m1_b[2]*m1_x\nm1_yend &lt;- m1_b[1] + m1_b[2]*m1_xend\n\nfig2c_gg_response &lt;- ggplot(data = fig2c,\n             aes(x = temperature,\n                y = transition_date)) +\n  \n  # regression line first, to not block point\n  geom_segment(x = m1_x,\n               y = m1_y,\n               xend = m1_xend,\n               yend = m1_yend) +\n  # create black edge to points\n  # geom_point(size = 4,\n  #            color = \"black\") +\n  geom_point(size = 3,\n             color = pal_okabe_ito[1]) +\n   scale_x_continuous(breaks = c(0, 2.25, 4.5, 6.75, 9)) +\n  xlab(\"Plot temperature (ΔT, °C)\") +\n  ylab(\"Autumn green-down (DOY)\") +\n  theme_pubr() +\n  \n  NULL\n\n# fig2c_gg_response\n\nCreating the effects plot (the top component)\n\nm1_coeff_dt &lt;- data.table(term = row.names(m1_coeff),\n                          data.table(m1_coeff))[2,] |&gt;\n  clean_names()\nm1_coeff_dt[ , p_pretty := pvalString(pr_t)]\n\nmin_bound &lt;- min(m1_coeff_dt[, x2_5_percent])\nmax_bound &lt;- min(m1_coeff_dt[, x97_5_percent])\n\ny_lo &lt;- min(min_bound+min_bound*0.2,\n            -max_bound)\ny_hi &lt;- max(max_bound + max_bound*0.2,\n            -min_bound)\ny_lims &lt;- c(y_lo, y_hi)\n\nfig2c_gg_effect &lt;- ggplot(data=m1_coeff_dt, \n                          aes(x = term,\n                              y = estimate)) +\n  # confidence level of effect\n  geom_errorbar(aes(ymin=x2_5_percent, \n                    ymax=x97_5_percent),\n                width=0, \n                color=\"black\") +\n  # estimate of effect\n  geom_point(size = 3) +\n  \n  # zero effect\n  geom_hline(yintercept=0, linetype = 2) +\n  \n  # p-value\n  annotate(geom = \"text\",\n           label = m1_coeff_dt$p_pretty,\n           x = 1,\n           y = 7.5) +\n  \n  # aesthetics\n  scale_y_continuous(position=\"right\") +\n  scale_x_discrete(labels = \"Temperature\\neffect\") +\n  ylab(\"Effects (day/°C)\") +\n  coord_flip(ylim = y_lims) + \n  theme_pubr() +\n  theme(axis.title.y = element_blank()) +\n  \n  NULL\n\n#fig2c_gg_effect\n\nCombining the response and effects plots into single plot\n\nfig3d_fig &lt;- plot_grid(fig2c_gg_effect,\n                       fig2c_gg_response,\n                       nrow=2,\n                       align = \"v\",\n                       axis = \"lr\",\n                       rel_heights = c(0.4,1))\nfig3d_fig\n\n\n\n9.4.2 Import and plot efig_3d (Ecosysem warming observational) data\nImport\n\ndata_from &lt;- \"Ecosystem warming extends vegetation activity but heightens vulnerability to cold temperatures\"\nfile_name &lt;- \"41586_2018_399_MOESM6_ESM.xlsx\"\nfile_path &lt;- here(data_folder, data_from, file_name)\nefig_3d &lt;- read_excel(file_path,\n                   range = \"J2:K87\",\n                   col_names = FALSE) |&gt; # header causing issues\n  data.table() # convert to data.table\nsetnames(efig_3d, old = colnames(efig_3d), new = c(\"autumn_temp_anomaly\", \"green_down_anomaly\"))\n\n# View(efig_3d)\n\nPlot\n\nm1 &lt;- lm(green_down_anomaly ~ autumn_temp_anomaly, data = efig_3d)\n\n# get x for drawing slope\nminx &lt;- min(efig_3d[,autumn_temp_anomaly])\nmaxx &lt;- max(efig_3d[,autumn_temp_anomaly])\nnew_x &lt;- seq(minx, maxx, length.out = 20)\nnew_data &lt;- data.table(autumn_temp_anomaly = new_x)\nnew_data[, yhat := predict(m1, newdata = new_data)]\nnew_data[, conf_lwr := predict(m1, \n                           se.fit = TRUE,\n                           interval = \"confidence\",\n                           newdata = new_data)$fit[, \"lwr\"]]\nnew_data[, conf_upr := predict(m1, \n                           se.fit = TRUE,\n                           interval = \"confidence\",\n                           newdata = new_data)$fit[, \"upr\"]]\nnew_data[, pred_lwr := predict(m1, \n                           se.fit = TRUE,\n                           interval = \"prediction\",\n                           newdata = new_data)$fit[, \"lwr\"]]\nnew_data[, pred_upr := predict(m1, \n                           se.fit = TRUE,\n                           interval = \"prediction\",\n                           newdata = new_data)$fit[, \"upr\"]]\n\ngg &lt;- ggscatter(data = efig_3d,\n          x = \"autumn_temp_anomaly\",\n          y = \"green_down_anomaly\",\n          color = \"black\",\n          shape = 21,\n          size = 3,\n          add = \"reg.line\",\n          add.params = list(color = \"steelblue\",\n                            fill = \"lightgray\"),\n          xlab = \"Temperature Anomaly (°C)\",\n          ylab = \"Day of Green-down Anomaly (DOY)\") +\n\n  geom_ribbon(data = new_data,\n              aes(ymin = pred_lwr,\n                  ymax = pred_upr,\n                  y = yhat,\n                  fill = \"band\"),\n              fill = \"gray\",\n              alpha = 0.3) +\n  \n  geom_ribbon(data = new_data,\n              aes(ymin = conf_lwr,\n                  ymax = conf_upr,\n                  y = yhat,\n                  fill = \"band\"), alpha = 0.3) +\n  \n  stat_cor(aes(label = paste(..rr.label.., ..p.label.., sep = \"~`,`~\")),\n           size = 4,\n           label.y = 10) +\n  \n  scale_fill_manual(values = pal_okabe_ito) +\n\n  theme(legend.position=\"none\") +\n\n  NULL  \n\ngg\n\n\n\n9.4.3 Import and plot of fig1f (methionine restriction) data\nImport\n\ndata_from &lt;- \"Dietary methionine influences therapy in mouse cancer models and alters human metabolism\"\nfile_name &lt;- \"41586_2019_1437_MOESM2_ESM.xlsx\"\nfile_path &lt;- here(data_folder, data_from, file_name)\n\nfig1f_wide &lt;- read_excel(file_path,\n                    sheet = \"f\",\n                    range = \"B7:R12\",\n                    col_names = FALSE) |&gt;\n  data.table() # convert to data.table\n\nsetnames(fig1f_wide,\n         old = colnames(fig1f_wide),\n         new = c(\"day\",\n                 paste0(\"Cn_\", 1:8),\n                 paste0(\"MR_\", 1:8)))\n\nfig1f_wide &lt;- transpose(fig1f_wide, make.names = 1, keep.names = \"id\")\nfig1f_wide[, treatment := factor(substr(id, 1, 2))]\n\ndays &lt;- c(21, 25, 28, 30, 34, 39)\nfig1f &lt;- melt(fig1f_wide,\n              id.vars &lt;- c(\"treatment\", \"id\"),\n              measure.vars &lt;- as.character(days),\n              variable.name = \"day\",\n              value.name = \"volume\")\nfig1f[, day := as.numeric(as.character(day))]\n# View(fig1f)\n# qplot(x = day, y = volume, color = treatment, data = fig1f) + geom_line(aes(group = id))\n\nCreating the response plot (the bottom component)\n\nfig1f_gg_response &lt;- ggplot(data = fig1f,\n                      aes(x = day, y = volume, color = treatment)) +\n  geom_point() +\n  geom_smooth(aes(group = id), method = \"lm\", se = FALSE) +\n  xlab(\"Day\") +\n  ylab(expression(Tumor~Volume~(mm^3))) +\n  scale_color_manual(values = pal_okabe_ito) +\n  theme_pubr() +\n  theme(\n    legend.position = c(.15, .98),\n    legend.justification = c(\"right\", \"top\"),\n    legend.box.just = \"right\",\n    legend.margin = margin(6, 6, 6, 6),\n    legend.title = element_blank()\n    ) +\n\n  NULL\n\n# fig1f_gg_response\n\nCreating the effects plot (the top component)\n\nm1_coeff_dt &lt;- data.table(term = row.names(m1_coef_table),\n                          data.table(m1_coef_table))[2,] |&gt;\n  clean_names()\nm1_coeff_dt[ , p_pretty := pvalString(pr_t)]\n\nmin_bound &lt;- min(m1_coeff_dt[, x2_5_percent])\nmax_bound &lt;- min(m1_coeff_dt[, x97_5_percent])\n\ny_lo &lt;- min(min_bound+min_bound*0.2,\n            -max_bound)\ny_hi &lt;- max(max_bound + max_bound*0.2,\n            -min_bound)\ny_lims &lt;- c(y_lo, y_hi)\n\nfig1f_gg_effect &lt;- ggplot(data=m1_coeff_dt, \n                          aes(x = term,\n                              y = estimate)) +\n  # confidence level of effect\n  geom_errorbar(aes(ymin=x2_5_percent, \n                    ymax=x97_5_percent),\n                width=0, \n                color=\"black\") +\n  # estimate of effect\n  geom_point(size = 3) +\n  \n  # zero effect\n  geom_hline(yintercept=0, linetype = 2) +\n  \n  # p-value\n  annotate(geom = \"text\",\n           label = m1_coeff_dt$p_pretty,\n           x = 1,\n           y = 7.5) +\n  \n  # aesthetics\n  scale_y_continuous(position=\"right\") +\n  scale_x_discrete(labels = \"MR\\neffect\") +\n  ylab(expression(Growth~(mm^3/day))) +\n  coord_flip(ylim = y_lims) + \n  theme_pubr() +\n  theme(axis.title.y = element_blank()) +\n  \n  NULL\n\n#fig1f_gg_effect\n\nCombining the response and effects plots into single plot\n\nfig1f_gg &lt;- plot_grid(fig1f_gg_effect,\n                      fig1f_gg_response,\n                      nrow=2,\n                      align = \"v\",\n                      axis = \"lr\",\n                      rel_heights = c(0.4,1))\nfig1f_gg",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Linear models with a single, continuous *X* (\"linear regression\")</span>"
    ]
  },
  {
    "objectID": "chapters/continuous_x.html#try-it",
    "href": "chapters/continuous_x.html#try-it",
    "title": "9  Linear models with a single, continuous X (“linear regression”)",
    "section": "9.5 Try it",
    "text": "9.5 Try it\n\n9.5.1 A prediction model from the literature\nThe data come from the top, middle plot of Figure 1e of\nParker, B.L., Calkin, A.C., Seldin, M.M., Keating, M.F., Tarling, E.J., Yang, P., Moody, S.C., Liu, Y., Zerenturk, E.J., Needham, E.J. and Miller, M.L., 2019. An integrative systems genetic analysis of mammalian lipid metabolism. Nature, 567(7747), pp.187-193.\nPublic source\nSource data\nThe researchers built prediction models from a hybrid mouse diversity panel (HMDP) to predict liver lipid levels from measured plasma lipid levels in mice and in humans. The value of the predictor (\\(X\\)) variable for an individual is not a measured value of a single plasma lipid but the predicted value, or score, from the prediction model based on an entire panel of lipid measurements in that individual. The \\(Y\\) variable for the individual is the total measured level across a family of lipids (ceramides, triacylglycerols, diacylglycerols) in the liver of the individual. The question is, how well does the prediction score predict the actual liver level?\n\nUse the data for TG (triacylglycerols) (the top, middle plot of Figure 1e). The column D is the “score” from the prediction model using plasma lipid levels. This is the \\(X\\) variable (the column header is “fitted.total.Cer”). The column E is the total measured liver TG, so is the \\(Y\\) variable.\nFit the linear model y ~ x.\nCreate a publication-quality plot of liver TG (Y-axis) against score (X-axis) – what the researchers labeled “fitted.total.Cer”. Include \\(R^2\\) on the plot.\nAdvanced – add a 95% prediction interval to the plot (the template code for this is in the Hidden Code section for efig3d)\nCreate a table of expected liver TG and 95% prediction interval of liver TG of the score values (13.5, 14, 14.5, 15, 15.5, 16).\nComment on the predictability of liver TG using the plasma scores.\n\n`",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Linear models with a single, continuous *X* (\"linear regression\")</span>"
    ]
  },
  {
    "objectID": "chapters/continuous_x.html#intuition-pumps",
    "href": "chapters/continuous_x.html#intuition-pumps",
    "title": "9  Linear models with a single, continuous X (“linear regression”)",
    "section": "9.6 Intuition pumps",
    "text": "9.6 Intuition pumps\n\n9.6.1 Correlation and $R^2\nIn the code below\n\nchange the value of n to explore effect on the variability of the estimate. Look at this variability, and the magnitude of the SE, and the magnitude of the p-value.\nchange the value of beta_1 to explore effect on what different slopes and correlations look like. Notice that if the variance is fixed (as in this simulation) the expected slope and expected correlation are equal. (Because I’ve fixed the variance in this simple simulation, this code will fail if abs(beta_1) &gt;= 1).\n\n\nn &lt;- 100 # choose between 3 and 10^5\nbeta_1 &lt;- 0.6 # choose a value between -0.99 and 0.99\nx &lt;- rnorm(n)\ny &lt;- beta_1*x + sqrt(1-beta_1^2)*rnorm(n)\nm1 &lt;- lm(y ~ x)\nslope &lt;- paste(\"b_1: \", round(coef(m1)[2], 3))\nse &lt;- paste(\"SE: \", round(coef(summary(m1))[2,2], 3))\nr &lt;- paste(\"r: \", round(cor(x,y), 3))\nr2 &lt;- paste(\"R^2: \", round(summary(m1)$r.squared, 3))\nggscatter(data = data.table(x=x, y=y), x = \"x\", y = \"y\",\n          add = \"reg.line\") +\n  ggtitle(label = paste(slope,se,r,r2,sep=\";  \"))",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Linear models with a single, continuous *X* (\"linear regression\")</span>"
    ]
  },
  {
    "objectID": "chapters/one-factor.html",
    "href": "chapters/one-factor.html",
    "title": "10  Linear models with a single, categorical X (“t-tests” and “ANOVA”)",
    "section": "",
    "text": "10.1 A linear model with a single factor (categorical Xvariable) estimates the effects of the levels of factor on the response\nTo introduce a linear model with a single factor (categorical \\(X\\) variable), I’ll use data from a set of experiments designed to measure the effect of the lipid 12,13-diHOME on brown adipose tissue (BAT) thermoregulation and the mechanism of this effect.\nLynes, M.D., Leiria, L.O., Lundh, M., Bartelt, A., Shamsi, F., Huang, T.L., Takahashi, H., Hirshman, M.F., Schlein, C., Lee, A. and Baer, L.A., 2017. The cold-induced lipokine 12, 13-diHOME promotes fatty acid transport into brown adipose tissue. Nature medicine, 23(5), pp.631-637.\nPublic source\nData source\nDownload the source data files and move to a new folder named “The cold-induced lipokine 12,13-diHOME promotes fatty acid transport into brown adipose tissue”.\nCold temperature and the neurotransmitter/hormone norepinephrine are known to stimulate increased thermogenesis in BAT cells. In this project, the researchers probed the question “what is the pathway that mediates the effect of cold-exposure on BAT thermogenesis?”. In the “discovery” component of this project, the researchers measured plasma levels of 88 lipids with known signaling properties in humans exposed to one hour of both normal (20 °C) and cold temperature (14 °C) temperature. Of the 88 lipids, 12,13-diHOME had the largest response to the cold treatment. The researchers followed this up with experiments on mice.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Linear models with a single, categorical *X* (\"t-tests\" and \"ANOVA\")</span>"
    ]
  },
  {
    "objectID": "chapters/one-factor.html#oneway-data",
    "href": "chapters/one-factor.html#oneway-data",
    "title": "10  Linear models with a single, categorical X (“t-tests” and “ANOVA”)",
    "section": "",
    "text": "10.1.1 Example 1 (fig3d) – two treatment levels (“groups”)\nLet’s start with the experiment in Figure 3d, which was designed to measure the effect of 12,13-diHOME on plasma triglyceride level. If 12,13-diHOME stimulates BAT activity, then levels in the 12,13-diHOME mice should be less than levels in the control mice.\n\n10.1.1.1 Step 1 – Understand the experiment design and the focal comparisons\nDesign: single, categorical X\nResponse variable: \\(\\texttt{serum\\_tg}\\), A measure of serum triglycerides (mg/dl). \\(\\texttt{serum\\_tg}\\) is a continuous variable.\nFactor variable: \\(\\texttt{treatment}\\), with levels:\n\n“Vehicle” – injected with saline; the negative control giving the expected response given handling and injection, but no 12,13-diHOME\n“12,13-diHOME”\n\nContrasts of interest\n12,13-diHOME - Vehicle. Estimates the effect of 12,13-diHOME treatment. This is the focal contrast (and the only contrast).\n\n\n10.1.1.2 Step 2 – import\nOpen the data and, if necessary, wrangle into an analyzable format. The script to import these data is in the section Hidden code below.\n\n\n10.1.1.3 Step 3 – inspect the data\nThe second step is to examine the data to\n\nget a sense of sample size and balance\ncheck for biologically implausible outliers that suggest measurement failure, or transcription error (from a notebook, not in a cell)\nassess outliers for outlier strategy or robust analysis\nassess reasonable distributions and models for analysis.\n\n\nggstripchart(data = fig3d,\n          x = \"treatment\",\n          y = \"serum_tg\",\n          add = c(\"mean_sd\")\n       )\n\n\n\n\n\n\n\n\nThere are no obviously implausible data points. A normal distribution model is a good, reasonable start. This can be checked more thoroughly after fitting the model.\n\n\n10.1.1.4 Step 4 – fit the model\n\nfig3d_m1 &lt;- lm(serum_tg ~ treatment, data = fig3d)\n\n\n\n10.1.1.5 Step 5 – check the model\n\nset.seed(1)\n# qqPlot(fig3d_m1, id=FALSE)\n# spreadLevelPlot(fig3d_m1, id=FALSE)\nggcheck_the_model(fig3d_m1)\n\n\n\n\n\n\n\n\nThe Q-Q plot indicates that the distribution of residuals is well within that expected for a normal sample and there is no cause for concern with inference. The spread-location plot shows no conspicuous trend in how the spread changes with the conditonal mean. There is no cause for concern with inference.\nWrite something like this in your .Rmd file following the model check code chunk:\n“The residuals are well within the range expected from sampling from a Normal distribution. The heterogeneity of the residuals is well within the range expected from sampling from a single distribution.”\n\n\n10.1.1.6 Step 6 – inference\n\n10.1.1.6.1 coefficient table\n\nfig3d_m1_coef &lt;- cbind(coef(summary(fig3d_m1)),\n                        confint(fig3d_m1))\ncolnames(fig3d_m1_coef)[4] &lt;- \"p value\"\nkable(fig3d_m1_coef,\n      digits = c(1,2,1,4,1,1)) |&gt;\n  kable_styling()\n\n\n\n\n\nEstimate\nStd. Error\nt value\np value\n2.5 %\n97.5 %\n\n\n\n\n(Intercept)\n42.6\n1.67\n25.6\n0.0000\n38.9\n46.3\n\n\ntreatment12,13-diHOME\n-7.2\n2.36\n-3.0\n0.0125\n-12.4\n-1.9\n\n\n\n\n\n\n\n\n\n10.1.1.6.2 emmeans table\n\nfig3d_m1_emm &lt;- emmeans(fig3d_m1, specs = \"treatment\")\nkable(fig3d_m1_emm, digits = c(1,1,2,0,1,1)) |&gt;\n  kable_styling()\n\n\n\n\ntreatment\nemmean\nSE\ndf\nlower.CL\nupper.CL\n\n\n\n\nVehicle\n42.6\n1.67\n10\n38.9\n46.3\n\n\n12,13-diHOME\n35.5\n1.67\n10\n31.7\n39.2\n\n\n\n\n\n\n\n\n\n10.1.1.6.3 contrasts table\n\nfig3d_m1_pairs &lt;- contrast(fig3d_m1_emm,\n                            method = \"revpairwise\") |&gt;\n  summary(infer = TRUE)\n\nkable(fig3d_m1_pairs, digits = c(1,1,1,0,1,1,2,4)) |&gt;\n  kable_styling()\n\n\n\n\ncontrast\nestimate\nSE\ndf\nlower.CL\nupper.CL\nt.ratio\np.value\n\n\n\n\n(12,13-diHOME) - Vehicle\n-7.2\n2.4\n10\n-12.4\n-1.9\n-3.04\n0.0125\n\n\n\n\n\n\n\n\n\n\n10.1.1.7 Step 6 – plot the model\nThe norm in bench biology research is a response plot.\n\ngg1 &lt;- ggplot_the_response(\n  fig3d_m1,\n  fig3d_m1_emm,\n  fig3d_m1_pairs,\n  legend_position = \"none\",\n  y_label = \"Serum TG (µg/dL)\",\n  palette = pal_okabe_ito_blue\n)\ngg1\n\n\n\n\n\n\n\nFigure 10.1: 12,13 diHOME reduced serum TG lvels. Six mice per group. P-value computed using lm(serum_tg ~ treatment) in R.\n\n\n\n\n\nIf the researchers want to explicitly communicate more about the treatment effect, then they should “plot the model”.\n\ngg2 &lt;- ggplot_the_model(\n  fig3d_m1,\n  fig3d_m1_emm,\n  fig3d_m1_pairs,\n  legend_position = \"none\",\n  y_label = \"Serum TG (µg/dL)\",\n  effect_label = \"Effects (µg/dL)\",\n  palette = pal_okabe_ito_blue,\n  rel_heights = c(0.5,1)\n)\ngg2\n\n\n\n\n\n\n\nFigure 10.2: 12,13 diHOME reduced serum TG lvels. Six mice per group. P-value computed using lm(serum_tg ~ treatment) in R.\n\n\n\n\n\n\n\n10.1.1.8 Step 7 – report the model results\n\nDifferent ways of reporting the results.\n\n\n“12,13-diHOME reduced serum TG (p = 0.012)”\n“12,13-diHOME reduced serum TG (Estimate = -7.17 µg/dL; 95% CI: -12.4, -1.9; p = 0.012)”\n“The estimated effect of 12,13-diHOME on serum TG is -7.17 µg/dL (95% CI: -12.4, -1.9, \\(p = 0.012\\)).”\n\n\nDon’t do this\n\n\n“12,13-diHOME significantly reduced serum TG (\\(p = 0.012\\))”\n\nWhy this is problematic: Significance applies to a p-value and not the effect. In English usage, “significant” means “large” or “important” and the p-value is not good evidence for either the size of an effect or the importance of an effect (see the p-value chapter). We interpret the size of effect from the estimated effect size and CI and the importance of an effect from knowledge of the physiological consequences of TG reduction over the range of the CI.\n\n\n\n10.1.2 Understanding the analysis with two treatment levels\nThe variable \\(\\texttt{treatment}\\) in the Figure 3d mouse experiment, is a single, categorical \\(X\\) variable. In a linear model, categorical variables are called factors. \\(\\texttt{treatment}\\) can take two different values, “Vehicle” and “12,13-diHOME”. The different values in a factor are the factor levels (or just “levels”). “Levels” is a strange usage of this word; a less formal name for levels is “groups”. In a Nominal categorical factor, the levels have no units and are unordered, even if the variable is based on a numeric measurement. For example, I might design an experiment in which mice are randomly assigned to one of three treatments: one hour at 14 °C, one hour at 18 °C, or one hour at 26 °C. If I model this treatment as a nominal categorical factor, then I simply have three levels. While I would certainly choose to arrange these levels in a meaningful way in a plot, for the analysis itself, these levels have no units and there is no order. Ordinal categorical factors have levels that are ordered but there is no information on relative distance. The treatment at 18 °C is not more similar to 14 °C than to 26 °C. Nominal categorical factors is the default in R and how all factors are analyzed in this text.\n\n10.1.2.1 Linear models are regression models\nThe linear model fit to the serum TG data is\n\\[\n\\begin{align}\nserum\\_tg &= \\beta_0 + \\beta_1 treatment_{12,13-diHOME} + \\varepsilon\\\\\n\\varepsilon &\\sim N(0, \\sigma^2)\n\\end{align}\n\\tag{10.1}\\]\nModel Equation 10.1 is a regression model where \\(treatment_{12,13-diHOME}\\) is not the variable \\(\\texttt{treatment}\\), containing the words “Vehicle” or “12,13-diHOME” but a numeric variable that indicates membership in the group “12,13-diHOME”. This variable contains the number 1 if the mouse belongs to “12,13-diHOME” and the number 0 if the mouse doesn’t belong to “12,13-diHOME”. \\(treatment_{12,13-diHOME}\\) is known as an indicator variable because it indicates group membership. There are several ways of coding indicator variables and the way described here is called dummy or treatment coding. Dummy-coded indicator variables are sometimes called dummy variables.\nThe lm function creates indicator variables under the table, in something called the model matrix.\n\nX &lt;- model.matrix(~ treatment, data = fig3d)\nN &lt;- nrow(X)\nX[1:N,]\n\n   (Intercept) treatment12,13-diHOME\n1            1                     0\n2            1                     0\n3            1                     0\n4            1                     0\n5            1                     0\n6            1                     0\n7            1                     1\n8            1                     1\n9            1                     1\n10           1                     1\n11           1                     1\n12           1                     1\n\n\nThe columns of the model matrix are the names of the model terms in the fit model. R names dummy variables by combining the names of the factor and the name of the level within the factor. So the \\(X\\) variable that R creates in the model matrix for the fit linear model in model Equation 10.1 is \\(treatment12,13-diHOME\\). You can see these names as terms in the coefficient table of the fit model.\nYou should prove to yourself that lm fits the regression model serum_tg ~ X where X is the model matrix.\n\nm1 &lt;- lm(serum_tg ~ treatment, data = fig3d)\ncoef(summary(m1))\n\n                       Estimate Std. Error   t value     Pr(&gt;|t|)\n(Intercept)           42.620042   1.667226 25.563447 1.926081e-10\ntreatment12,13-diHOME -7.167711   2.357813 -3.039982 1.246296e-02\n\n\n\nX &lt;- model.matrix(~ treatment, data = fig3d)\nm2 &lt;- lm(serum_tg ~ X, data = fig3d)\ncoef(summary(m2))\n\n                        Estimate Std. Error   t value     Pr(&gt;|t|)\n(Intercept)            42.620042   1.667226 25.563447 1.926081e-10\nXtreatment12,13-diHOME -7.167711   2.357813 -3.039982 1.246296e-02\n\n\n\n\n\n\n\n\nExpand For Phenomenal Cosmic Power – alternatives to dummy coding\n\n\n\n\n\nThere are alternatives to dummy coding for creating indicator variables. Dummy coding is the default in R and it makes sense when thinking about experimental data with an obvious control level. I also like the interpretation of a “interaction effect” using Dummy coding. The classical coding for ANOVA is deviation effect coding, which creates coefficients that are deviations from the grand mean. In contrast to R, Deviation coding is the default in many statistical software packages including SAS, SPSS, and JMP. The method of coding can make a difference in an ANOVA table. Watch out for this – I’ve found several published papers where the researchers used the default dummy coding but interpreted the ANOVA table as if they had used deviation coding. This is both getting ahead of ourselves and somewhat moot, because I don’t advocate reporting ANOVA tables.\n\n\n\n\n\n\n\n\n\n\nExpand For Phenomenal Cosmic Power – regression using matrix algebra\n\n\n\n\n\nRecall from stats 101 that the slope of \\(X\\) in the model \\(Y = b_0 + b_1 X\\) is \\(b_1 = \\frac{\\textrm{COV}(X,Y)}{\\textrm{VAR}(X)}\\). This can be generalized using the equation\n\\[\n\\mathbf{b} = (\\mathbf{X}^\\top \\mathbf{X})^{-1} \\mathbf{X}^\\top \\mathbf{y}\n\\]\nwhere \\(\\mathbf{X}\\) is the model matrix containing a column for an intercept, columns for all indicator variables, and columns for all numeric covariates. \\(\\mathbf{b}\\) is a vector containing the model coefficients, including the intercept in the first element. The first part of the RHS (\\(\\mathbf{X}^\\top \\mathbf{X}\\)) is a matrix of the “sums of squares and cross-products” of the columns of \\(\\mathbf{X}\\). Dividing each element of this matrix by \\(N-1\\) gives us the covariance matrix of the \\(\\mathbf{X}\\), which contains the variances of the \\(X\\) columns along the diagonal, so this component has the role of the denominator in the stats 101 equation. Matrix algebra doesn’t do division, so the inverse of this matrix is multiplied by the second part. The second part or the RHS (\\(\\mathbf{X}^\\top \\mathbf{y}\\)) is a vector containing the cross-products of each column of \\(\\mathbf{X}\\) with \\(\\mathbf{y}\\). Dividing each element of this vector by \\(N-1\\) gives us the covariances of each \\(X\\) with \\(y\\), so this component has the role of the numerator in the stats 101 equation.\n\n\n\n\n\n10.1.2.2 The Estimates in the coefficient table are estimates of the parameters of the linear model fit to the data.\n\n\n\n\n\n\nEstimate\nStd. Error\nt value\np value\n2.5 %\n97.5 %\n\n\n\n\n(Intercept)\n42.6\n1.67\n25.6\n0.0000\n38.9\n46.3\n\n\ntreatment12,13-diHOME\n-7.2\n2.36\n-3.0\n0.0125\n-12.4\n-1.9\n\n\n\n\n\n\n\nThe row names of the coefficient table are the column names of the model matrix. These are the model terms. There are two terms (two rows) because there are two parameters in the regression model Equation 10.1. The values in the column \\(\\texttt{Estimate}\\) in the coefficient table are the estimates of the regression parameters \\(\\beta_0\\) and \\(\\beta_1\\). These estimates are the coefficients of the fit model, \\(b_0\\) and \\(b_1\\).\n\n\n10.1.2.3 The coefficients of a linear model using dummy coding have a useful interpretation\n\n\n\n\nTable 10.1: Understanding model coefficients of a linear model with a single treatment variable with two groups. The means in the interpretation column are conditional means.\n\n\n\n\n\n\n\ncoefficient\n\n\nparameter\n\n\nmodel term\n\n\ninterpretation\n\n\n\n\n\n\n\\(b_0\\)\n\n\n\\(\\beta_0\\)\n\n\n(Intercept)\n\n\n\\(\\overline{Vehicle}\\)\n\n\n\n\n\\(b_1\\)\n\n\n\\(\\beta_1\\)\n\n\ntreatment12,13-diHOME\n\n\n\\(\\overline{12,13\\;diHOME} - \\overline{Vehicle}\\)\n\n\n\n\n.\n\n\n\n\n\nIt is important to understand the interpretation of the coefficients of the fit linear model Equation 10.1 ( Table 10.1).\n\nThe coefficient \\(b_0\\) is the is the conditional mean of the response for the reference level, which is “Vehicle”. Remember that a conditional mean is the mean of a group that all have the same value for one or more \\(X\\) variables.\nThe coefficient \\(b_1\\) is the difference between the conditional means of the “12,13-diHOME” level and the reference (“Vehicle”) level:\n\n\\[\n\\mathrm{E}[serum\\_tg|treatment = \\texttt{\"12,13-diHOME\"}] - \\mathrm{E}[serum\\_tg|treatment = \\texttt{\"Vehicle\"}]\n\\]\nBecause there are no additional covariates in model, this difference is equal to the difference between the sample means \\(\\bar{Y}_{12,13-diHOME} - \\bar{Y}_{Vehicle}\\). The direction of this difference is important – it is non-reference level minus the reference level.\nThe estimate \\(b_1\\) is the effect that we are interested in. Specifically, it is the measured effect of 12,13-diHOME on serum TG. When we inject 12,13-diHOME, we find the mean serum TG decreased by -7.2 µg/dL relative to the mean serum TG in the mice that were injected with saline. Importantly, the reference level is not a property of an experiment but is set by whomever is analyzing the data. Since the non-reference estimates are differences in means, it often makes sense to set the “control” treatment level as the reference level.\nMany beginners mistakenly memorize the coefficient \\(b_1\\) to equal the mean of the non-reference group (“12,13-diHOME”). Don’t do this. In a regression model, only \\(b_0\\) is a mean. The coefficient \\(b_1\\) in model Equation 10.1 is a difference in means.\n\n\n\n\n\n\n\n\nFigure 10.3: What the coefficients of a linear model with a single categorical X mean. The means of the two treatment levels for the serum TG data are shown with the large, filled circles and the dashed lines. The intercept (\\(b_0\\)) is the mean of the reference treatment level (“Vehicle”). The coefficient \\(b_1\\) is the difference between the treatment level’s mean and the reference mean. As with a linear model with a continuous \\(X\\), the coefficient \\(b_1\\) is an effect.\n\n\n\n\n\nA geometric interpretation of the coefficients is illustrated in Figure Figure 10.3. \\(b_0\\) is the conditional mean of the reference level (“Vehicle”) and is an estimate of \\(\\beta_0\\), the true, conditional mean of the population. \\(b_1\\) is the difference in the conditional means of the first non-reference level (“12,13-diHOME”) and the reference level (“Vehicle”) and is an estimate of \\(\\beta_1\\), the true difference in the conditional means of the population with and without the treatment 12,13-diHOME.\n\n\n10.1.2.4 Better know the coefficient table\n\nfig3d_m1_coef|&gt;\n  kable(digits = c(1,2,1,4,1,1)) |&gt;\n  kable_styling()\n\n\n\n\n\nEstimate\nStd. Error\nt value\np value\n2.5 %\n97.5 %\n\n\n\n\n(Intercept)\n42.6\n1.67\n25.6\n0.0000\n38.9\n46.3\n\n\ntreatment12,13-diHOME\n-7.2\n2.36\n-3.0\n0.0125\n-12.4\n-1.9\n\n\n\n\n\n\n\n\nThe \\(\\texttt{(Intercept)}\\) row contains the statistics for \\(b_0\\) (the estimate of \\(\\beta_0\\)). Remember that \\(b_0\\) is the conditional mean of the reference treatment (“Vehicle”).\nThe \\(\\texttt{treatment12,13-diHOME}\\) row contains the statistics for \\(b_1\\) (the estimate of \\(\\beta_1\\)). Remember that \\(b_1\\) is the difference in conditional means of the groups “12,13-diHOME” and “Vehicle”.\nThe column \\(\\texttt{Estimate}\\) contains the model coefficients, which are estimates of the parameters.\nThe column \\(\\texttt{Std. Error}\\) contains the model SEs of the coefficients. The SE of \\(\\texttt{(Intercept)}\\) is a standard error of a mean (SEM). The SE of \\(\\texttt{treatment12,13-diHOME}\\) is a standard error of a difference (SED).\nThe column \\(\\texttt{t value}\\) contains the test statistic of the coefficients. This value is the ratio \\(\\frac{Estimate}{SE}\\). For this model, we are only interested in the test statistic for \\(b_1\\). Effectively, we will never be interested in the test statistic for \\(b_0\\) because the mean of a group will never be zero.\nThe column \\(\\texttt{Pr(&gt;|t|)}\\) contains the p-values for the test statistic of the coefficients. For this model, and all models in this text, we are only interested in the p-value for the non-intercept coefficients.\nThe columns \\(\\texttt{2.5 %}\\) and \\(\\texttt{97.5 %}\\) contain the lower and upper limits of the 95% confidence interval of the estimate.\n\n\n\n10.1.2.5 The emmeans table is a table of modeled means and inferential statistics\n\n\n\n\nTable 10.2: Estimated marginal means table for model fig3d_m1.\n\n\n\n\n\n\ntreatment\nemmean\nSE\ndf\nlower.CL\nupper.CL\n\n\n\n\nVehicle\n42.6\n1.67\n10\n38.9\n46.3\n\n\n12,13-diHOME\n35.5\n1.67\n10\n31.7\n39.2\n\n\n\n\n\n\n\n\n\n\nThe table of marginal means of a fit model gives the **modeled* mean, standard error and confidence interval for all specified groups. We use these modeled means, SEs, and CIs, for the response plot. Note that here is no test-statistic with a p-value because there is no significance test.\n\nIn this text, I’ll refer to this table as the “emmeans table”, since it is the output from the emmeans function (“em” is the abbreviation for “estimated marginal”).\nI’ll use “modeled means”, “modeled SEs”, and “modeled CIs” to refer to the statistics in the table, as these are the estimate of means, SE, and CI from a fit linear model.\n\n\n\n\n\n\n\nBetter Know: marginal mean\n\n\n\nA marginal mean is the mean over a set of conditional means. For example, if a treatment factor has three levels, the conditional means are the means for each level and the marginal mean is the mean of the three means. Or, if the conditional means are the expected values given a continous covariate, the marginal mean is the expected value at the mean of covariate. The specified emmeans table of the fig3d data is not too exciting because it simply contains the conditional means – the values are not marginalized over any \\(X\\). Because the emmeans table contains different sorts of means (conditional, marginal, adjusted), this text will generally refer to the means in this table as “modeled means”\n\n\n\n\n\n\n\n\nImportant\n\n\n\nIt is exceptionally important to understand the difference between the means, SEs, and CIs in the emmeans table and the statistics of the same name in a summary table of the data.\n\n\nThe emmeans table and a summary statistics table show different statistics – this is important to know for plotting.\n\n\n\nA summary table contains sampled means, SE, and CIs.\n\n\ntreatment\nmean\nSE\nlower.CL\nupper.CL\n\n\n\n\nVehicle\n42.62004\n1.77325\n38.66899\n46.57109\n\n\n12,13-diHOME\n35.45233\n1.55398\n31.98984\n38.91482\n\n\n\n\n\n\n\n\n\n\nEstimated marginal means table for model fig3d_m1.\n\n\ntreatment\nemmean\nSE\ndf\nlower.CL\nupper.CL\n\n\n\n\nVehicle\n42.62004\n1.66723\n10\n38.90523\n46.33485\n\n\n12,13-diHOME\n35.45233\n1.66723\n10\n31.73752\n39.16714\n\n\n\n\n\n\n\nThe top table is the summary statistics table for fig3d data. The bottom table is a the emmeans table of the fit model (m1). I show both to five decimal places for comparison of the values.\n\nThe statistics in a summary table are sampled means, SEs, and CIs – these statistics are computed for the group using only the data in the group.\nThe means in the emmeans table are modeled means. Here, and for many linear models, these will be equal to the sampled means. This will not be the case in some more complex models.\nThe modeled SEs and CIs show the range of means that are consistent with the data. This is important for understanding any mismatch between the data and the model (the model assumptions).\nUnlike the modeled means, the modeled standard errors and confidence intervals will never equal sample standard errors and confidence intervals. In more complex models, plots using sample statistics can lead to very deceiving inference of differences between groups.\nTo understand modeled SEs and CIs, recall that the standard error of a sample mean is \\(\\frac{s}{\\sqrt{n}}\\), where \\(s\\) is the sample standard deviation and \\(n\\) is the sample size in the group. The computation of the SE in the emmeans table uses the same equation, except the numerator is not the sample standard deviation of the group but the model standard deviation, which is an estimate of the true standard deviation \\(\\sigma\\). As with the sample SE, the denominator for the modeled SE is the sample size \\(n\\) for the group. Since the numerator of the modeled SE is the same for all groups, the modeled SE will be the same in all groups that have the same sample size, as seen in the marginal means table for the model fit to the Figure 3d data. This is not true for sampled SEs, since sampled standard deviations will always differ.\n\n\n\n\n\n\n\nBetter Know: model sigma\n\n\n\nIt may seem odd to use a common standard deviation in the computation of the modeled SEs. It is not. Remember that an assumption of the linear model is homogeneity of variances – that all residuals \\(e_i\\) are drawn from the same distribution (\\(N(0, \\sigma^2)\\)) (a “single hat”) regardless of group. The model standard deviation \\(\\hat{\\sigma}\\) is the estimate of the square root of the variance of this distribution. Given this interpretation, it is useful to think of each sample standard deviation as an estimate of \\(\\sigma\\) (the linear model assumes that all differences among the sample standard deviations are due entirely to sampling). The model standard deviation is a more precise estimate of \\(\\sigma\\) since it is computed from a larger sample (all \\(N\\) residuals).\n\n\n\n\n\n\n\n\nBetter Know: model sigma\n\n\n\nThe model standard deviation is called the “pooled” standard deviation in the ANOVA literature and is computed as a sample-size weighted average of the sample standard deviations.\n\n\n\n\n\n\n\n\nBetter Know: model sigma\n\n\n\nThe modeled standard error of the mean uses the estimate of \\(\\sigma\\) from the fit model. This estimate is\n\\[\\begin{equation}\n\\hat{\\sigma} = \\sqrt{\\frac{\\sum{(y_i - \\hat{y}_i)^2}}{df}}\n\\end{equation}\\]\nCreate a code chunk that computes this. Recall that \\((y_i - \\hat{y}_i)\\) is the set of residuals from the model, which can be extracted using residuals(fit) where “fit” is the fit model object. \\(df\\) is the model degrees of freedom, which is \\(N-k\\), where \\(N\\) is the total sample size and \\(k\\) is the number of parameters that are fit. This makes sense – for the sample variance there is one parameter that is fit, the mean of the group, so the denominator is \\(n - 1\\). In model fig3d_m1, there are two parameters that are fit, the intercept and the coefficient of treatment12,13-diHOME, so the denominator is \\(N - 2\\) where \\(N\\) is the total sample size (the sum of the \\(n\\) for each treatment level).\n\n\n\n\n10.1.2.6 Estimates of the effects are in the contrasts table\n\n\n\n\n\ncontrast\nestimate\nSE\ndf\nlower.CL\nupper.CL\nt.ratio\np.value\n\n\n\n\n(12,13-diHOME) - Vehicle\n-7.2\n2.36\n10\n-12.4\n-1.9\n-3\n0.012\n\n\n\n\n\n\n\n\nThis table is important for reporting treatment effects and CIs and for plotting the model. A contrast is a difference in means. With only two treatment levels, the table of contrasts doesn’t give any more information than the coefficient table – the single contrast is the coefficient \\(b_1\\) in the coefficient table. Nevertheless, I advocate computing this table to stay consistent and because the script (or function) to plot the model uses this table and not the coefficient table.\nThe value in the column \\(\\texttt{Estimate}\\) is the mean of the non-reference group (“12,13-diHOME”) minus the mean of the reference group (“Vehicle”).\nThe value in the “SE” column is the standard error of the difference (SED), specifically the difference in the estimate column. This SE is computed using the model standard deviation \\(\\sigma\\).\nThe values in the “lower.CL” and “upper.CL” columns are the bounds of the 95% confidence interval of the estimate. Remember (from Chapter @ref(variability)) to think of this interval as containing potential values of the true parameter (the true difference in means between the two groups) that are reasonably compatible with the data. Don’t think of the interval as having 95% probability of containing the true effect. Remember that a confidence interval applies to the procedure and not a parameter – 95% of the CIs from hypothetical, replicate experiments that meet all the assumptions used to compute the CI will include the true effect.\nThe columns “t.ratio” and “p.value” contains the t and p values of the significance (not hypothesis!) test of the estimate. The t-statistic is the ratio of the estimate to the SE of the estimate (use the console to confirm this given the values in the table). It is a signal (the estimate) to noise (SE of the estimate) ratio. The p-value is the probability of sampling from normal distribution with the observed standard deviation, randomly assigning the sampled values to either “Vehicle” or “12,13-diHOME”, fitting the linear model, and observing a t-value as or more extreme than the observed t. A very small p-value is consistent with the experiment “not sampling from distributions with the same mean” – meaning that adding a treatment affects the mean of the distribution. This is the logic used to infer a treatment effect. Unfortunately, it is also consistent with the experiment not approximating other conditions of the model, including non-random assignment, non-independence, non-normal conditional responses, and variance heterogeneity. It is up to the rigorous researcher to be sure that these other model conditions are approximated or “good enough” to use the p-value to infer a treatment effect on the mean.\n\n\n\n10.1.2.7 t and p from the contrasts table – when there are only two levels in \\(X\\) – are the same as t and p from a t-test\nCompare\ncoefficient table:\n\nm1 &lt;- lm(serum_tg ~ treatment, data = fig3d)\ncoef(summary(m1)) |&gt;\n  kable() |&gt;\n  kable_styling()\n\n\n\n\n\nEstimate\nStd. Error\nt value\nPr(&gt;&#124;t&#124;)\n\n\n\n\n(Intercept)\n42.62004\n1.667226\n25.563447\n0.000000\n\n\ntreatment12,13-diHOME\n-7.16771\n2.357813\n-3.039982\n0.012463\n\n\n\n\n\n\n\ncontrast table:\n\nemmeans(m1, specs = \"treatment\") |&gt;\n  contrast(method = \"revpairwise\") |&gt;\n  kable() |&gt;\n  kable_styling()\n\n\n\n\ncontrast\nestimate\nSE\ndf\nt.ratio\np.value\n\n\n\n\n(12,13-diHOME) - Vehicle\n-7.16771\n2.357813\n10\n-3.039982\n0.012463\n\n\n\n\n\n\n\nt-test:\n\nm2 &lt;- t.test(fig3d[treatment == \"12,13-diHOME\", serum_tg],\n             fig3d[treatment == \"Vehicle\", serum_tg],\n             var.equal = TRUE)\nbroom::glance(m2) |&gt; # glance is from the broom package\n  kable() |&gt;\n  kable_styling()\n\n\n\n\nestimate\nestimate1\nestimate2\nstatistic\np.value\nparameter\nconf.low\nconf.high\nmethod\nalternative\n\n\n\n\n-7.16771\n35.45233\n42.62004\n-3.039982\n0.012463\n10\n-12.42125\n-1.914175\nTwo Sample t-test\ntwo.sided\n\n\n\n\n\n\n\nNotes\n\nThe default t.test in R is the Welch t-test for heterogenous variance. To compute the Student t-test, use var.equal = TRUE.\nThe “statistic” in the t-test output contains the t-value of the t-test. It is precisely the same as the t-statistic in the coefficient table and the contrast table.\nThe p-values in all three tables are precisely the same.\n\nThe t and p values for the t-test are the same as those for the linear model, because the t-test is a specific case of the linear model. Reasons to abandon classic t-tests and learn the linear modeling strategy include\n\nA linear modeling strategy encourages researchers to think about the effect and uncertainty in the effect and not just a p-value.\nThe linear model is nearly infinitely flexible and expandible while the t-test is limited to a few variations.\n\nThere is rarely a reason to ever use the t.test() function. Throw the function away. Ignore web pages that teach you to use it. The t-test is easy to learn, which encourages its overuse. If your only tool is a t-test, every problem looks like a comparison between two-means.\n\n\n\n10.1.3 Example 2 – three treatment levels (“groups”)\n\n10.1.3.1 Understand the experiment design\nThe data come from the experiment reported in Figure 2a of the 12,13-diHOME article described above. This experiment was designed to probe the hypothesis that 12,13-diHOME is a mediator of known stimulators of increased BAT activity (exposure to cold temperature and sympathetic nervous system activation). Mice were assigned to control (30 °C), one-hour exposure to 4 °C, or 30 minute norepinephrine (NE) treatment level (NE is the neurotransmitter of the sympathetic neurons targeting peripheral tissues).\ndesign: single, categorical X with three levels.\nresponse variable: \\(\\texttt{diHOME}\\), the serum concentration of 12,13-diHOME. a continuous variable.\nfactor variable: \\(\\texttt{treatment}\\), with levels:\n\n“Control” – the negative control. We expect diHOME to be low relative to the two treated conditions.\n“Cold” – focal treatment 1. Given the working model of 12,13-diHome as a mediator between stimulation and BAT, this response should be relatively high compared to Control. In the archived data, this group is “1 hour cold”.\n“NE” – focal treatment 2. Given the working model of 12,13-diHome as a mediator between stimulation and BAT, this response should be relatively high compared to Control. In the archived data, this group is “30 min NE”.\n\nplanned contrasts\n\nCold - Control – If diHOME is a mediator of cold, then difference should be positive.\nNE - Control – If diHOME is a mediator of NE, then difference should be positive.\n\nThe contrast Cold - NE is not of interest.\n\n\n10.1.3.2 fit the model\n\nfig2a_m1 &lt;- lm(diHOME ~ treatment, data = fig2a)\n\n\n\n10.1.3.3 check the model\n\nset.seed(1)\nggcheck_the_model(fig2a_m1)\n\nWarning in rlm.default(x, y, weights, method = method, wt.method = wt.method, :\n'rlm' failed to converge in 20 steps\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nThe Q-Q plot indicates that the distribution of residuals is within that expected for a normal sample. The spread-location plot shows no conspicuous trend in how the spread changes with the conditonal mean. There is little cause for concern with inference from a linear model.\nWrite something like this in your .Rmd file following the model check code chunk:\n“The residuals are within the range expected from sampling from a Normal distribution. The heterogeneity of the residuals is well within the range expected from sampling from a single distribution.”\n\n\n10.1.3.4 Inference from the model\n\n10.1.3.4.1 coefficient table\n\nfig2a_m1_coef &lt;- cbind(coef(summary(fig2a_m1)),\n                        confint(fig2a_m1))\nfig2a_m1_coef|&gt;\n  kable(digits = c(1,2,1,4,1,1)) |&gt;\n  kable_styling()\n\n\n\n\n\nEstimate\nStd. Error\nt value\nPr(&gt;&#124;t&#124;)\n2.5 %\n97.5 %\n\n\n\n\n(Intercept)\n12.0\n3.08\n3.9\n0.0016\n5.4\n18.6\n\n\ntreatmentCold\n7.1\n4.57\n1.6\n0.1405\n-2.7\n16.9\n\n\ntreatmentNE\n14.8\n4.36\n3.4\n0.0044\n5.4\n24.1\n\n\n\n\n\n\n\n\n\n10.1.3.4.2 emmeans table\n\nfig2a_m1_emm &lt;- emmeans(fig2a_m1, specs = \"treatment\")\n\nfig2a_m1_emm|&gt;\n  kable(digits = c(1,1,2,0,1,1)) |&gt;\n  kable_styling()\n\n\n\n\ntreatment\nemmean\nSE\ndf\nlower.CL\nupper.CL\n\n\n\n\nControl\n12.0\n3.08\n14\n5.4\n18.6\n\n\nCold\n19.2\n3.38\n14\n11.9\n26.4\n\n\nNE\n26.8\n3.08\n14\n20.2\n33.4\n\n\n\n\n\n\n\n\n\n10.1.3.4.3 contrasts table\n\nfig2a_m1_planned &lt;- contrast(fig2a_m1_emm,\n                           method = \"trt.vs.ctrl\",\n                           adjust = \"none\",\n                           level = 0.95) |&gt;\n  summary(infer = TRUE)\n\nfig2a_m1_planned|&gt;\n  kable(digits = c(1,1,1,0,1,1,2,4)) |&gt;\n  kable_styling()\n\n\n\n\ncontrast\nestimate\nSE\ndf\nlower.CL\nupper.CL\nt.ratio\np.value\n\n\n\n\nCold - Control\n7.1\n4.6\n14\n-2.7\n16.9\n1.56\n0.1405\n\n\nNE - Control\n14.8\n4.4\n14\n5.4\n24.1\n3.40\n0.0044\n\n\n\n\n\n\n\nNotes:\n\nthe adjust = argument is used to choose the method of p-value adjustment. This is a big topic and this book has a hot-take, which is all covered in Chapter 12.\n\n\n\n\n10.1.3.5 plot the model\nThe usual response plot…\n\nggplot_the_response(\n  fig2a_m1,\n  fig2a_m1_emm,\n  fig2a_m1_planned,\n  legend_position = \"none\",\n  y_label = \"12,13-diHOME (pmol/mL)\",\n  palette = pal_okabe_ito_blue\n)\n\n\n\n\n\n\n\n\nOr, if we want to emphasize the effect…\n\nggplot_the_model(\n  fig2a_m1,\n  fig2a_m1_emm,\n  fig2a_m1_planned,\n  legend_position = \"none\",\n  y_label = \"12,13-diHOME (pmol/mL)\",\n  effect_label = \"Effects (pmol/mL)\",\n  palette = pal_okabe_ito_blue,\n  rel_heights = c(0.5,1)\n)\n\n\n\n\n\n\n\n\n\n\n10.1.3.6 Report the model results\n\nDifferent ways of reporting the results in increasing order of making claims that are evidenced by the statistical analysis,\n\n\n“NE exposure increased 12,13-diHOME but the effect of cold exposure on 12,13-diHOME is not clear”.\n“The estimated effect of NE exposure is consistent with a NE-induced increase in 12,13-diHOME while the estimated effect of cold exposure is less clear”.\n“The estimated effect of NE exposure is consistent with a NE-induced increase in 12,13-diHOME (Estimate = 14.8 pmol/mL; 95% CI: 5.4, 24.1; \\(p = 0.004\\)) while the estimated effect of cold exposure is less clear (Estimate = 7.1 pmol/mL; 95% CI: -2.7, 16.9; \\(p = 0.14\\))”.\n\nThe first statement makes the definitive claim that NE causes the increase but there is not unreasonable probability of this magnitude of effect occurring by random sampling. In addition the experiment could be infected by experiment implementation decisions that make this p-value unreliable. The second statement makes a tentative claim. The third statement adds the statistics that provide the evidence for the tentative claim. The statistics could be moved to some combination of the figure, the figure caption, and a supplement.\n\nDon’t do this\n\n\nNE exposures significantly increased 12,13-diHOME (\\(p = 0.004\\))”\nThere is no effect of cold exposure on 12,13-diHOME (\\(p = 0.14\\))”\n\nWhy the first statement is problematic: Significance applies to a p-value and not the effect. In English usage, “significant” means “large” or “important” and the p-value is not good evidence for either the size of an effect or the importance of an effect (see the p-value chapter). We interpret the size of effect from the estimated effect size and CI and the importance of an effect from knowledge of the physiological consequences of TG reduction over the range of the CI.\nWhy the second statement is problematic: p &gt; 0.05, or a high p-value more generally, is not evidence of no effect because a p-value (or 1 - p) does not give the probability that a treatment effect is zero. One could use an equivalence test to give [the probability that an effect is less than some physiologically meaningful magnitude][https://journals.sagepub.com/doi/abs/10.1177/1948550617697177]\n\n\n\n10.1.4 Understanding the analysis with three (or more) treatment levels\n\n10.1.4.1 Better know the coefficient table\nThe fit regression model for the data in Figure 2a is\n\\[\\begin{equation}\ndiHOME_i = b_0 + b_1 treatment_{Cold,i} + b_2 treatment_{NE,i} + e_i\n(\\#eq:fit-dihome)\n\\end{equation}\\]\nThe coefficients of the model are in the \\(\\texttt{Estimate}\\) column of the coefficient table.\n\n\n\n\n\n\nEstimate\nStd. Error\nt value\nPr(&gt;&#124;t&#124;)\n2.5 %\n97.5 %\n\n\n\n\n(Intercept)\n12.0\n3.1\n3.9\n0\n5.4\n18.6\n\n\ntreatmentCold\n7.1\n4.6\n1.6\n0\n-2.7\n16.9\n\n\ntreatmentNE\n14.8\n4.4\n3.4\n0\n5.4\n24.1\n\n\n\n\n\n\n\n\nThe \\(\\texttt{(Intercept)}\\) row contains the statistics for \\(b_0\\) (the estimate of \\(\\beta_0\\)). Here, \\(b_0\\) is the mean of the reference group, which is “Control”.\nThe \\(\\texttt{treatmentCold}\\) row contains the statistics for \\(b_1\\) (the estimate of \\(\\beta_1\\)). Here, \\(b_1\\) is the difference \\(\\mathrm{E}[diHOME|treatment = \\texttt{\"Cold\"}] - \\mathrm{E}[diHOME|treatment = \\texttt{\"Control\"}]\\). This difference in conditional means is equal to the difference in the sample means of the two groups for this model because there are no additional covariates in the model.\nThe \\(\\texttt{treatmentNE}\\) row contains the statistics for \\(b_2\\) (the estimate of \\(\\beta_2\\)). Here, \\(b_2\\) is the difference \\(\\mathrm{E}[diHOME|treatment = \\texttt{\"NE\"}] - \\mathrm{E}[diHOME|treatment = \\texttt{\"Control\"}]\\). Do not make the mistake in thinking that the value in \\(\\texttt{Estimate}\\) is the mean of the “NE” group.\nThe number of non-intercept coefficients generalizes to any number of levels of the factor variable. If there are \\(k\\) levels of the factor, there are \\(k-1\\) indicator variables, each with its own coefficient (\\(b_1\\) through \\(b_{k-1}\\)) estimating the effect of that treatment level relative to the control (if using dummy coding).\n\nAgain – Do not make the mistake in thinking that the values in \\(\\texttt{Estimate}\\) for the \\(\\texttt{treatmentCold}\\) and \\(\\texttt{treatmentNE}\\) rows are the means of the “Cold” and “NE” groups. These coefficients are differences in means. And, to emphasize further understanding of these coefficients, both \\(b_1\\) and \\(b_2\\) are “slopes”. Don’t visualize this as a single line from the control mean through both non-control means. Slopes is plural – there are two regression lines. \\(b_1\\) is the slope of the line from the control mean to the “Cold” mean while \\(b_2\\) is the slope of the line from the control mean to the “NE” mean. The numerator of each slope is the difference between that group’s mean and the control mean. The denominator of each slope is 1 (because each has the value 1 when the row is assigned to that group).\nTwo understand the names of the model terms, it’s useful to recall the order of the factor levels of \\(\\texttt{treatment}\\), which is\n\nlevels(fig2a$treatment) \n\n[1] \"Control\" \"Cold\"    \"NE\"     \n\n\nGiven this ordering, the lm function creates a regression model with an intercept column for the “Control” group (because the first group in the list is the reference level), an indicator variable for the “Cold” group called treatmentCold, and an indicator variable for the “NE” group called treatmentNE. We can see these model names by peeking at the model matrix of the fit model\n\nfig2a_m1_X &lt;- model.matrix(fig2a_m1)\n\nhead(fig2a_m1_X)\n\n  (Intercept) treatmentCold treatmentNE\n1           1             0           0\n2           1             1           0\n3           1             0           1\n4           1             0           0\n5           1             1           0\n6           1             0           1\n\n\nThe column \\(\\texttt{treatmentCold}\\) is a dummy-coded indicator variable containing the number 1, if the individual is in the “Cold” group, or the number 0, otherwise. The column \\(\\texttt{treatmentNE}\\) is a dummy-coded indicator variable containing the number 1, if the individual is in the “NE” group, or the number 0, otherwise.\nThe model coefficients, parameters, model term, and interpretation are summarized in the following table.\n\n\n\nUnderstanding model coefficients of a linear model with a single treatment variable with three groups. The means in the interpreation column are conditional means.\n\n\ncoefficient\nparameter\nmodel term\ninterpretation\n\n\n\n\n$b_0$\n$\\beta_0$\n(Intercept)\n$\\overline{Control}$\n\n\n$b_1$\n$\\beta_1$\ntreatmentCold\n$\\overline{1\\;hour\\;cold} - \\overline{Control}$\n\n\n$b_2$\n$\\beta_2$\ntreatmentNE\n$\\overline{30\\;min\\;NE} - \\overline{Control}$\n\n\n\n\n\n\n\n\n\n10.1.4.2 The emmeans table\n\n\n\n\n\ntreatment\nemmean\nSE\ndf\nlower.CL\nupper.CL\n\n\n\n\nControl\n12.0\n3.08\n14\n5.4\n18.6\n\n\nCold\n19.2\n3.38\n14\n11.9\n26.4\n\n\nNE\n26.8\n3.08\n14\n20.2\n33.4\n\n\n\n\n\n\n\nThis table is important for reporting means and CIs and for plotting the model. As in example 1, the modeled means in the column “emmean” are the sample means of each group (what you would compute if you simply computed the mean for that group). Again, this is true for this model, but is not generally true.\nAnd again, as in example 1, the SE for each mean is not the sample SE but the modeled SE – the numerator is the estimate of \\(\\sigma\\) from the fit model, which includes residuals from all groups combined. These are the SEs that you should report because it is these SEs that are used to compute the p-value and CI that you report, that is, they tell the same “story”. The SE for the “Cold” group is a bit higher because the sample size \\(n\\) for this group is smaller by 1.\n\n\n10.1.4.3 The contrasts table\n\n\n\n\n\ncontrast\nestimate\nSE\ndf\nlower.CL\nupper.CL\nt.ratio\np.value\n\n\n\n\nCold - Control\n7.1\n4.57\n14\n-2.7\n16.9\n1.6\n0.1405\n\n\nNE - Control\n14.8\n4.36\n14\n5.4\n24.1\n3.4\n0.0044\n\n\n\n\n\n\n\n\nThis table is important for reporting treatment effects and CIs and for plotting the model. A contrast is a difference in means.\nThe contrast table here has no more information than is in the coefficient table, but that is not generally true for models with treatment factors with more than two groups. In “Working in R” below, I show how to compute a contrast table with all pairwise comparisons (contrasts between all possible pairings of the groups)\nThe column \\(\\texttt{Contrast}\\) contains the names of the contrasts. Note that the name gives the direction of the difference.\nThe values in the column \\(\\texttt{estimate}\\) are the contrasts. These are the differences in the conditional means of the groups identified in the \\(\\texttt{Contrast}\\) column. These are the effects that we are interested in.\nThe value in the “SE” column is the standard error of the difference (SED) of each contrast. This SE is computed using the model standard deviation \\(\\sigma\\).\nThe values in the “lower.CL” and “upper.CL” columns are the bounds of the 95% confidence interval of the estimate. Remember (from Chapter @ref(variability)) to think of this interval as containing potential values of the true parameter (the true difference in means between the two groups) that are reasonably compatible with the data. Don’t think of the interval as having 95% probability of containing the true effect. Remember that a confidence interval applies to the procedure and not a parameter – 95% of the CIs from hypothetical, replicate experiments that meet all the assumptions used to compute the CI will include the true effect.\nThe columns “t.ratio” and “p.value” contains the t and p values of the significance (not hypothesis!) test of the estimate. The t-statistic is the ratio of the estimate to the SE of the estimate (use the console to confirm this given the values in the table). It is a signal (the estimate) to noise (SE of the estimate) ratio. The p-value is the probability of sampling from normal distribution with the observed standard deviation, randomly assigning the sampled values to the three groups (using the original sample sizes for each), fitting the linear model, and observing a t-value as or more extreme than the observed t. A very small p-value is consistent with the experiment “not sampling from distributions with the same mean” – meaning that adding a treatment affects the mean of the distribution. This is the logic used to infer a treatment effect. Unfortunately, it is also consistent with the experiment not approximating other conditions of the model, including non-random assignment, non-independence, non-normal conditional responses, and variance heterogeneity. It is up to the rigorous researcher to be sure that these other model conditions are approximated or “good enough” to use the p-value to infer a treatment effect on the mean.\n\n\n\n10.1.4.4 t and p from the contrasts table – when there are more than two levels in \\(X\\) – are not the same as those from pairwise t-tests among pairs of groups\n\nfig2a_m1_pairs &lt;- contrast(fig2a_m1_emm,\n                           method = \"revpairwise\",\n                           adjust = \"none\") |&gt;\n  summary(infer = TRUE)\n\nThe chunk above computes a contrast table that includes comparisons of all pairs of groups in the factor \\(\\texttt{treatment}\\) (this adds a 3rd comparison to the contrast table of planned comparisons above). The t-tests for the contrasts are derived from a single fit linear model.\nIn contrast to the analysis in the chunk above, researchers commonly fit separate t-tests for each pair of treatment levels.\n\n# classic t-test\ntest1 &lt;- t.test(fig2a[treatment == \"Cold\", diHOME],\n                fig2a[treatment == \"Control\", diHOME],\n                var.equal = TRUE)\n\ntest2 &lt;- t.test(fig2a[treatment == \"NE\", diHOME],\n                fig2a[treatment == \"Control\", diHOME],\n                var.equal = TRUE)\n\ntest3 &lt;- t.test(fig2a[treatment == \"NE\", diHOME],\n                fig2a[treatment == \"Cold\", diHOME],\n                var.equal = TRUE)\n\nNotes\n\nAgain, the default t.test in R is the Welch t-test for heterogenous variance. To compute the Student t-test, use var.equal = TRUE\nTo see the full t.test output, type “test1” into the console.\n\nCompare the t and p values from the three independent tests with the t and p-values from the single linear model.\n\n\n\n\n\ncontrast\nt (lm)\np (lm)\nt (t-test)\np (t-test)\n\n\n\n\nCold - Control\n1.562324\n0.1405278\n2.415122\n0.0389207\n\n\nNE - Cold\n1.674696\n0.1161773\n1.380666\n0.2007006\n\n\nNE - Control\n3.395015\n0.0043559\n3.238158\n0.0088971\n\n\n\n\n\n\n\nThe t and p-values computed from three separate tests differ from the t and p-values computed from the single linear model shown in the contrasts table above. The values differ because the SE in the denominators used to compute the \\(t\\)-values differ. The linear model uses the same value of \\(\\sigma\\) to compute the SED (the denominator of t) for all three t-tests in the contrast table. Each separate t-test uses a different value of \\(\\sigma\\) to compute the SED. Which is correct? Neither – they simply make different assumptions about the data generating model.\n\nMost importantly, never do both methods, look at the p-values, and then convince yourself that the method with the p-values that match your hypothesis is the correct method. Human brains are very, very good at doing this. This is called p-hacking. When you p-hack, the interpretation of the p-value is effectively meaningless. P-hacking leads to irreproducible science.\nIn general, using the linear model is a better practice than the separate t-tests. The reason is the homogeneity of variance assumption. If we assume homogeneity of variances, then we can think of the sample standard deviation of all three groups as an estimate of \\(\\sigma\\). In the linear model, we use three groups to estimate \\(\\sigma\\) but in each separate t-test, we use only two groups. Consequently, our estimate of \\(\\sigma\\) in the linear model is more precise than that in the t-tests. While the difference can be large with any individual data set (it’s pretty big with the fig2a data), the long-run advantage of using the linear model instead of separate t-tests is pretty small, especially with only three groups (the precision increases with more groups).\nWe can drop the homogeneity of variance assumption with either the linear model or the three, separate t-tests. This is outlined below in “Heterogeneity of variance”. In this case, the t and p-values for the three comparisons are the same. Still, the linear model (that models heterogenity) is better practice than the separate t-tests because the linear model is much more flexible and expandable.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Linear models with a single, categorical *X* (\"t-tests\" and \"ANOVA\")</span>"
    ]
  },
  {
    "objectID": "chapters/one-factor.html#working-in-r",
    "href": "chapters/one-factor.html#working-in-r",
    "title": "10  Linear models with a single, categorical X (“t-tests” and “ANOVA”)",
    "section": "10.2 Working in R",
    "text": "10.2 Working in R\n\n10.2.1 Fit the model\n\nm1 &lt;- lm(diHOME ~ treatment, data = fig2a)\n\n\nThe two arguments are the model formula diHOME ~ treatment and the data object fig2a.\nThe model formula has the form y ~ x. I read this as “y as a function of x” but say “y squiggly x” when I’m speaking it out loud. y is the response variable diHome. Here, x is the factor treatment.\nIn this text, the y and x parts of the model formula will always be the names of columns of data in the data object. But, either or both can be vectors or matrices that are not in a data.table (or other kind of data object).\nAll functions that will be used to fit models in this text will use a model formula.\nMany base R and package functions that are meant to be either “easy” or follow an ANOVA strategy use lists of dependent and independent variables instead of a model formula. We won’t use these because they are not consistent with a linear modeling way of thinking about data analysis.\ntreatment was specifically coded as a factor variable in the import and wrangle chunk above and R will automatically create the correct indicator variables.\nIf categorical variables on the RHS of the formula have not be converted to factors by the user, then R will treat character variables as factors and create the indicator variables.\n\n\n10.2.1.1 Reordering the levels of the factor variables to some meaningful order\nThis book emphasizes ordering our factor levels in order to contro\n\nthe direction of coefficients in the coefficient table from the fit model\nthe order of groups of the emmeans table\nthe direction of contrasts in the contrast table\nthe ordering of the groups on the X axis of a ggplot\n\nTo understand this better, peek at the ordering of the levels of treatment\n\nlevels(fig2a$treatment)\n\n[1] \"Control\" \"Cold\"    \"NE\"     \n\n\n\nThe first level in this list is the reference level. Contrasts with this will be computed using the direction non_reference - reference. Almost always, we want our “control” or “wildtype” to be the reference because we want our contrast to be the effect of intervention.\nTo set the order of levels within a factor variable, use code like this, which I typically include within the import chunk immediately after the import\n\n\ntreatment_order &lt;- c(\"Control\", \"Cold\", \"NE\")\nfig2a[, treatment := factor(treatment,\n                            levels = treatment_order)] # order levels\n\n\nThe first line simply creates an object containing the group labels in the order that I want.\nThe second line converts treatment to a factor variable AND orders the levels to that in treatment_order.\n\n\n\n\n\n\n\nWhat if I don’t reorder factor levels?\n\n\n\nThe default ordering for a factor is alphabetical, so the default order for fig2a would be (“Cold”, “Control”, “NE”). This doesn’t change any statistical inference but it will create a plot in which the order of the groups on the horizontal axis is goofy and mindless.\n\n\n\n\n10.2.1.2 The R formula can include transformations such as log(y)\nIf is common in biostatistics to log transform the response variable (this is common with count measures or things that grow geometrically). One could simply create a log-transformed column of the response variable and then use this log transformed column in the model formula.\n\nfig2a[, diHOME_log := log(diHOME)]\n\nA better way to do this is to simply do the transformation in the model formula itself. See the note for why!\n\nm1_log &lt;- lm(log(diHOME) ~ treatment, data = fig2a)\nm1_log_emm &lt;- emmeans(m1_log, specs = \"treatment\")\nm1_log_pairs &lt;- contrast(m1_log_emm,\n                     method = \"revpairwise\") |&gt;\n  summary(infer = TRUE)\nm1_log_pairs\n\n contrast       estimate    SE df lower.CL upper.CL t.ratio p.value\n Cold - Control    0.442 0.218 14   -0.129     1.01   2.026  0.1425\n NE - Control      0.751 0.208 14    0.206     1.30   3.606  0.0075\n NE - Cold         0.309 0.218 14   -0.263     0.88   1.413  0.3612\n\nResults are given on the log (not the response) scale. \nConfidence level used: 0.95 \nConf-level adjustment: tukey method for comparing a family of 3 estimates \nP value adjustment: tukey method for comparing a family of 3 estimates \n\n\nNotes\n\nThe emmeans and contrast functions are smart enough to recognize that you have a log transformation in the model formula, so these give you the estimates on the log scale. This means that the contrasts in the estimates column are the differences of the means of the log-transformed variables. And the SE and CIs are on this log scale. 2.\nYou can see the back-transformed estimates and statistics on the response scale by adding a type = \"response argument to the emmeans function.\n\n\nm1_log &lt;- lm(log(diHOME) ~ treatment, data = fig2a)\nm1_log_emm &lt;- emmeans(m1_log,\n                  specs = \"treatment\",\n                  type = \"response\")\nm1_log_pairs &lt;- contrast(m1_log_emm,\n                     method = \"revpairwise\") |&gt;\n  summary(infer = TRUE)\nm1_log_pairs\n\n contrast       ratio    SE df lower.CL upper.CL null t.ratio p.value\n Cold / Control  1.56 0.340 14    0.879     2.76    1   2.026  0.1425\n NE / Control    2.12 0.441 14    1.229     3.65    1   3.606  0.0075\n NE / Cold       1.36 0.297 14    0.769     2.41    1   1.413  0.3612\n\nConfidence level used: 0.95 \nConf-level adjustment: tukey method for comparing a family of 3 estimates \nIntervals are back-transformed from the log scale \nP value adjustment: tukey method for comparing a family of 3 estimates \nTests are performed on the log scale \n\n\nNotes\n\nNow the contrast is a ratio instead of a difference. This is because the inverse log of a difference is a ratio!\n\n\\[\nlog(\\frac{a}{b}) = log(a) - log(b)\n\\]\n\n\n10.2.1.3 The R formula can include object type transformations such as factor(x)\nSometimes we have a numeric or integer \\(X\\) variable but want to model this as a categorical factor variable. For example, let’s create fake data with glucose_uptake measured at three different times in each mouse.\n\nfake_data &lt;- data.table(\n  mouse_id = rep(paste(\"mouse\", 1:6), each = 3),\n  genotype = rep(c(\"WT\", \"KO\"), each = 6 * 3),\n  time = rep(1:3, 6),\n  glucose_uptake = rnorm(6 * 3)\n)\n\nIn these data, time is a numeric variable. If we want to model time as a factor, we could create a new data column that contains the factor variable\n\nfake_data[, time_fac := factor(time)]\nm1_fake &lt;- lm(glucose_uptake ~ genotype * time_fac, data = fake_data)\n\nOr we could just change the object time in the model formula\n\nm1_fake &lt;- lm(glucose_uptake ~ genotype * factor(time), data = fake_data)\n\nNotes\n\nIn this text, we would not fit the above models but would instead use the model formula\n\nglucose_uptake ~ genotype * factor(time) + (1 | mouse_id)\nbut we haven’t gotten to this yet!\n\n\n\n\n\n\nWarning\n\n\n\nRecognize that\nglucose_uptake ~ genotype * time\nand\nglucose_uptake ~ genotype * factor(time)\nare two different models! The first models time as a numeric variable while the second models time as a factor variable.\n\n\n\n\n\n10.2.2 Controlling the output in tables using the coefficient table as an example\n\nm1 &lt;- lm(diHOME ~ treatment, data = fig2a)\nm1_coef &lt;- coef(summary(m1))\n\n# or if we want the CIs of the coefficeints too\n\nm1_coef &lt;- cbind(coef(summary(m1)),\n                 confint(m1))\nm1_coef\n\n               Estimate Std. Error  t value    Pr(&gt;|t|)     2.5 %   97.5 %\n(Intercept)   12.023075   3.081337 3.901902 0.001595771  5.414264 18.63189\ntreatmentCold  7.140386   4.570362 1.562324 0.140527829 -2.662066 16.94284\ntreatmentNE   14.794354   4.357669 3.395015 0.004355868  5.448083 24.14063\n\n\nFor many table-like objects, I use knitr::kable to print the table with fewer decimal places and kableExtra::kable_styling to make it a little prettier.\n\n# the row names are not part of the m1_coef object\n# so there is no digit designation for this column\n\nm1_coef|&gt; # pipe the m1_coef object to kable\n  kable(digits = c(2,3,3,5,2,2)) |&gt; \n  kable_styling()\n\n\n\n\n\nEstimate\nStd. Error\nt value\nPr(&gt;&#124;t&#124;)\n2.5 %\n97.5 %\n\n\n\n\n(Intercept)\n12.02\n3.081\n3.902\n0.00160\n5.41\n18.63\n\n\ntreatmentCold\n7.14\n4.570\n1.562\n0.14053\n-2.66\n16.94\n\n\ntreatmentNE\n14.79\n4.358\n3.395\n0.00436\n5.45\n24.14\n\n\n\n\n\n\n# explore other styles in the kableExtra package\n\nNote that as of this writing (09/29/2024), kable_styling() is not correctly printing the absolute value signs in the p-value label. This is annoying, so as a fix, I’m changing the column name to p value\n\ncolnames(m1_coef)[4] &lt;- \"p value\"\nm1_coef|&gt; # pipe the m1_coef object to kable\n  kable(digits = c(2,3,3,5,2,2)) |&gt; \n  kable_styling()\n\n\n\n\n\nEstimate\nStd. Error\nt value\np value\n2.5 %\n97.5 %\n\n\n\n\n(Intercept)\n12.02\n3.081\n3.902\n0.00160\n5.41\n18.63\n\n\ntreatmentCold\n7.14\n4.570\n1.562\n0.14053\n-2.66\n16.94\n\n\ntreatmentNE\n14.79\n4.358\n3.395\n0.00436\n5.45\n24.14\n\n\n\n\n\n\n\n\n\n10.2.3 Using the emmeans function\n\nm1_emm &lt;- emmeans(m1, specs = \"treatment\")\nm1_emm\n\n treatment emmean   SE df lower.CL upper.CL\n Control     12.0 3.08 14     5.41     18.6\n Cold        19.2 3.38 14    11.92     26.4\n NE          26.8 3.08 14    20.21     33.4\n\nConfidence level used: 0.95 \n\n\nNotes\n\nNote that printing the emmeans object displays useful information. Here, this information includes the confidence level used. If the object is printed using kable (as in the “Inference” and “Understanding” sections above), only the table is printed and the additional information is lost.\nemmeans computes the modeled means of all combinations of the levels of the factor variables specified in the argument specs.\nAn important argument of the emmeans function is type =, which controls the “space” of the response variable.\nOutput options are more complex if there are two or more X variables. This will be covered in the chapter “Models for two (or more) categorical X variables”.\nIf there are continuous covariates in the model, the modeled means are computed at the average values of these covariates. These covariates do not need to be passed to the specs argument.\nYou can pass numeric and integer covariates to specs to control the value of the covariates used to compute the modeled means. This is outlined in Adding covariates to a linear model\n\n\n\n10.2.4 Using the contrast function\n\nm1_planned &lt;- contrast(m1_emm,\n                       method = \"trt.vs.ctrl\",\n                       adjust = \"none\",\n                       level = 0.95) |&gt;\n  summary(infer = TRUE)\n\nm1_planned\n\n contrast       estimate   SE df lower.CL upper.CL t.ratio p.value\n Cold - Control     7.14 4.57 14    -2.66     16.9   1.562  0.1405\n NE - Control      14.79 4.36 14     5.45     24.1   3.395  0.0044\n\nConfidence level used: 0.95 \n\n\nNotes\n\nNote that printing the contrast object displays useful information, including the confidence level used and the method of adjustment for multiple tests. If the object is printed using kable() |&gt; kable_styling() (as in the “Inference” and “Understanding” sections above), only the table is printed and the additional information is lost.\nThe method argument is used to control the set of contrasts that are computed. See below.\nThe adjust argument controls if and how to adjust for multiple tests. Each method has a default adjustment method. See below.\nThe level argument controls the percentile boundaries of the confidence interval. The default is 0.95. Including this argument with this value makes this level transparent.\n\n\n10.2.4.1 the method argument controls the set of contrasts\nThe method = argument is used to control the set of contrasts that are computed. Type help(\"contrast-methods\") into the console to see the list of available methods. Also, read the comparisons and contrasts vignette for more on emmeans::contrast()\n\nmethod = \"revpairwise\" computes all pairwise contrasts. The contrasts that include the reference are in the direction \\(nonreference - reference\\), which is the direction prferred in this text since the result is the effect if you add the treatment. The default p-value adjustment is “tukey”, which is the Tukey’s HSD method\nmethod = \"pairwise\" is the default method. “pairwise” computes all pairwise contrasts but in the direction opposite to “revpairwise”. The default p-value adjustment is “tukey.”\nmethod = \"trt.vs.ctrl\" and method = \"dunnett\" are the same. These compute all non-reference minus reference contrasts. This method was used in the “Inference” section because it gives the two contrasts of the planned comparisons identified in the “understand the experimental design” step. The default p-value adjustment for multiple tests is “dunnettx”, which is Dunnett’s test.\n\nHere are all pairwise contrasts using “revpairwise”\n\ncontrast(m1_emm,\n        method = \"revpairwise\") |&gt;\n  summary(infer = TRUE)\n\n contrast       estimate   SE df lower.CL upper.CL t.ratio p.value\n Cold - Control     7.14 4.57 14    -4.82     19.1   1.562  0.2936\n NE - Control      14.79 4.36 14     3.39     26.2   3.395  0.0114\n NE - Cold          7.65 4.57 14    -4.31     19.6   1.675  0.2490\n\nConfidence level used: 0.95 \nConf-level adjustment: tukey method for comparing a family of 3 estimates \nP value adjustment: tukey method for comparing a family of 3 estimates \n\n\nand using “pairwise”\n\ncontrast(m1_emm,\n        method = \"pairwise\") |&gt;\n  summary(infer = TRUE)\n\n contrast       estimate   SE df lower.CL upper.CL t.ratio p.value\n Control - Cold    -7.14 4.57 14    -19.1     4.82  -1.562  0.2936\n Control - NE     -14.79 4.36 14    -26.2    -3.39  -3.395  0.0114\n Cold - NE         -7.65 4.57 14    -19.6     4.31  -1.675  0.2490\n\nConfidence level used: 0.95 \nConf-level adjustment: tukey method for comparing a family of 3 estimates \nP value adjustment: tukey method for comparing a family of 3 estimates \n\n\nNotice that inference is the same, only the direction of the effect differs.\n\n\n10.2.4.2 Planned comparisons using custom contrasts\nIn any experiment with more than two groups, we have multiple contrasts (or “multiple tests”). Almost always, researchers are only interested in a subset of these. This subset of focal contrasts are planned comparisons. We can limit the number of contrasts computed by the contrast() function by passing a custom contrast object in method =.\nLet’s say the focal contrasts are Cold - Control and NE - Cold but not the contrast NE - Control. To pass a custom contrast object with only these contrasts, I first peek at the emm object to see the order of group means.\n\nm1_emm\n\n treatment emmean   SE df lower.CL upper.CL\n Control     12.0 3.08 14     5.41     18.6\n Cold        19.2 3.38 14    11.92     26.4\n NE          26.8 3.08 14    20.21     33.4\n\nConfidence level used: 0.95 \n\n\nNow we use this to create the contrasts and send these to the contrast() function.\n\ncntrl &lt;- c(1, 0, 0)\ncold &lt;- c(0, 1, 0)\nne &lt;- c(0, 0, 1)\n\nplanned_contrasts &lt;- list(\n  \"Cold - Control\" = cold - cntrl,\n  \"NE - Cold\" = ne - cold)\n\ncontrast(m1_emm,\n        method = planned_contrasts,\n        adjust = \"holm\") |&gt;\n  summary(infer = TRUE)\n\n contrast       estimate   SE df lower.CL upper.CL t.ratio p.value\n Cold - Control     7.14 4.57 14    -4.33     18.6   1.562  0.2324\n NE - Cold          7.65 4.57 14    -3.82     19.1   1.675  0.2324\n\nConfidence level used: 0.95 \nConf-level adjustment: bonferroni method for 2 estimates \nP value adjustment: holm method for 2 tests \n\n\nWhat did I do?\n\nWe have three means so I created three vectors of length 3.\nEach vector contains a “1” in the position that indicates that group’s row in the emmeans table.\nI then create the contrast object as a list containing the contrasts in the form\n\n“output contrast name” = group1 - group2\n\n\n\n\n\n\nTip\n\n\n\nMindless p-values waste research time and money so contrasts take thought. Note that adjustments depend on the number of contrasts so if an advisor or reviewer or benighted colleague insists on adjusting for multiple tests, then fewer tests using this planned comparison approach will have higher power. However, all pairwise contrasts can be useful to a reader, so I would recommend creating a table with all pairwise contrasts with both unadjusted and adjusted p-values and adding this to a supplementary file.\n\n\n\n\n10.2.4.3 Adjustment for multiple tests\nChapter 12 explains the hot-take – why there is no justification for adjusting p-values in many bench biology experiments that do adjust, but there is justification for adjusting p-values in many bench biology experiments that do not adjust!\nHere, I just outline choices available in the emmeans::contrast function using the argument adjust =.\n\n“none” – no adjustment\n“dunnettx” – Dunnett’s test is a method used when comparing all treatments to a single control.\n“tukey” – Tukey’s HSD method is a method used to compare all pairwise comparisons.\n“bonferroni” – Bonferroni is a general purpose method to compare any set of multiple tests. The test is conservative. A better method is “holm”\n“holm” – Holm-Bonferroni is a general purpose method like the Bonferroni but is more powerful.\n“BH” or “fdr” – controls the false discovery rate not the Type I error rate for a family of tests.\n“mvt” – based on the multivariate t distribution and using covariance structure of the variables.\n\n\n\n\n10.2.5 How to generate ANOVA tables\nANOVA is Analysis of Variance. Researchers frequently use the term “ANOVA” as the name for an analysis of an experiment with single-factor with more than two groups. However, ANOVA is a general method of inference for complex experimental designs. ANOVA models and regression models are different ways of expressing the same underlying linear model.\nANOVA tables can be useful for some sophisticated analyses. But for most researchers, an ANOVA table is more likely to lead to misconception than enlightenment. I would typpically recommend to never look at an ANOVA table. But, if your PI, manager, thesis committee, or journal editor insists that you do ANOVA, and you cannot convince them otherwise, here is how to generate an ANOVA table in R. Note that even though we are generating that table, the computation of the contrast table and all inference from that is not part of the ANOVA.\n\n\n\nANOVA table for the Figure 2a (example 2) data.\n\n\n\nDf\nSum Sq\nMean Sq\nF value\nPr(&gt;F)\n\n\n\n\ntreatment\n2\n656.9\n328.4\n5.765\n0.015\n\n\nResiduals\n14\n797.5\n57.0\n\n\n\n\n\n\n\n\n\nNotes\n\nDo not confuse the statistics in an ANOVA table with those in a coefficient table.\nThe ANOVA table @ref(tab:fig2a-anova-table) has two rows. The first row contains the statistics for the factor \\(\\texttt{treatment}\\). The statistics address the null hypothesis “there is no effect of treatment – all groups have the same population mean”. This is a p value about the factor as a whole and not the individual comparisons of different groups within the factor.\nThe second row contains the statistics for the error – the residuals of the model.\n\\(\\texttt{F value}\\) is the test statistic. It is a ratio of variances, which is why this analysis is called Analysis of Variance. The numerator variance is the \\(\\texttt{Mean Sq}\\) of \\(\\texttt{treatment}\\). The numerator variance is the value in $ of \\(\\texttt{treatment}\\). The denominator variance is the value in $ of \\(\\texttt{Residuals}\\).\n\\(\\texttt{Mean Sq}\\) contains the mean square for each term (row) in the table. The mean square is a variance. Remember that the numerator of a variance is a sum of squared differences between observed and mean values. The numerator of \\(\\texttt{Mean Sq}\\) is the value in \\(\\texttt{Sum Sq}\\) from the same row. And, remember that the denominator of a variance is a degree of freedom. The denominator of \\(\\texttt{Mean Sq}\\) is the value in \\(\\texttt{DF}\\) from the same row.\n\nAn ANOVA table for a single factor with more than two groups has a single p-value for the treatment term. The single p-value is the probability of sampling a value of F as large or larger than the observed F under the null (no true effects of either treatment and all specifications of the generating model are true). There is not much we can do with this number - we want to estimate the effect sizes and their uncertainty and we don’t get this from an ANOVA table. Many textbooks, websites, and colleagues suggest to 1) fit the ANOVA, 2) check the F, and, if \\(F &lt; 0.05\\), 3) do “tests after an ANOVA”. These tests after an ANOVA are the planned comparisons and post-hoc tests described above using the linear model. In classical ANOVA, the initial computation of the cell means (means of treatment combinations) and sums of squares was a logical first step to the decomposition of these sums of squares to compute the contrasts. With modern linear models using regression, the ANOVA first step is unnecessary and not recommended.\n\n10.2.5.1 The afex aov_4 function\nThe package afex was developed to make it much easier for researchers to generate ANOVA tables that look like those from other statistics packages including SAS, SPSS, JMP, and Graphpad Prism.\n\n# .I is a data.table function that returns the row number\nfig2a[, fake_id := paste(\"mouse\", .I)]\n\nm1_aov4 &lt;- aov_4(diHOME ~ treatment + (1|fake_id),\n                 data = fig2a)\n\nanova(m1_aov4)\n\nAnova Table (Type 3 tests)\n\nResponse: diHOME\n          num Df den Df    MSE      F     ges  Pr(&gt;F)  \ntreatment      2     14 56.968 5.7651 0.45163 0.01491 *\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nNotes\n\nThe afex package has three function names for generating the same ANOVA table and statistics – here I’m using aov_4 because this functions uses a linear model formula argument (specifically, that used in the lme4 package), which is consistent with the rest of this text.\nThe formula includes the addition of a random factor ((1|id)) even though there really is no random factor in this model. See Section @ref(oneway-paired-t) below for a brief explanation of a random factor. The random factor (the factor variable “id” created in the line before the fit model line) identifies the individual mouse from which the response variable was measured. Because the response was only measured once on each individual mouse, “id” is not really a random factor but the addition of this in the model formula is necessary for the aov_4 function to work.\nIt is easy to get an ANOVA table that you don’t want in R. If you want an ANOVA table that matches one from Graphpad Prism or JMP or similar software, the best practice is using the ANOVA functions from the afex package.\nWhat do I mean by “an ANOVA table that you don’t want”? With factorial ANOVA with unbalanced data, there are three ways to compute the sums of squares for the different terms of the ANOVA table. SAS termed these Type I, II, and III sums of squares and these names have stuck. Following SAS, almost all statistics packages use Type III as the default (or only) method for computing ANOVA tables. R uses Type I as the default. There are very good arguments for using Type II. This distinction is moot for single factor ANOVA or multi-factor ANOVA for balanced designs but is not moot for unbalanced multi-factor ANOVA or any ANOVA with covariates. If you want an ANOVA table from R to match what would be generated by Graphpad Prism or JMP (Type III), then the afex package is the best practice.\n\n\n\n10.2.5.2 The car Anova function\nThe car package has the extremely useful Anova function although using it is a bit like doing brain surgery having only watched a youtube video.\n\ntype3 &lt;- list(treatment = contr.sum)\nm1_type3 &lt;- lm(diHOME ~ treatment,\n                data = fig2a,\n                contrasts = type3)\nAnova(m1_type3, type=\"3\")\n\nAnova Table (Type III tests)\n\nResponse: diHOME\n            Sum Sq Df  F value   Pr(&gt;F)    \n(Intercept) 6308.4  1 110.7355 4.95e-08 ***\ntreatment    656.9  2   5.7651  0.01491 *  \nResiduals    797.5 14                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nNotes\n\ncar::Anova has arguments for reporting the Type III sum of squares. Again, this is not relevant to a single factor ANOVA with no covariates but to avoid catastrophic code in the future, its good to know about best practices now, so I’m pre-peating what is written in Section @ref(twoway-car-anova).\nBackground: The default model matrix in the lm function uses dummy (or treatment) coding. For a Type 3 SS ANOVA (the kind that matches that in Graphpad Prism or JMP), we need to tell lm to use sum (or deviation) coding.\nThe best practice method for changing the contrasts in the model matrix is using the contrasts argument within the lm function, as in the code above to fit m1_type3. This is the safest practice because this sets the contrasts only for this specific fit.\nThe coefficients of m1_type3 will be different from m1. The intercept will be the grand mean and the coefficients of the non-reference levels (the effects) will be their deviations from the grand mean. I don’t find this definition of “effects” very useful for most experiments in biology.\nThe contrasts (differences in the means among pairs of groups) in the contrast table will be the same, regardless of the contrast coding.\n\n\n\n\n\n\n\nWarning\n\n\n\nDanger! Many online sites suggest this bit of code before a Type III ANOVA using car::Anova()\noptions(contrasts = c(\"contr.sum\", \"contr.poly\")\nIf you’re reading this book, you almost certainly don’t want to do this because this code resets how R computes coefficients of linear models and SS of ANOVA tables. This will effect all future analyses until the contrasts are set to something else or a new R session is started.\n\n\nbase R aov and anova\n\nm1_aov &lt;- aov(diHOME ~ treatment, data = fig2a)\nsummary(m1_aov)\n\n            Df Sum Sq Mean Sq F value Pr(&gt;F)  \ntreatment    2  656.9   328.4   5.765 0.0149 *\nResiduals   14  797.5    57.0                 \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n1 observation deleted due to missingness\n\n\n\n# same as m1 in the Example 2 section\nm1 &lt;- lm(diHOME ~ treatment, data = fig2a)\nanova(m1)\n\nAnalysis of Variance Table\n\nResponse: diHOME\n          Df Sum Sq Mean Sq F value  Pr(&gt;F)  \ntreatment  2 656.85  328.43  5.7651 0.01491 *\nResiduals 14 797.55   56.97                  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nNotes\n\nMany introduction to statistics textbooks and websites use the base R aov function. I don’t find this function useful given the afex package functions.\nThe base R anova is useful if you know what you are doing with it.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Linear models with a single, categorical *X* (\"t-tests\" and \"ANOVA\")</span>"
    ]
  },
  {
    "objectID": "chapters/one-factor.html#hidden-code",
    "href": "chapters/one-factor.html#hidden-code",
    "title": "10  Linear models with a single, categorical X (“t-tests” and “ANOVA”)",
    "section": "10.3 Hidden Code",
    "text": "10.3 Hidden Code\n\n10.3.1 Importing and wrangling the fig3d data for example 1\n\ndata_folder &lt;- \"data\"\ndata_from &lt;- \"The cold-induced lipokine 12,13-diHOME promotes fatty acid transport into brown adipose tissue\"\n\n\n# need data_folder and data_from from earlier chunk\nfile_name &lt;- \"41591_2017_BFnm4297_MOESM3_ESM.xlsx\"\nfile_path &lt;- here(data_folder, data_from, file_name)\n\n# ignore the column with animal ID. Based on methods, I am inferring\n# that the six mice in vehicle group *are different* from the\n# six mice in the 1213 group.\ncol_names_3d &lt;- c(\"Vehicle\", \"1213\")\ntreatment_levels &lt;- c(\"Vehicle\", \"12,13-diHOME\")\nfig3d &lt;- read_excel(file_path,\n                     sheet = \"Figure 3d\",\n                     range = \"B3:C9\",\n                     col_names = TRUE) |&gt;\n  data.table() |&gt;\n  melt(measure.vars = col_names_3d,\n       variable.name = \"treatment\",\n       value.name = \"serum_tg\")\n\n# change group name of \"1213\"\nfig3d[treatment == \"1213\", treatment := \"12,13-diHOME\"]\n\n# make treatment a factor with the order in \"treatment_levels\"\nfig3d[, treatment := factor(treatment, treatment_levels)]\n\n#View(fig3d)\n\n\n\n10.3.2 Importing and wrangling the fig2a data for example 2\n\nfile_name &lt;- \"41591_2017_BFnm4297_MOESM2_ESM.xlsx\"\nfile_path &lt;- here(data_folder, data_from, file_name)\n\n# assuming mice are independent and not same mouse used for all three treatment\nmelt_col_names &lt;- paste(\"Animal\", 1:6)\nfig2a &lt;- read_excel(file_path,\n                     sheet = \"Fig 2a\",\n                     range = \"A3:G6\",\n                     col_names = TRUE) |&gt;\n  data.table() |&gt;\n  melt(measure.vars = melt_col_names,\n       variable.name = \"id\",\n       value.name = \"diHOME\") # cannot start a variable with number\nsetnames(fig2a, old = colnames(fig2a)[1], new = \"treatment\")\n\n# make better group names for analysis\nfig2a[treatment == \"1 hour cold\", treatment := \"Cold\"]\nfig2a[treatment == \"30 min NE\", treatment := \"NE\"]\n\ntreatment_order &lt;- c(\"Control\", \"Cold\", \"NE\")\nfig2a[, treatment := factor(treatment, treatment_order)] # order levels\n\n#View(fig2a)",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Linear models with a single, categorical *X* (\"t-tests\" and \"ANOVA\")</span>"
    ]
  },
  {
    "objectID": "chapters/model-checking.html",
    "href": "chapters/model-checking.html",
    "title": "11  Model checking",
    "section": "",
    "text": "11.1 All statistical analyses should be followed by model checking\nWe use a linear model to infer effects or predict future outcomes. Our inference is uncertain. Given the model assumptions, we can quantify this uncertainty with standard errors, and from these standard errors we can compute confidence intervals and p-values. It is good practice to use a series of diagnostic plots, diagnostic statistics, and simulation to check how well the data approximate the fit model and model assumptions. Model checking is used to both check our subjective confidence in the modeled estimates and uncertainty and to provide empirical evidence for subjective decision making in the analysis workflow.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Model checking</span>"
    ]
  },
  {
    "objectID": "chapters/model-checking.html#check-check",
    "href": "chapters/model-checking.html#check-check",
    "title": "11  Model checking",
    "section": "",
    "text": "NHST Blues\n\n\n\nResearchers are often encouraged by textbooks, colleagues, or the literature to test the assumptions of a t-test or ANOVA with formal hypothesis tests of distributions such as a Shapiro-Wilks test of normality or a Levine test of homogeneity. In this strategy, an alternative to the t-test/ANOVA is used if the distribution test’s p-value is less than some cut-off (such as 0.05). Common alternatives include 1) transformations of the response to either make it more normal or the variances more homogenous, 2) implementation of alternative tests such as a Mann-Whitney-Wilcoxon (MWW) test for non-normal data or a Welch t-test/ANOVA for heterogenous variances. The logic of a test of normality or homogeneity before a t-test/ANOVA is a bit inconsistent with frequentist thinking because the failure to reject a null hypothesis does not mean the null hypothesis is true. That is, we shouldn’t conclude that a sample is “normal” or that the variances are “homogenous” because a distributional test’s p-value &gt; 0.05.\nBut, maybe we should simply be pragmatic, and use a distributional pre-test as an “objective” model check. The logic of this objective decision rule suffers from several issues. First, for tests of normality, at least, a major issue is: Distribution tests are less likely to reject the null for small \\(n\\) (because of low power) than large \\(n\\) (because all real data is, at best, only approximately normal), but inference from confidence intervals and p-values from t-tests/ANOVAs are less reliable with small \\(n\\) than with large \\(n\\). As a consequence, researchers will often use normal-models when inference is least reliable and alternative models when they inference is most reliable. Second, a p-value of the ttest/ANOVA test following a distribution test is not strictly valid because a p-value is the long-run frequency of a test-statistic as large or larger than the observed statistic conditional on the null – not conditional on the subset of nulls with \\(p &gt; 0.05\\) in a distribution test. Third, and most importantly, our analysis should follow the logic of our goals. If our goal is the estimation of effects, we cannot get meaningful estimates from a non-parametric test (with a few exceptions) or a transformed response, as these methods are entirely about computing a “correct” p-value. Good alternatives to classic non-parametric tests and transformations are bootstrap estimates of confidence limits, permutation tests, and generalized linear models.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Model checking</span>"
    ]
  },
  {
    "objectID": "chapters/model-checking.html#linear-model-assumptions",
    "href": "chapters/model-checking.html#linear-model-assumptions",
    "title": "11  Model checking",
    "section": "11.2 Linear model assumptions",
    "text": "11.2 Linear model assumptions\nTo facilitate explanation of assumptions of the linear model and extensions of the linear model, I will use both the error-draw and conditional-draw specifications of a linear model with a single \\(X\\) variable.\nerror draw: \\[\n\\begin{align}\ny &= \\beta_0 + \\beta_1 x_i + \\varepsilon_i\\\\\n\\varepsilon_i &\\sim N(0, \\sigma^2)\n\\end{align}\n\\tag{11.1}\\]\nconditional draw: \\[\n\\begin{align}\ny_i &\\sim N(\\mu_i, \\sigma^2)\\\\\n\\mathrm{E}(Y|X=x_i) &= \\mu_i\\\\\n\\mu_i &= \\beta_0 + \\beta_1 x_i\n\\end{align}\n\\tag{11.2}\\]\nThis model generates random data using the set of rules specified in the model equations. To quantify uncertainty in our estimated parameters, including standard errors, confidence intervals, and p-values, we make the assumption that the data from the experiment is a random sample generated using these rules.\nThe two rules specified in the model above are\n\nThe systematic component of data generation is \\(\\beta_0 + \\beta_1 X\\).\n\n\nMore generally, all linear models in this text specify systematic components that are linear in the parameters. Perhaps a better name for this is “additive in the parameters”. Additive (or linear) simply means that we can add up the products of a parameter and an \\(X\\) variable to get the conditional expectation \\(\\mathrm{E}(Y|X)\\).\nFor observational inference, the rule \\(\\mathrm{E}(Y| see\\;X=x_i) = \\mu_i\\) is sufficient. For causal inference with data from an experiment, or with observational data and a well defined causal diagram, we would need to modify this to \\(\\mathrm{E}(Y|do\\; X=x_i) = \\mu_i\\).\n\n\nThe stochastic component of data generation is “IID Normal”, where IID is independent and identically distributed and Normal refers to the Normal (or Gaussian) distribution. The IID assumption is common to all linear models. Again, for the purpose of this text, I define a “linear model” very broadly as a model that is linear in the parameters. This includes many extensions of the classical linear model including generalized least squares linear models, linear mixed models, generalized additive models, and generalized linear models. Parametric inference form all linear models requires the specification of a distribution family to sample. Families that we will use in this book include Normal, gamma, binomial, poisson, and negative binomial. This text will also cover distribution free methods of quantifying uncertainty using the bootstrap and permutation, which do not specify a sampling distribution family.\n\n\n11.2.1 A bit about IID\n\nIndependent means that the random draw for one case cannot be predicted from the random draw of any other case. A lack of independence creates correlated error. There are lots or reasons that errors might be correlated. Multiple measures from the same mouse/litter/cage/preparation are expected to be more similar to each other than to measures from different mice/litters/cages/preparations. Multiple measures within any unit that shares variance not shared by other units creates “clusters” of error. Lack of independence or clustered error can be modeled using generalized least squares (GLS) models that directly model the structure of the error and with random effects models. Random effects models go by many names including linear mixed models (common in Ecology), hierarchical models, and multilevel models. Both GLS and random effects models are variations of linear models.\nIdentical means that all random draws at a given value of \\(X\\) are from the same distribution. Using the error-draw specification of the model above, this can be restated as, the error-draw (\\(\\varepsilon_i\\)) for every \\(i\\) is from the same distribution \\(N(0, \\sigma^2\\)). Using the conditional-draw specification, this can be restated as, the random-draw \\(y_i\\) for every \\(i\\) with the same expected value \\(\\mu = \\mu_i\\) is from the same distribution \\(N(\\mu_i, \\sigma^2\\)). Understand the importance of this. Parametric inference using this model assumes that the sampling variance of \\(\\mu\\) at a single value of \\(X\\) is the same for all values of \\(X\\). If \\(X\\) is continuous, this means the spread of the points around the regression line is the same at all values of \\(X\\) in the data. If \\(X\\) is categorical, this means the spread of the points around the mean of a group is the same for all groups. A consequence of “identical”, then, for all classical linear models, is the assumption of homogeneity (or homoskedasticity) of variance. If the sampling variance differs among the \\(X\\), then the variances are heterogenous or heteroskedastic. Experimental treatments can affect the variance of the response in addition to the mean of the response. Heterogenous variance can be modeled using Generalized Least Squares (GLS) linear models. Many natural biological processes generate data in which the error is a function of the mean. For example, measures of biological variables that grow, such as size of body parts, have variances that “grow” with the mean. Or, measures of counts, such as the number of cells damaged by toxin, the number of eggs in a nest, or the number of mRNA transcripts per cell have variances that are a function of the mean. Both growth and count measures can sometimes be reasonably modeled using a linear model but more often, they are better modeled using a Generalized Linear Model (GLM).",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Model checking</span>"
    ]
  },
  {
    "objectID": "chapters/model-checking.html#diagnostic-plots-use-the-residuals-from-the-model-fit",
    "href": "chapters/model-checking.html#diagnostic-plots-use-the-residuals-from-the-model-fit",
    "title": "11  Model checking",
    "section": "11.3 Diagnostic plots use the residuals from the model fit",
    "text": "11.3 Diagnostic plots use the residuals from the model fit\n\n11.3.1 Residuals\nA residual of a statistical model is \\(y_i - \\hat{y}_i\\). Remember that \\(\\hat{y}_i\\) is the predicted value of \\(Y\\) when \\(X\\) has the value \\(x_i\\) (compactly written as \\(X=x_i\\)). And remember that \\(\\hat{y}_i\\) is the estimate of \\(\\mu_i\\). For linear models (but not generalized linear models), the residuals of the fit model are estimates of the \\(\\varepsilon\\) in Equation 11.1. This is not true for generalized linear models because GLMs are not specified using Equation 11.1.\n\n\n\n\n\n\nAlert\n\n\n\nA common misconception is that inference from a linear model assumes that the response (the measured \\(Y\\)) is IID Normal. This is wrong. Either specification of the linear model shows precisely why this conception is wrong. Equation 11.1 explicitly shows that it is the error that has the normal distribution – the distribution of \\(Y\\) is a mix of the distribution of \\(X\\) and that of the error. A more general way of thinking about the assumed distribution uses the specification in Equation 11.2, which shows that it is the conditional response that is assumed to be IID normal. Remember, a conditional response is a random draw from the infinite set of responses at a given value of \\(X\\)\n\n\nLet’s look at the distribution of residuals versus the distribution of responses for a hypothetical experiment with a single, categorical \\(X\\) variable (the experimental factor) with two levels (“Cn” for control and “Tr” for treatment). The true parameters are \\(\\beta_0 = 10\\) (the true mean for the control group, or \\(\\mu_{0}\\)), \\(\\beta_1=4\\) (the difference between the true mean for the treatment minus the true mean for the control, or \\(\\mu_1 - \\mu_0\\)), and \\(\\sigma = 2\\) (the error standard deviation).\n\n\n\n\n\n\n\n\nFigure 11.1: Histogram of the (A) response, showing two modes, one near the true mean of each group, and (B) residuals, with a mode for both groups at zero.\n\n\n\n\n\nThe plot above shows a histogram of the response (A) and residuals (B). In the plot of the response, the mode (the highest bar, or bin with the most cases) includes true mean for each group. And, as expected given \\(\\beta_1=4\\), the modes of the two groups are 4 units apart. It should be easy to see from this plot that the response does not have a normal distribution. Instead, it is distincly bimodal. But the distribution of the response within each level looks like these are drawn from a normal distribution – and it should. In the plot of the residuals, the values of both groups are shifted so that the mean of each group is at zero. The consequence of the shift is that the combined set of residuals does look like it is drawn from a Normal distribution.\nThe two plots suggest two different approaches for model checking. First, we could examine the responses within each level of the experimental factor. Or, second, we could examine the residuals of the fit model, ignoring that the residuals come from multiple groups. The first is inefficient because it requires as many checks as there are levels in the factor. The second requires a single check.\n\n\n\n\n\n\nAlert\n\n\n\nSome textbooks that recommend formal hypothesis tests of normality recommend the inefficient, multiple testing on each group separately. This isn’t wrong, it’s just more work than it needs to be and also suffers from “multiple testing”.\n\n\n\n\n11.3.2 A Normal Q-Q plot is used to check for characteristic departures from Normality\nA Normal Q-Q plot is a scatterplot of\n\nsample quantiles on the y axis. The sample quantiles is the vector of \\(N\\) residuals in rank order, from smallest (most negative) to largest (most positive). Sometimes this vector is standardized by dividing the residual by the standard deviation of the residuals (doing this makes no difference to the interpretation of the Q-Q plot).\nstandard normal quantiles on the x axis. This is the vector of standard, Normal quantiles given \\(N\\) elements in the vector. “Standard Normal” means a normal distribution with mean zero and standard deviation (\\(\\sigma\\)) one. A Normal quantile is the expected deviation given a probability. For example, if the probability is 0.025, the Normal quantile is -1.959964. Check your understanding: 2.5% of the values in a Normal distribution with mean 0 and standard deviation one are more negative than -1.959964. The Normal quantiles of a Normal Q-Q plot are computed for the set of \\(N\\) values that evenly split the probability span from 0 to 1. For \\(N=20\\), this would be\n\n\np &lt;- data.frame(quantile = qnorm(ppoints(1:20)))\nrow.names(p) &lt;- ppoints(1:20)\np\n\n         quantile\n0.025 -1.95996398\n0.075 -1.43953147\n0.125 -1.15034938\n0.175 -0.93458929\n0.225 -0.75541503\n0.275 -0.59776013\n0.325 -0.45376219\n0.375 -0.31863936\n0.425 -0.18911843\n0.475 -0.06270678\n0.525  0.06270678\n0.575  0.18911843\n0.625  0.31863936\n0.675  0.45376219\n0.725  0.59776013\n0.775  0.75541503\n0.825  0.93458929\n0.875  1.15034938\n0.925  1.43953147\n0.975  1.95996398\n\n\nA Normal Q-Q plot is not a test if the data are Normal. Instead, a Normal Q-Q plot is used to check for characteristic departures from Normality that are signatures of certain well-known distribution families. A researcher can look at a QQ-plot and reason that a departure is small and choose to fit a classic linear model using the Normal distribution. Or, a researcher can look at a QQ-plot and reason that the departure is large enough to fit a generalized linear model with a specific distribution family.\n\n\n\n\n\n\nStats 101\n\n\n\nA quantile is the value of a distribution that is greater than \\(p\\) percent of the values in the distribution. The 2.5% quantile of a uniform distribution from 0 to 1 is 0.025. The 2.5% quantile of a standard normal distribution is -1.96 (remember that 95% of the values in a standard normal distribution are between -1.96 and 1.96). The 50% quantile of a uniform distribution is 0.5 and the 50% quantile of a standard normal distribution is 0.0 (this is the median of the distribtion – 50% of the values are smaller and 50% of the values are larger).\n\n\n\n\n\n\n\n\nStats 201\n\n\n\nA Q-Q plot more generally is a scatter plot of two vectors of quantiles either of which can come from a sample or a theoretical distribution. In the GLM chapter, the text will introduce Q-Q plots of residual quantiles transformed to have an expected uniform distribution. These are plotted against theoretical uniform quantiles from 0 to 1.\n\n\n\n11.3.2.1 Let’s construct a Normal Q-Q plot\nA quantile (or percentile) of a vector of numbers is the value of the point at a specified percentage rank. The median is the 50% quantile. The 95% confidence intervals are at the 2.5% and 97.5% quantiles. In a Normal Q-Q plot, we want to plot the quantiles of the residuals against a set of theoretical quantiles.\n\nTo get the observed quantiles, rank the residuals of the fit linear model from most negative to most positive – these are your quantiles! For example, if you have \\(n=145\\) residuals, then the 73rd point is the 50% quantile.\nA theoretical quantile from the normal distribution can be constructed using the qnorm function which returns the normal quantiles for a specified vector of percents. Alternatively, one could randomly sample \\(n\\) points using rnorm. These of course will be sampled quantiles so will only approximate the expected theoretical quantiles, but I add this here because we use this method below.\n\nNow simply plot the observed against theoretical quantiles. Often, the standardized quantiles are plotted. A standardized variable has a mean of zero and a standard deviation of one and is computed by 1) centering the vector at zero by subtracting the mean from every value, and 2) dividing each value by the standard deviation of the vector. Recognize that because a standard deviation is a function of deviations from the mean, it doesn’t matter which of these operations is done first. A standardized theoretical quantile is specified by qnorm(p, mean = 0, sd = 1), which is the default.\nCode for this would look something like this\n\n# try this with real data. Here I just make it up\nfake_data &lt;- data.table(\n  treatment = rep(c(\"cn\", \"tr\"), each = 10),\n  response = rnorm(20,\n                   mean = rep(c(10, 11), each = 10),\n                   sd = 1)\n)\nm1 &lt;- lm(response ~ treatment, data = fake_data)\nfake_data[, y_residual := residuals(m1)]\n\nquantile_observed &lt;- fake_data[, y_residual] %&gt;%\n  scale() %&gt;%\n  sort()\n\nN &lt;- length(quantile_observed)\nq &lt;- N + 1\nx &lt;- seq(1/q, 1 - 1/q, by = 1/q)\nquantile_theoretical &lt;- qnorm(x)\n\n# this is not the robust regression line\nqplot(x = quantile_theoretical, y = quantile_observed) +\n  geom_abline(slope = 1, intercept = 0) +\n  xlab(\"theoretical normal quantile\") +\n  ylab(\"observed normal quanitle\") +\n  theme_pubr()\n\nWarning: `qplot()` was deprecated in ggplot2 3.4.0.\n\n\n\n\n\n\n\n\n\n\n\n11.3.2.2 Normal QQ-plot of the fake data generated in Figure 11.1\nIf the sampled distribution approximates a sample from a normal distribution, the scatter should fall along a line from the bottom, left to the top, right of the plot. The interpretation of a normal Q-Q plot is enhanced with a line of “expected values” of the sample quantiles if the sample residuals are drawn from a normal distribution. The closer the sample quantiles are to the line, the more closely the residuals approximate the expectation from a normal distribution. Because of sampling, the sampled values always deviate from the line, especially at the ends. The shaded gray area in the Q-Q plot in Figure ?fig-modelcheck-qq are the 95% confidence bands of the quantiles. A pattern of observed quantiles with some individual points outside of these boundaries indicates a sample that would be unusual if sampled from a Normal distribution.\nBiological datasets frequently have departures on a Normal Q-Q plot that are characteristic of specific distribution families, including lognormal, binomial, poisson, negative binomial, gamma, and beta. It is useful to learn how to read a Normal Q-Q plot to help guide how to model your data.\nWhat about the intepretation of the Q-Q plot in Figure @ref(fig:model-check-qq)? At the small end of the distribution (bottom-left), the sample values are a bit more negative than expected, which means the left tail is a bit extended. At the large end (upper-right), the sample values are, a bit less positive than expected, which means the right tail is a bit shortened. This is a departure in the direction of a left skewed distribution. Should we fit a different model given these deviations? To guide us, we compare the quantiles to the 95% confidence band of the quantiles. Clearly the observed quantiles are within the range of quantiles that we’d expect if sampling from a Normal distribution.\n\n\n\n11.3.3 Mapping QQ-plot departures from Normality\nLet’s look at simulated samples drawn from non-normal distributions to identify their characteristic deviations. Each set of plots below shows\n\n(left panel) A histogram of 10,000 random draws from the non-Normal distribution (blue). This histogram is superimposed over that of 10,000 random draws from a Normal distribution (orange) with the same mean and variance as that of the non-normal distribution.\n(middle panel) Box plots and strip chart of a random subset (\\(N=1000\\)) of data in the left panel.\n(right panel) Normal Q-Q plot of the non_Normal data only.\n\nSkewed-Right Q-Q\n\n\n\n\n\n\n\n\nFigure 11.2: Comparison of samples from a right skewed (Negative Binomial) and Normal distribution.\n\n\n\n\n\nThe Normal Q-Q plot of a sample from a right-skewed distribution is characterized by sample quantiles at the high (right) end being more positive than the expected Normal quantiles. Often, the quantiles at the low (left) end are also less negative than the expected normal quantiles. The consequence is a concave up pattern.\nThe histograms in the left panel explain this pattern. The right tail of the skewed-right distribution extends further than the right tail of the Normal. It is easy to see from this that, if we rank the values of each distribution from small to large (these are the quantiles), the upper quantiles of the skewed-right distribution will be larger than the matching quantile of the Normal distribution. For example, the 99,990th quantile for the skewed-right distribution will be much more positive than the 99,990th quantile for the Normal distribution. The opposite occurs at the left tail, which extends further in the negative direction in the Normal than the skewed-right distribution.\nThe middle panel compares a boxplot and stripchart of samples from the two distributions to show what researchers should look for in their own publication-ready plots as well as the published plots of colleagues. The skewed-right plot exhibits several hallmarks of a skewed-right distribution including 1) a median line (the horizontal line within the box) that is closer to the 25th percentile line (the lower end of the box) than to the 75th percentile line (the upper end of the box), 2) a longer upper than lower whisker (the vertical lines extending out of the box), 3) more outliers above the upper whisker than below the lower whisker, and 4) a lengthened, upward smear of the scatter of points at the high end of the values, relative to the more compact smear at the low end of the values.\nSkewed-Left Q-Q\n\n\n\n\n\n\n\n\nFigure 11.3: Comparison of samples from a left skewed (upper end of Beta) and Normal distribution.\n\n\n\n\n\nThe Normal Q-Q plot of a sample from a left-skewed distribution is characterized by sample quantiles at the low (left) end being more negative than the expected Normal quantiles. Often, the quantiles at the high (right) end are also less positive than the expected normal quantiles. The consequence is a concave down pattern.\nThe histograms in the left panel explain this pattern. The left tail of the skewed-left distribution extends further than the left tail of the Normal. It is easy to see from this that, if we rank the values of each distribution from small to large (these are the quantiles), the lower quantiles of the skewed-left distribution will be more negative than the matching quantile of the Normal distribution. For example, the 10th quantile for the skewed-left distribution will be much more negative than the 10th quantile for the Normal distribution. The opposite occurs at the right tail, which extends further in the positive direction in the Normal than the skewed-left distribution.\nThe skewed-left plot in the middle panel highlights several hallmarks of a skewed-left distribution including 1) a median line (the horizontal line within the box) that is closer to the 75th percentile line (the lower end of the box) than to the 25th percentile line (the upper end of the box), 2) a longer lower than upper whisker (the vertical lines extending out of the box), 3) more outliers below the lower whisker than above the upper whisker, and 4) a lengthened, downward smear of the scatter of points at the low end of the values, relative to the more compact smear at the upper end of the values.\nHeavy Tail Q-Q\n\n\n\n\n\n\n\n\nFigure 11.4: Comparison of samples from a heavy tailed (t) and Normal distribution.\n\n\n\n\n\nThe Normal Q-Q plot of a sample from a heavy-tail distribution is characterized by sample quantiles at the low (left) end being more negative than the expected Normal quantiles and quantiles at the high (right) end that are more positive than the expected normal quantiles.\nThe histograms in the left panel explain this pattern. At each tail, the heavy-tail distribution has more density – there are more values far from the mean – compared to the Normal distribution. This is the origin of “heavy tail”. It is easy to see from this that, if we rank the values of each distribution from small to large (these are the quantiles), the lower quantiles of the heavy tail distribution will be more negative than the matching quantile of the Normal distribution. For example, the 10th quantile for the heavy-tail distribution will be much more negative than the 10th quantile for the Normal distribution. Likewise, the upper quantiles of the heavy tail distribution will be more positive than the matching quantile of the Normal distribution. For example, the 99,990th quantile for the heavy-tail distribution will be much more positive than the 99,990th quantile for the Normal distribution.\nThe heavy-tail plot in the middle panel shows more boxplot outliers than in the Normal plot. This would be hard to recognize in a plot of real data.\n\n\n\n\n\n\nMapping characteristic departures on a Q-Q plot to specific distributions\n\n\n\n\nContinuous response variables of length, area, weight, or duration will often look like samples from a continous probability distribution that is right-skewed, such as the lognormal or gamma distributions.\nCount response variables will frequently look like samples from a discrete probability distribution that is right-skewed, such as the poisson, quasi-poisson, or negative binomial distributions.\nProportion (fraction of a whole) response variables will frequently look like samples from a continuous probability distribution bounded by 0 and 1, such as the beta distribution. Samples from a beta distribution can be left skewed, if the mean is near 1, right-skewed, if the mean is near zero, or symmetrical, if the mean is near 0.5.\n\n\n\n\n11.3.3.1 Pump your intuition – confidence bands of a Q-Q plot\nIn introducing the confidence bands of the Q-Q plot above, I stated “A pattern of observed quantiles with some individual points outside of these boundaries indicates a sample that would be unusual if sampled from a Normal distribution.” Let’s use a parametric bootstrap to explore this.\n\nSample \\(n\\) values from a Normal distribution\nCompute the sample quantiles by re-ordering the residuals of the sampled values from the sampled mean, from most negative to most positive.\nPlot the quantiles against Normal quantiles for \\(n\\) points.\nRepeat steps 1-3 \\(n\\_iter\\) times, superimposing the new sample quantiles over all previous sample quantiles. This creates a band of all sample quantiles over \\(n\\_iter\\) iterations of sampling \\(n\\) values from a Normal distribution.\nAt each value of the Normal quantile, compute the 95 percentile range of the sampled quantiles. Draw a ribbon inside these boundaries.\n\n\nn_iter &lt;- 1000\nn &lt;- 20\nnormal_qq &lt;- ppoints(n) %&gt;%\n  qnorm()\nsample_qq &lt;- numeric(n_iter*n)\ninc &lt;- 1:n\nfor(iter in 1:n_iter){\n  y &lt;- rnorm(n)\n  y_res &lt;- y - mean(y)\n  sample_qq[inc] &lt;- y_res[order(y_res)]\n  inc &lt;- inc + n\n}\n\nqq_data &lt;- data.table(normal_qq = normal_qq,\n                      sample_qq = sample_qq)\n\nqq_ci &lt;- qq_data[, .(median = median(sample_qq),\n                     lower = quantile(sample_qq, 0.025),\n                     upper = quantile(sample_qq, 0.975)),\n                 by = normal_qq]\n\nggplot(data = qq_data,\n       aes(x = normal_qq,\n           y = sample_qq)) +\n  geom_point(alpha = 0.2) +\n  geom_ribbon(data = qq_ci,\n              aes(ymin = lower,\n                  ymax = upper,\n                  y = median,\n                  fill = \"band\"),\n              fill = pal_okabe_ito[1],\n              alpha = 0.3) +\n  xlab(\"Normal Quantile\") +\n  ylab(\"Sample Quantile\") +\n  theme_grid() +\n  \n  NULL\n\n\n\n\n11.3.4 Model checking homoskedasticity",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Model checking</span>"
    ]
  },
  {
    "objectID": "chapters/model-checking.html#working-in-r",
    "href": "chapters/model-checking.html#working-in-r",
    "title": "11  Model checking",
    "section": "11.4 Working in R",
    "text": "11.4 Working in R\nLet’s use a fake dataset to show qq plots generated by different packages.\n\nset.seed(20)\nn &lt;- 50\nmu &lt;- 10\nsigma &lt;- 2\nbeta &lt;- sigma*2\nfake_data &lt;- data.table(\n  treatment = rep(c(\"Cn\", \"Tr\"), each=n),\n  response = c(rnorm(n, mean = mu, sd = sigma),\n               rnorm(n, mean = mu + beta, sd = sigma))\n)\n\nm1 &lt;- lm(response ~ treatment, data = fake_data)\n\n\n11.4.1 ggcheck_the_qq from the ggplot_the_model source code\nggcheck_the_qq from the ggplot_the_model source code replicates the default behavior of the car::qqPlot (see below), which is a robust line through the points with a 95% confidence interval.\n\nset.seed(1)\nggcheck_the_qq(m1)\n\n\n\n\n\n\n\n\nThe practice in this text is to print both the qqplot and the spread-level plot using ggcheck_the_model\n\nset.seed(1)\nggcheck_the_model(m1)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n\n11.4.2 qqPlot from the car package\ncar::qqPlot has several important arguments to control the type of Q-Q plot. The function uses base graphics instead of ggplot2. Typically, these plots would not be published other than possibly a supplement. Q-Q Plots and Worm Plots from Scratch is a good source of some of the arguments in qqPlot. Three important arguments are:\n\nsimulate. If passing a lm object, then the default confidence band is generated by a parametric bootstrap (simulate = TRUE). This band will differ somewhat each time you replot unless you set the seed with set.seed. Setting the argument simulate = FALSE returns the parametric band.\nline. If passing a lm object, then the default line is a fit from a robust regression (line = \"robust\"). Setting the argument line = \"quartiles\" fits a line throught the 25th and 75th percentile (or “quartiles”) quantiles.\nid. The default identifies the index of the two points with the most extreme quartiles. Set to FALSE to hide.\n\nThe robust line is more sensitive to departures from Normality than the quartiles line.\n\n# defaults: robust line with bootstrap CI\nset.seed(1)\nqqPlot(m1, id = FALSE)\n\n\n\n\n\n\n\n\n\n# classic: standard line with parametric CI\nqqPlot(m1,\n       line = \"quartiles\",\n       simulate = FALSE,\n       id = FALSE)\n\n\n\n11.4.3 ggqqplot from the ggpubr package\nggpubr::ggqqplot generates a pretty, ggplot2 based Normal Q-Q plot, using the standard method for computing the line and confidence band.\n\nm1_residuals &lt;- data.table(m1_residuals = residuals(m1))\nm1_residuals[, studentized := m1_residuals/sd(m1_residuals)]\nggqqplot(data = m1_residuals,\n                          x = \"studentized\")",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Model checking</span>"
    ]
  },
  {
    "objectID": "chapters/model-checking.html#hidden-code",
    "href": "chapters/model-checking.html#hidden-code",
    "title": "11  Model checking",
    "section": "11.5 Hidden Code",
    "text": "11.5 Hidden Code\nSource: Wellenstein, M.D., Coffelt, S.B., Duits, D.E., van Miltenburg, M.H., Slagter, M., de Rink, I., Henneman, L., Kas, S.M., Prekovic, S., Hau, C.S. and Vrijland, K., 2019. Loss of p53 triggers WNT-dependent systemic inflammation to drive breast cancer metastasis. Nature, 572(7770), pp.538-542.\nPublic source\nData source\n\ndata_from &lt;- \"Loss of p53 triggers WNT-dependent systemic inflammation to drive breast cancer metastasis\"\nfile_name &lt;- \"41586_2019_1450_MOESM3_ESM.xlsx\"\nfile_path &lt;- here(data_folder, data_from, file_name)\n  \ntreatment_levels &lt;- c(\"Trp53+/+\", \"Trp53-/-\")\nfig1f &lt;- read_excel(file_path,\n                     sheet = \"Fig. 1f\",\n                     range = \"A2:B23\") %&gt;%\n  data.table() %&gt;%\n  melt(measure.vars = treatment_levels,\n       variable.name = \"treatment\",\n       value.name = \"il1beta\")\nfig1f[, treatment := factor(treatment, treatment_levels)]\n\n# be careful of the missing data. This can create mismatch between id and residual unless specified in lm\n\n# head(fig1f)\n\nFit a linear model\n\nm1 &lt;- lm(il1beta ~ treatment,\n         data = fig1f)",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Model checking</span>"
    ]
  },
  {
    "objectID": "chapters/multiple-testing.html",
    "href": "chapters/multiple-testing.html",
    "title": "12  Multiple tests – why and when to adjust p-values",
    "section": "",
    "text": "12.1 In a family of multiple tests, what is the probability of at least one p-value less than 0.05?\nA family of tests is a set of tests that are used to make some inference about one question. For example, “In this mouse model of diabetes, are any genes differently expressed between diabetes mice and wildtype mice”? \\(p &lt; 0.05\\) for any of the tested genes answers the question. If a researcher only tests one gene, then the probability of \\(p &lt; 0.05\\) if there is no expression difference between phenotypes is 5%. But if a researcher tests two genes, there is a higher probability than 5% that at least one will have \\(p &lt; 0.05\\) if there is no expression difference between phenotypes for both genes. And, if a researcher tests ten genes, there is a much higher probability than 5% that at least one will have \\(p &lt; 0.05\\) if there is no expression difference between phenotypes for any of the ten genes. What are these probabilities?\nThe probability of a significant p-value in one test is 0.05, so the probability of no non-significant p-values in one test is 1 - 0.05. The probability of no non-significant p-values in two tests is \\((1 - 0.05)^2\\), so the probability of at least one significant p-value is \\(1 - (1 - 0.05)^2\\). The probability of no non-significant p-values in ten tests is \\((1 - 0.05)^{10}\\), so the probability of at least one significant p-value in ten tests is \\(1 - (1 - 0.05)^{10}\\). Let’s compute these probabilities, and more.\nFamily-wise error rate for unadjusted p-values\n\n\nNumber of tests\nProb(at least 1 &lt; 0.05)\n\n\n\n\n1\n0.05\n\n\n2\n0.10\n\n\n5\n0.23\n\n\n10\n0.40\n\n\n100\n0.99\nThe second column is the family-wise error rate (FWER), the probability of at least one \\(p &lt; 0.05\\) in a family of tests in which there are no true effects. So the FWER for a family of 10 tests is 40% if our p-values are not adjusted for multiple testing. We adjust p values in experiments with multiple tests in order to control the FWER.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Multiple tests -- why and when to adjust *p*-values</span>"
    ]
  },
  {
    "objectID": "chapters/multiple-testing.html#sec-fwer",
    "href": "chapters/multiple-testing.html#sec-fwer",
    "title": "12  Multiple tests – why and when to adjust p-values",
    "section": "",
    "text": "Note\n\n\n\nAn assumption of these probabilities is the tests are independent. Importantly, this independence assumption is also true for most, but not all, of the adjustment methods.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Multiple tests -- why and when to adjust *p*-values</span>"
    ]
  },
  {
    "objectID": "chapters/multiple-testing.html#example-1-in-many-experiments-we-dont-want-to-adjust-for-multiple-tests-exfig2d-data",
    "href": "chapters/multiple-testing.html#example-1-in-many-experiments-we-dont-want-to-adjust-for-multiple-tests-exfig2d-data",
    "title": "12  Multiple tests – why and when to adjust p-values",
    "section": "12.2 Example 1 – In many experiments, we don’t want to adjust for multiple tests (exfig2d data)",
    "text": "12.2 Example 1 – In many experiments, we don’t want to adjust for multiple tests (exfig2d data)\nIn many experiments, we don’t want to adjust for multiple tests because there is only a single test in the family even if there are multiple tests in the experiment. Researchers are adjusting because of a confusion on what a family is. The cost of this adjustment is more conservative (higher) p-values, which leads to less power, which leads to more failed discoveries or more research costs (mice, time) to discover by p-value.\nLet’s explore what a family is using data from the article\nSource: Lyu, Q., Xue, W., Liu, R. et al. A brain-to-gut signal controls intestinal fat absorption. Nature (2024). https://doi.org/10.1038/s41586-024-07929-5\nData source\nFigure: Extended Figure 2d\nThe import code is in Section 12.7.1 below.\nIn this study, the researchers investigate the control of fat absorption in the intestine by the dorsal motor nucleus of the vagus nerve (DMV) and show that the plant metabolite puerarin mimics this DMV suppression.\nThe experiment in Extended Figure 2d follows up initial experiments from Figure 1 that show that inactivation of the DMV in mice decreased weight gain on a high fat diet relative to control mice and that DMV-inactive mice had lower plasma triglycerides, higher fat content in the feces, and lower fat absorption in the jejunum part of the small intestine. The DMV was inactivated by Clozapine N-oxide (CNO) binding to a human-designed hM4D(Gi) inhibitory receptor whose expression is controlled by the researchers.\nIn the experiment in Extended Figure 2d, the response is fecal NEFA (non-esterified fatty acid).\nThe three treatments are\n\n“con” is a negative control with a designer receptor that doesn’t bind CNO. The researchers expect fecal NEFA equal to mice with no designed receptors.\n“3q” is the focal treatment using a designer receptor that should activate the DMV. If the DMV regulation of fat absorption is working as thought, the researchers expect increased fat absorption so lower fecal NEFA levels relative to “con”.\n“4i” is a positive control using the DMV-inactivating receptor in the previous experiments. The researchers expect higher NEFA levels relative to “con”, as in the experiments in Fig. 1.\n\nThere are three contrasts of single treatments (“3q - con”, “4i - con”, “4i - 3q”) so three tests. The researchers follow the norm in experimental bench biology research and report p-values adjusted for multiple tests. Specifically, the researchers adjusted for the two contrasts comparing a treatment to the negative control (“3q - con” and “4i - con”). This adjustment seems to follow the logic that a family is the set of tests of interest to the researchers (the contrast “4i - 3q” is not of interest). But “of interest to the researchers” doesn’t define a family of tests. Instead, “same question” defines a family of tests. If were were to include both contrasts in a family, what is the question? It would have to be something like “if we manipulate the activity of the DMV, do we get a change in fecal NEFA?”.\nThis isn’t the question addressed by the research. The researchers have one focal question: “does activating the DMV increase fat absorption and consequently, decrease fecal NEFA levels?” Only one test (the contrast “3q - con”) answers this question so the family has a single test and there is no need to adjust. The researchers are interested in a second question as a check on the experiment itself. This question is something like “Are we actually getting the known response given the intervention”. Only one test (the contrast “4i - con”) answers this question so there is no need to adjust.\nLet’s look at the cost of adjustment. The researchers used the Holm-Sidák method to adjust the two p-values.\n\nm1 &lt;- lm(nefa ~ treatment, data = exfig2)\nm1_emm &lt;- emmeans(m1, specs = \"treatment\")\nm1_pairs &lt;- contrast(m1_emm,\n                     method = \"trt.vs.ctrl\",\n                     adjust = \"none\") |&gt;\n  summary(infer = TRUE) |&gt;\n  data.table()\n\nm1_pairs[, holm_sidak := holm_sidak(p.value)]\nm1_pvals &lt;- m1_pairs[, .SD, .SDcols = c(\"contrast\", \"estimate\", \"p.value\", \"holm_sidak\")]\ncolnames(m1_pvals) &lt;- c(\"Contrast\", \"Estimate\", \"Unadjusted P\", \"Holm-Sidak P\")\n\nm1_pvals |&gt;\n  kable(digits = c(1, 3, 5, 5)) |&gt;\n  kable_styling(full_width = FALSE)\n\n\n\nTable 12.1: Unadjusted p-value and Holm-Sidak adjusted p-value for the experiment in Extended Figure 2d\n\n\n\n\n\n\nContrast\nEstimate\nUnadjusted P\nHolm-Sidak P\n\n\n\n\n3q - con\n-1.090\n0.01261\n0.02506\n\n\n4i - con\n1.048\n0.01361\n0.02506\n\n\n\n\n\n\n\n\n\n\nThe cost is about double because there are two tests. If we use \\(p &lt; 0.05\\) to act as if there is an effect, then the adjustment costs us nothing here. Adjusting for a small (two to three) number of tests is not going to have much impact on the quality of science either way. Nevertheless\n\nadjusting is unnecessary in experiments like this, where there are multiple questions addressed by the experiment and each question is answered by a single test. This is the case for many, many experiments in experimental bench biology.\nalthough small, the increased power with not adjusting is free.\n\n\n12.2.1 Or, maybe we do want to adjust, but not in the way that is usually done\nExtended figure 2d presents only one of six response variables that were measured on each mouse. The other response variables were body weight, fecal triglyceride, jejunal triglyceride, duodenal triglyceride, and ileal triglyceride. Some thoughts:\n\nFecal NEFA and Fecal triglyceride effectively answer the same focal question “does activating the DMV increase fat absorption and consequently, decrease fecal fat levels?” So, adjusting for two tests is probably justified but the two tests are the same contrast (“3q - con”) with two different response variables and not different contrasts for the same response variable.\njejunal, duodenal, and ileal triglyceride effectively answer the same focal question “does activating the DMV increase fat absorption in a specific part of the small intestine”. So adjusting for three tests is probably justified here, but again, the adjustment is for the same contrast (“3q - con”) from the model fit to three different response variables.\nAdjustment for multiple responses is common in some subfields of experimental bench biology that are measuring thousands of responses (gene expression, functional MRI) but I I haven’t seen adjustment for multiple phenotypic responses from a single experiment.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Multiple tests -- why and when to adjust *p*-values</span>"
    ]
  },
  {
    "objectID": "chapters/multiple-testing.html#sec-mult-tests-tukey",
    "href": "chapters/multiple-testing.html#sec-mult-tests-tukey",
    "title": "12  Multiple tests – why and when to adjust p-values",
    "section": "12.3 Example 2 – Tukey adjustment is extremely common, and (mostly) unjustified (fig5l data)",
    "text": "12.3 Example 2 – Tukey adjustment is extremely common, and (mostly) unjustified (fig5l data)\nA very common research design in experimental bench biology is a design with two factors, each with two levels, with mice assigned to all 2 x 2 = 4 combinations of the levels. Analysis of these models for two (or more) factors is the focus of Chapter 13. The experiment in Figure 5l is a good example of a 2 x 2 design.\nData source\nFigure: Figure 5l\nThe import code is in Section 12.7.2 below\nThe experiment in Fig. 5 follows up experiments that show that the plant metabolite puerarin binds to the GABA receptor encoded by Gabra1 and increases GABA’s inhibitory effect on the target neuron. In the experiment in Fig. 5, the researchers are investigating the mechanism of the effect of DVM-vagus supression on fat absorption in the jejunum. Figure 5c shows that suppressing DMV using COS activation of the designer 4i inhibitory receptor (as above) results in decreased jejunal microvilli length – the reduced length decreases surface area for absorption. Figure 5d shows that suppressing DMV results in reduced expression of four genes involved in microvilli growth and organization. The design of the experiment in Fig. 5c, d is the same as that in Extended fig 2d above.\nFig. 5l uses a 2 x 2 crossed design to investigate the effect of puerarin with and without a functional Gabra1 GABA receptor.\nHere, we’ll focus on the first gene, ezr, which encodes the protein ezrin.\nThe two factors are\n\ngenotype, with levels “WT” and “KO”. KO is the conditional knockout of Gabra1 in the DMV.\ntreatment, with levels “Vehicle” and “Puerarin”. Infusion of a saline solution (Vehicle) or a solution with Puerarin.\n\nAll levels of both factors are combined to create the four experimental combinations (the design is fully factorial)\n\n“WT Vehicle” is the combination “WT” + “Vehicle”. This is the negative control.\n“WT Puerarin” is the combination “WT” + “Puerarin”.\n“KO Vehicle” is the combination “KO” + “Vehicle”.\n“KO Puerarin” is the combination “KO” + “Puerarin”\n\nThe primary questions of the experiment are:\n\ndoes addition of puerarin reduce RNA expression in mice with functional Gabra1 GABA receptor?\ndoes deletion of Gabra1 remove this effect of puerarin?\n\nThe researchers answer these questions using these two contrasts\n\nWT Puerarin - WT Vehicle\nKO Puerarin - KO Vehicle\n\nSo, there are two research questions, each answered by a single contrast. There is nothing to adjust because each family consists of only a single test.\n\n\n\n\n\n\nNote\n\n\n\nLook at question 2 closely – its a comparison of contrast 1 and contrast 2. Ultimately, the researchers are asking about the effects of Puerarin with and without a functional Gabra1 GABA receptor, so there is a single primary contrast of interest\n(KO Puerarin - KO Vehicle) - (WT Puerarin - WT Vehicle)\nThis contrast is called the interaction effect or simply the interaction. The interaction is the difference in contrasts 1 and 2. An interaction is a difference of differences. The estimation of interaction effects almost always answers the focal question in 2 x 2 designs but is almost never estimated in experimental bench biology. More information on designs for two factors and interaction effects is in chapter (two-factors?)\n\n\nWhat did the researchers do? Of the six pairwise contrasts, only two of which they focussed on in the text, the researchers report the four “simple effects” in the figure and in the archived Excel file. These are:\n\n(WT Puerarin) - (WT Vehicle) answers the question “how does microvilli-related gene expression change when we add puerarin to mice with fuctional Gabra1?”. This is a positive control for contrast 2.\n(KO Puerarin) - (KO Vehicle) answers the question “how does microvilli-related gene expression change when we add puerarin to mice with knocked out Gabra1?”. This is a focal contrast.\n(KO Vehicle)) - (WT Vehicle) answers the question “how does microvilli-related gene expression change when we knock out Gabra1 and don’t give puerarin administration? This is a positive control for contrast 4.\n(KO Puerarin) - (WT Puerarin) answers the question “how does microvilli-related gene expression change when we knock out Gabra1 but give puerarin administration? This is a focal contrast.\n\nInstead of adjusting for the two tests of interest, or the four computed tests, the researchers followed the norm in experimental bench biology and used the Tukey HSD adjustment for a single family of six tests (all six pairwise tests). Perhaps the norm is following a definition of family as “all tests within an experiment”. Regardless, the adjustment logic in Fig. 5l is inconsistent with that in Fig 2d., where the researchers adjusted for the two tests of interest instead of the set of all three pairwise comparisons in the experiment. In Fig. 5l, the researchers adjust for all six pairwise comparisons, show only four contrasts in the figure and in the Excel file of the data, but only seem interested in the two contrasts that answer the focal questions.\nWhat is the cost of adjustment? In Table 12.2 below, for the two focal tests, I give the unadjusted p-values, the Tukey p-values based on all six pairwise tests, and the Holm p-values based on the four simple effects. The cost of the Tukey and Holm adjustments are given in the last two columns as the ratio of the adjusted to unadjusted p-value. This loss of power is an unnecessary and results from a confusion in the experimental bench biology community of what a family of tests mean. Loss of power leads to more time, more dollars, more animals, and more failed discoveries.\n\nm1 &lt;- lm(ezrin ~ genotype * treatment, data = fig5l)\nm1_emm &lt;- emmeans(m1, specs = c(\"genotype\", \"treatment\"))\nm1_pairs &lt;- contrast(m1_emm,\n                     method = \"revpairwise\",\n                     adjust = \"none\") |&gt;\n  summary(infer = TRUE) |&gt;\n  data.table()\n\nm1_tukey &lt;- contrast(m1_emm,\n                     method = \"revpairwise\") |&gt;\n  summary(infer = TRUE) |&gt;\n  data.table()\n\nm1_simple &lt;- contrast(m1_emm,\n                      method = \"revpairwise\",\n                      simple = \"each\",\n                      combine = TRUE,\n                      adjust = \"holm\") |&gt;\n  summary(infer = TRUE) |&gt;\n  data.table()\n\nm1_pairs[, tukey := m1_tukey$p.value]\nm1_pairs[c(1,2,5,6), holm := m1_simple[c(1,3,4,2), p.value]]\n\nm1_pairs[ , cost_tukey := tukey/p.value]\nm1_pairs[ , cost_holm := holm/p.value]\n\nm1_pvals &lt;- m1_pairs[, .SD, .SDcols = c(\"contrast\", \"estimate\", \"p.value\", \"tukey\", \"holm\", \"cost_tukey\", \"cost_holm\")]\ncolnames(m1_pvals) &lt;- c(\"Contrast\", \"Estimate\", \"Unadjusted P\", \"Tukey P\", \"Holm P\", \"Tukey/Unadjusted\", \"Holm/Unadjusted\")\n\n# only the simple effects are of interest. These are rows 1, 2, 5, 6\nm1_pvals[c(2,5), ] |&gt;\n  kable(digits = c(1, 3, 4, 4, 4, 1, 1)) |&gt;\n  kable_styling(full_width = FALSE)\n\n\n\nTable 12.2: Unadjusted p-value and Tukey adjusted p-value for the experiment in Fig. 5l\n\n\n\n\n\n\nContrast\nEstimate\nUnadjusted P\nTukey P\nHolm P\nTukey/Unadjusted\nHolm/Unadjusted\n\n\n\n\nWT Puerarin - WT Vehicle\n-0.554\n0.0032\n0.0159\n0.0096\n5.0\n3\n\n\nKO Puerarin - KO Vehicle\n-0.020\n0.9085\n0.9994\n0.9085\n1.1\n1\n\n\n\n\n\n\n\n\n\n\n\n12.3.1 Or, maybe we do want to adjust, but not in the way that is usually done\nFor each of the contrasts in the Fig. 5l example above, the researchers measured the response in four microvilli-related genes. Each of these genes answers the question posed for a contrast. For example, all four genes answer the question for contrast 1, which is “how does microvilli-related gene expression change when we add puerarin to mice with functional Gabra1?” The family is the four contrasts (WT Puerarin - WT Vehicle) for each of the four genes. The researchers should adjust for the four tests within this family. The next section shows how this is done.\n\n\n\nTable 12.3",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Multiple tests -- why and when to adjust *p*-values</span>"
    ]
  },
  {
    "objectID": "chapters/multiple-testing.html#sec-mult-tests-eg3-fdr",
    "href": "chapters/multiple-testing.html#sec-mult-tests-eg3-fdr",
    "title": "12  Multiple tests – why and when to adjust p-values",
    "section": "12.4 Example 3 – When we do want to adjust, we often want to use a method that controls the False Discovery Rate (FDR) (lipocalin fig3 data)",
    "text": "12.4 Example 3 – When we do want to adjust, we often want to use a method that controls the False Discovery Rate (FDR) (lipocalin fig3 data)\n\n\n\n\n\n\nWait What?\n\n\n\nWhen \\(p &lt; 0.05\\), researchers act as if they have discovered or confirmed something, so \\(p &lt; 0.05\\) is evidence of a discovery. Consider these two situations\n\n\\(p &lt; 0.05\\) and the Null Hypothesis is false – there is a true effect of treatment, or, there is a true difference in means between the two treatment levels. This is a true discovery.\n\\(p &lt; 0.05\\) but the Null Hypothesis is true – there is no true effect of treatment, or, there is no true difference in means between the two treatment levels. This is a false discovery.\n\nThe false discovery rate (FDR) in a batch of tests is\n\\[\nFDR = \\frac{number\\;of\\;false\\;discoveries}{number\\;of\\;false\\;discoveries + number\\;of\\;true\\;discoveries}\n\\]\nRemember that the family wise error rate (FWER) is the long term probability of at least one \\(p &lt; 0.05\\) in a batch of tests in which all Null Hypothesis are true – there are no true effects (Section 12.1).\nNote that the FWER for a single batch of tests is either 1 (there was at least one false positive) or 0 (there were no false positives). By contrast, an omniscient researcher can compute the FDR for a single batch of tests.\nFor any batch of tests in which some effects are real, and some are not\n\nA FWER method of p-value adjustment controls the long-term FWER at \\(\\leq\\) 0.05.\nAn FDR method of p-value adjustment controls the long-term FDR at \\(\\leq\\) 0.05.\n\n\n\nSource: Su, H., Guo, H., Qiu, X. et al. Lipocalin 2 regulates mitochondrial phospholipidome remodeling, dynamics, and function in brown adipose tissue in male mice. Nat Commun 14, 6729 (2023). https://doi.org/10.1038/s41467-023-42473-2\nData source\nFigure: Figure 3\nThe import code is in Section 12.7.3 below\nThe researchers are investigating the role of Lipocalin-2 (LCN2) on mitochondrial membrane function. Using a LCN2 knockout and the thermogenic molecule CL316,243, the researchers scanned for differences in levels of 246 lipids between WT and KO mice with and without CL316,243 treatment. Of the different classes of lipids, the biggest differences were found in cardiolipins. The panels in Fig 3d-u show cardiolipin levels for each of the four groups for 18 of the 26 cardiolipins with at least one significant p-value for a difference between WT and KO mice.\nThe two factors are\n\ngenotype, with levels “WT” and “KO”, where KO is the LCN2 knockout.\ntreatment, with levels “Saline” and “CL316243”, where CL316243 is administration of CL316,243.\n\nAll levels of both factors are combined to create the four treatment combinations\n\n“WT Saline” is the combination “WT” + “Saline”. This is the negative control.\n“WT CL316243” is the combination “WT” + “CL316,243”.\n“KO Saline” is the combination “KO” + “Saline”.\n“KO CL316243” is the combination “KO” + “CL316,243”\n\nThe researchers do not have a model of how any specific cardiolipin will respond to any specific treatment combination. Instead, the research question seems to be something like, “do we see differences in CL synthesis under different combinations of LCN2 activity and CL316,243?” For each cardiolipin, the four simple effect contrasts answer this question. But the question remains the same for each cardiolipin, so there are 104 (4 x 26) tests in the family. Even though the researchers only present results for 18 cardiolipins, we have to include the 8 non-presented cardiolipins because these were also analyzed.\nNow this is an example of an experiment where we should adjust p-values for multiple testing!\n\n\n\n\n\n\nWhich adjustment method?\n\n\n\n\nHolm. This modification of Bonferroni adjusts p-values to control the family-wise error rate (FWER) and would be relevant if the researchers were asking the question “does at least one cardiolipin have different synthesis level under any combination of genotype and treatment?”\nBH (Benjamini-Hochberg). This method based on Benjamini-Hochberg adjusts p-values to control the false discovery rate (FDR) and would be relevant if the researchers were asking “which cardiolipins have different synthesis levels and under which combination of genotype and treatment?”\n\nWe control FWER if we want to show that “we have evidence of something happening in this system”. We control FDR if we want to show that “we have evidence that we’ve discovered a specific component in the system”.\nI don’t think the FWER question is very relevant to most experimental bench biology.\nIn a sense, BH is more liberal and Holm is more conservative but this isn’t a good way to think about the these because this is comparing apples and oranges. Instead of using BH to “gain power” or Holm to “be conservative”, just use a different alpha. If the cost of false discovery is cheap but the potential for reward is high, increase alpha. If the cost of false discovery is expensive, decrease alpha.\n\n\n\ncl_cols &lt;- names(fig3)[which(substr(names(fig3), 1, 11) == \"cardiolipin\")]\nn_cardiolipins &lt;- length(cl_cols)\n\np_table &lt;- data.table(NULL)\nfor(i in 1:n_cardiolipins){\n  lipid_id &lt;- cl_cols[i]\n  formula_i &lt;- paste(lipid_id, \"~\", \"group\") |&gt;\n    formula()\n  m1 &lt;- lm(formula_i, data = fig3)\n  m1_pairs &lt;- emmeans(m1, specs = \"group\") |&gt;\n    contrast(method = \"revpairwise\",\n             adjust = \"none\") |&gt;\n    summary() |&gt;\n    data.table()\n  m1_pairs &lt;- m1_pairs[c(1,6,2,5),]\n  p1 &lt;- t.test(fig3[group == \"WT-Saline\", get(lipid_id)],\n               fig3[group == \"WT-CL\", get(lipid_id)],\n               var.equal = FALSE)$p.value\n  p2 &lt;- t.test(fig3[group == \"KO-Saline\", get(lipid_id)],\n               fig3[group == \"KO-CL\", get(lipid_id)],\n               var.equal = TRUE)$p.value\n  p3 &lt;- t.test(fig3[group == \"WT-Saline\", get(lipid_id)],\n               fig3[group == \"KO-Saline\", get(lipid_id)],\n               var.equal = TRUE)$p.value\n  p4 &lt;- t.test(fig3[group == \"WT-CL\", get(lipid_id)],\n               fig3[group == \"KO-CL\", get(lipid_id)],\n               var.equal = TRUE)$p.value\n  p_table &lt;- rbind(\n    p_table,\n    data.table(\n      \"lipid_id\" = lipid_id,\n      m1_pairs[, .SD, .SDcols = c(\"contrast\", \"p.value\")],\n      ttest = c(p1, p2, p3, p4)\n    )\n  )\n}\n\n# adjust over all contrasts\npool_contrast &lt;- TRUE\nif(pool_contrast == TRUE){\n  # add 8 more cardiolipins not included in the plot\n  # us lm pvalue instead of t test pvalue because t is less efficient\n  p_set &lt;- c(p_table[, p.value], rep(0.99, 8 * 4)) # 8 cardiolipins * 4 contrasts\n  p_holm &lt;- p.adjust(p_set, \"holm\")[1:(18 * 4)]\n  p_bh &lt;- p.adjust(p_set, \"BH\")[1:(18 * 4)]\n  p_by &lt;- p.adjust(p_set, \"BY\")[1:(18 * 4)]\n  \n  p_table[, holm := p_holm]\n  p_table[, fdr_bh := p_bh]\n  p_table[, fdr_by := p_by]\n}\n\n# adjust within each contrast, not over all contrasts\nby_contrast &lt;- FALSE\nif(by_contrast == TRUE){\n  contrast_list &lt;- unique(p_table$contrast)\n  for(contrast_i in 1:length(contrast_list)){\n    p_set &lt;- p_table[contrast == contrast_list[contrast_i], ttest]\n    p_set &lt;- c(p_set, rep(0.99, 8))\n    p_holm &lt;- p.adjust(p_set, \"holm\")[1:18]\n    p_bh &lt;- p.adjust(p_set, \"BH\")[1:18]\n    p_by &lt;- p.adjust(p_set, \"BY\")[1:18]\n    p_table[contrast == contrast_list[contrast_i],\n            holm := p_holm]\n    p_table[contrast == contrast_list[contrast_i],\n            fdr_bh := p_bh]\n    p_table[contrast == contrast_list[contrast_i],\n            fdr_by := p_by]\n  }\n}\n\n\n# p_table_order &lt;- doBy::orderBy(~ contrast + lipid_id, p_table)\n# p_table_order[, .SD, .SDcols = setdiff(names(p_table), \"contrast\")] |&gt;\n#   setnames(\"lipid_id\", \"\") |&gt;\n#   kable(digits = 3) |&gt;\n#   kable_styling(full_width = FALSE) |&gt;\n#   pack_rows(contrast_list[1], 1, 18) |&gt;\n#   pack_rows(contrast_list[2], 19, 36) |&gt;\n#   pack_rows(contrast_list[3], 37, 54)\n\n# researcher results\n# set1 d-h\n#WT: cl lower\n#KO: cl not lower\n# set2 i-m\n#WT cl not lower\n#KO: cl lower\n# set3 n-u\n#WT cl higher\n#KO: cl higher in 3/8\n\ndata.table(\n  \"Method\" = c(\"lm (unadjusted)\", \"t test (unadjusted)\", \"BH (FDR)\", \"BY (FDR)\", \"Holm (FWER)\"),\n  \"N (p&lt;0.05)\" = c(sum(p_table[, p.value] &lt; 0.05),\n             sum(p_table[, ttest] &lt; 0.05),\n             sum(p_table[, fdr_bh] &lt; 0.05),\n             sum(p_table[, fdr_by] &lt; 0.05),\n             sum(p_table[, holm] &lt; 0.05)\n  )\n) |&gt;\n  kable() |&gt;\n  kable_styling(full_width = FALSE)\n\n\n\nTable 12.4: Number of discoveries (p &lt; 0.05) among the 104 tests\n\n\n\n\n\n\nMethod\nN (p&lt;0.05)\n\n\n\n\nlm (unadjusted)\n28\n\n\nt test (unadjusted)\n26\n\n\nBH (FDR)\n16\n\n\nBY (FDR)\n9\n\n\nHolm (FWER)\n7\n\n\n\n\n\n\n\n\n\n\nNotes on Table 12.4\n\nFor these data, there are more discoveries (p &lt; 0.05) using the linear model with all four groups than using the pairwise t-tests used by the researchers. The linear model is slightly more powerful than pairwise t-tests because of a smaller SE, but differences in discovery rate between the two methods in any one batch will depend more on patterns of sample variances.\nThe BH adjustment reduced the number of discoveries by nearly half. The BY adjustment reduced the number of discoveries by nearly 2/3. BY is an FDR method based on Benjamini & Yekutieli that is modified for non-independent tests. The tests here are not independent because the cardiolipin levels are correlated. Consequently, BY will have better control of FDR than BH.\nThe Holm adjustment reduced the number of discoveries by three-fourths. I include it here simply for comparison. As argued above, Holm, or any FWER, doesn’t answer the right question.\nThe researchers didn’t adjust. Should they have? If the goal is discovery on how combinations of LCN2 and CL316,243 effect cardiolipin synthesis, and they don’t have working models of the system, then yes, they should adjust.\nOne way that R, or any programming language, is more efficient than menu-driven statistical software is the ability to write scripts to combine analyses, which is what we need to do to adjust p-values from multiple fit statistical models.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Multiple tests -- why and when to adjust *p*-values</span>"
    ]
  },
  {
    "objectID": "chapters/multiple-testing.html#better-know-fdr-sec-mult-tests-fdr",
    "href": "chapters/multiple-testing.html#better-know-fdr-sec-mult-tests-fdr",
    "title": "12  Multiple tests – why and when to adjust p-values",
    "section": "12.5 Better Know FDR {sec-mult-tests-fdr}",
    "text": "12.5 Better Know FDR {sec-mult-tests-fdr}\nUse this simulation to explore FDR, FWER, and Type I error. Here, I use it to explore how the BH adjustment behaves in a batch of 100 tests in which only 20% have true effects, and a batch in which 80% have true effects. I use BH because the tests are independent in this simulation.\n\nset.seed(1)\nfileout_path &lt;- here(\"output\", \"chap_mult_tests_fdr_sim.Rds\")\ndo_it &lt;- FALSE\n\nn_iter &lt;- 10000\nn_tests &lt;- 100\nalpha &lt;- 0.05\n\nif(do_it){\n  out_table &lt;- data.table(NULL)\n  for(true_fraction in c(.2, 0.8)){\n    # true_fraction &lt;- .2 # fraction of tests with true effect\n    false_fraction &lt;- 1 - true_fraction # fraction of tests with no effect\n    n_true &lt;- true_fraction * n_tests # number of tests with true effect\n    n_false &lt;- false_fraction * n_tests # number of tests with no effect\n    power_exp &lt;- .8\n    delta &lt;- power.t.test(n = 10, sd = 1, power = power_exp, sig.level = alpha)$delta\n    p_true &lt;- numeric(n_true)\n    p_false &lt;- numeric(n_false)\n    \n    true_discoveries_none &lt;- 0.0\n    false_nondiscoveries_none &lt;- 0.0\n    false_discoveries_none &lt;- 0.0\n    true_nondiscoveries_none &lt;- 0.0\n    \n    true_discoveries_holm &lt;- 0.0\n    false_nondiscoveries_holm &lt;- 0.0\n    false_discoveries_holm &lt;- 0.0\n    true_nondiscoveries_holm &lt;- 0.0\n    \n    true_discoveries_fdr &lt;- 0.0\n    false_nondiscoveries_fdr &lt;- 0.0\n    false_discoveries_fdr &lt;- 0.0\n    true_nondiscoveries_fdr &lt;- 0.0\n    \n    FWE_none &lt;- 0.0\n    FWE_holm &lt;- 0.0\n    FWE_fdr &lt;- 0.0\n    type_1 &lt;- 0.0\n    power &lt;- 0.0\n    \n    for(iter in 1:n_iter){\n      for(j in 1:n_true){\n        p_true[j] &lt;- t.test(rnorm(n = 10), rnorm(n = 10, mean = delta), var.equal = TRUE)$p.value\n      }\n      for(j in 1:n_false){\n        p_false[j] &lt;- t.test(rnorm(n = 10), rnorm(n = 10), var.equal = TRUE)$p.value\n      }\n      type_1 &lt;- type_1 + sum(p_false &lt; 0.05)/n_false\n      power &lt;- power + sum(p_true &lt; 0.05)/n_true\n      \n      p_none &lt;- c(p_true, p_false)\n      p_holm &lt;- p.adjust(p_none, method = \"holm\")\n      p_fdr &lt;- p.adjust(p_none, method = \"BH\")\n      p_true_holm &lt;- p_holm[1:n_true]\n      p_true_fdr &lt;- p_fdr[1:n_true]\n      p_false_holm &lt;- p_holm[(n_true + 1):n_tests]\n      p_false_fdr &lt;- p_fdr[(n_true + 1):n_tests]\n      \n      if(sum(p_false &lt; 0.05) &gt; 0){FWE_none &lt;- FWE_none + 1}\n      true_discoveries_none &lt;- true_discoveries_none + sum(p_true &lt; 0.05)\n      false_nondiscoveries_none &lt;- false_nondiscoveries_none + sum(p_true &gt; 0.05)\n      false_discoveries_none &lt;- false_discoveries_none + sum(p_false &lt; 0.05)\n      true_nondiscoveries_none &lt;- true_nondiscoveries_none + sum(p_false &gt; 0.05)\n      \n      if(sum(p_false_holm &lt; 0.05) &gt; 0){FWE_holm &lt;- FWE_holm + 1}\n      true_discoveries_holm &lt;- true_discoveries_holm + sum(p_true_holm &lt; 0.05)\n      false_nondiscoveries_holm &lt;- false_nondiscoveries_holm + sum(p_true_holm &gt; 0.05)\n      false_discoveries_holm &lt;- false_discoveries_holm + sum(p_false_holm &lt; 0.05)\n      true_nondiscoveries_holm &lt;- true_nondiscoveries_holm + sum(p_false_holm &gt; 0.05)\n      \n      if(sum(p_false_fdr &lt; 0.05) &gt; 0){FWE_fdr &lt;- FWE_fdr + 1}\n      true_discoveries_fdr &lt;- true_discoveries_fdr + sum(p_true_fdr &lt; 0.05)\n      false_nondiscoveries_fdr &lt;- false_nondiscoveries_fdr + sum(p_true_fdr &gt; 0.05)\n      false_discoveries_fdr &lt;- false_discoveries_fdr + sum(p_false_fdr &lt; 0.05)\n      true_nondiscoveries_fdr &lt;- true_nondiscoveries_fdr + sum(p_false_fdr &gt; 0.05)\n    }\n    \n    total_discoveries_none &lt;- true_discoveries_none + false_discoveries_none\n    total_non_discoveries_none &lt;- false_nondiscoveries_none + true_nondiscoveries_none\n    TDR_none &lt;- true_discoveries_none/total_discoveries_none\n    FDR_none &lt;- false_discoveries_none/total_discoveries_none\n    FNDR_none &lt;- false_nondiscoveries_none/total_non_discoveries_none\n    type_1_none &lt;- false_discoveries_none/n_false/n_iter\n    power_none &lt;- true_discoveries_none/n_true/n_iter\n    FWER_none &lt;- FWE_none/n_iter\n    \n    total_discoveries_holm &lt;- true_discoveries_holm + false_discoveries_holm\n    total_non_discoveries_holm &lt;- false_nondiscoveries_holm + true_nondiscoveries_holm\n    TDR_holm &lt;- true_discoveries_holm/total_discoveries_holm\n    FDR_holm &lt;- false_discoveries_holm/total_discoveries_holm\n    FNDR_holm &lt;- false_nondiscoveries_holm/total_non_discoveries_holm\n    type_1_holm &lt;- false_discoveries_holm/n_false/n_iter\n    power_holm &lt;- true_discoveries_holm/n_true/n_iter\n    FWER_holm &lt;- FWE_holm/n_iter\n    \n    total_discoveries_fdr &lt;- true_discoveries_fdr + false_discoveries_fdr\n    total_non_discoveries_fdr &lt;- false_nondiscoveries_fdr + true_nondiscoveries_fdr\n    TDR_fdr &lt;- true_discoveries_fdr/total_discoveries_fdr\n    FDR_fdr &lt;- false_discoveries_fdr/total_discoveries_fdr\n    FNDR_fdr &lt;- false_nondiscoveries_fdr/total_non_discoveries_fdr\n    type_1_fdr &lt;- false_discoveries_fdr/n_false/n_iter\n    power_fdr &lt;- true_discoveries_fdr/n_true/n_iter\n    FWER_fdr &lt;- FWE_fdr/n_iter\n    \n    out_table &lt;- rbind(\n      out_table,\n      data.table(\n        true_frac = true_fraction,\n        Stat = c(\"TD\", \"FD\", \"TDR\", \"FDR\", \"FNDR\", \"Type1\", \"power\", \"FWER\"),\n        None = c(true_discoveries_none, false_discoveries_none, TDR_none,\n                 FDR_none, FNDR_none, type_1_none, power_none, FWER_none),\n        Holm = c(true_discoveries_holm, false_discoveries_holm, TDR_holm,\n                 FDR_holm, FNDR_holm, type_1_holm, power_holm, FWER_holm),\n        FDR = c(true_discoveries_fdr, false_discoveries_fdr, TDR_fdr,\n                FDR_fdr, FNDR_fdr, type_1_fdr, power_fdr, FWER_fdr)\n      )\n    )\n    saveRDS(out_table, fileout_path)\n  } \n}else{\n  out_table &lt;- readRDS(fileout_path)\n}\n\n  out_table_keep &lt;- out_table[Stat %in% c(\"FDR\", \"FNDR\", \"Type1\", \"power\", \"FWER\"), ]\n\n  row_digits &lt;- function(df, d){\n    # df is a data.frame or data.table\n    # d is a single value for all rows or vector of digits for each row\n    \n    inc &lt;- which(unlist(lapply(df, is.numeric)))\n    x &lt;- round(Filter(is.numeric, df), d)|&gt;\n      apply(2, as.character) |&gt;\n      data.table()\n    df_out &lt;- apply(df, 2, as.character) |&gt;\n      data.table()\n    df_out[, inc] &lt;- x\n    return(df_out)\n  }\n  \n    options(knitr.kable.NA = '')\n\n#  out_table_keep[, .SD, .SDcols = c(\"Stat\", \"None\", \"Holm\", \"FDR\")] |&gt;\n  out_table_keep[, true_frac := \" \"]\n  setnames(out_table_keep, \"true_frac\", \" \")\n  out_table_keep |&gt;\n    kable(digits = c(1, 1,3,3,3)) |&gt;\n    kable_styling(full_width = FALSE) |&gt;\n    pack_rows(\"True Fraction = 0.2\", 1, 5) |&gt;\n    pack_rows(\"True Fraction = 0.8\", 6, 10)\n\n\n\nTable 12.5: Performance statistics for no adjustment, BH adjustment, and Holm adjustment for a batch of 100 independent tests with two different fractions of true effects. FNDR is the False Non-Discovery Rate, the fraction of false non-discoveries (p &gt; 0.05 but the effect is true) to the total number of non-discoveries (p &gt; 0.05). We want this fraction to be low.\n\n\n\n\n\n\n\nStat\nNone\nHolm\nFDR\n\n\n\n\nTrue Fraction = 0.2\n\n\n\nFDR\n0.200\n0.012\n0.047\n\n\n\nFNDR\n0.050\n0.173\n0.126\n\n\n\nType1\n0.050\n0.001\n0.005\n\n\n\npower\n0.801\n0.164\n0.424\n\n\n\nFWER\n0.985\n0.040\n0.324\n\n\nTrue Fraction = 0.8\n\n\n\nFDR\n0.015\n0.001\n0.010\n\n\n\nFNDR\n0.457\n0.768\n0.536\n\n\n\nType1\n0.050\n0.001\n0.030\n\n\n\npower\n0.801\n0.174\n0.720\n\n\n\nFWER\n0.641\n0.012\n0.449\n\n\n\n\n\n\n\n\n\n\nNotes on Table 12.5\n\nNote that it’s not the Type I error rate that increases with multiple tests, it’s the FWER that increases. It’s important to understand this difference. If every test answers a different question, all we care about is the Type I error rate.\nIf a batch of tests contains both true effects and null (zero) effects, the fraction of tests with true effects doesn’t effect the Type I error rate because the denominator is not the number of tests but the number of tests with no true effect.\nIf a batch of tests contains both true effects and null effects, the fraction of tests with true effects does effect the FDR because there are more false discoveries relative to true discoveries as the fraction of tests with true effects goes to zero. Remember, the denominator is the sum of false and true discoveries. If there are no true effects, 100% of the discoveries are false! If there are only true effects, 0% of the discoveries are false!",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Multiple tests -- why and when to adjust *p*-values</span>"
    ]
  },
  {
    "objectID": "chapters/multiple-testing.html#working-in-r",
    "href": "chapters/multiple-testing.html#working-in-r",
    "title": "12  Multiple tests – why and when to adjust p-values",
    "section": "12.6 Working in R",
    "text": "12.6 Working in R\n\n12.6.1 The contrast() function has different default adjustment methods for each of the contrast methods\n\nTukey HSD is the default for all pairwise contrasts using either method = \"pairwise\" or method = \"revpairwise\"\n\n\n# use the exfig2 data to explore this\nm1 &lt;- lm(nefa ~ treatment, data = exfig2)\nm1_emm &lt;- emmeans(m1, specs = \"treatment\")\nm1_pairs &lt;- contrast(m1_emm,\n                     method = \"revpairwise\") |&gt;\n  summary(infer = TRUE)\nm1_pairs\n\n contrast estimate    SE df lower.CL upper.CL t.ratio p.value\n 3q - con    -1.09 0.407 26  -2.1011  -0.0793  -2.680  0.0327\n 4i - con     1.05 0.396 26   0.0642   2.0320   2.647  0.0351\n 4i - 3q      2.14 0.407 26   1.1274   3.1491   5.256  &lt;.0001\n\nConfidence level used: 0.95 \nConf-level adjustment: tukey method for comparing a family of 3 estimates \nP value adjustment: tukey method for comparing a family of 3 estimates \n\n\nNotes:\n\nThe adjustment method for both the confidence interval and the p-value is given in the contrast object.\nDunnett’s test is the default for comparing all treatments (non-reference groups) to the control (the reference group). Both method = \"trt.vs.ctrl\" and method = \"dunnett\" work.\n\n\n# use the exfig2 data to explore this\nm1 &lt;- lm(nefa ~ treatment, data = exfig2)\nm1_emm &lt;- emmeans(m1, specs = \"treatment\")\nm1_pairs &lt;- contrast(m1_emm,\n                     method = \"trt.vs.ctrl\") |&gt;\n  summary(infer = TRUE)\nm1_pairs\n\n contrast estimate    SE df lower.CL upper.CL t.ratio p.value\n 3q - con    -1.09 0.407 26   -2.047   -0.133  -2.680  0.0241\n 4i - con     1.05 0.396 26    0.117    1.979   2.647  0.0260\n\nConfidence level used: 0.95 \nConf-level adjustment: dunnettx method for 2 estimates \nP value adjustment: dunnettx method for 2 tests \n\n\n\n\n12.6.2 For non-default adjustment, use the adjust = argument\nFor no adjustment, use adjust = \"none\"\n\n# use the exfig2 data to explore this\nm1 &lt;- lm(nefa ~ treatment, data = exfig2)\nm1_emm &lt;- emmeans(m1, specs = \"treatment\")\nm1_pairs &lt;- contrast(m1_emm,\n                     method = \"revpairwise\",\n                     adjust = \"none\") |&gt;\n  summary(infer = TRUE)\nm1_pairs\n\n contrast estimate    SE df lower.CL upper.CL t.ratio p.value\n 3q - con    -1.09 0.407 26   -1.926   -0.254  -2.680  0.0126\n 4i - con     1.05 0.396 26    0.234    1.862   2.647  0.0136\n 4i - 3q      2.14 0.407 26    1.302    2.974   5.256  &lt;.0001\n\nConfidence level used: 0.95 \n\n\n\n\n12.6.3 Options in adjust =\n\n“none” – no adjustment.\n\nOptions for controlling FDR\n\n“BH” or “fdr” – controls the false discovery rate using the logic of Benjamini & Hochberg. The BH method assumes independent tests. This or BY should be used for most uses of p-value adjustment in experimental bench biology.\n“BY” – is a modification of BH using the logic of Benjamini & Yekutieli. The BY method relaxes the independent test assumption and consequently is more consertive than BH. This or BY should be used for most uses of p-value adjustment in experimental bench biology.\n\nGeneral options for controlling FWER\n\n“holm” – Holm-Bonferroni is the Holm modification of Bonferroni and is used to control the FWER. It is more powerful than Bonferroni while still holding FWER at alpha. For most uses of p-value adjustment in experimental bench biology, we are less interested in controling the FWER than the FDR.\n“bonferroni” – Bonferroni is the original, general-purpose adjustment for a set of p-values to control the FWER. It has less power than the Holm modification, so it’s use should be historic only.\n“mvt” – based on the multivariate t distribution and using covariance structure of the variables.\n\nOptions for controlling FWER that are specific to experimental designs\n\n“dunnettx” – Dunnett’s test is a method used when comparing all treatments to a single control.\n“tukey” – Tukey’s HSD method is a method used to compare all pairwise comparisons.\n\n\n\n12.6.4 Using the p.adjust() function\nWe use the base R p.adjust() function to adjust a set of p-values that were computed from different models, as in Example 3 above.\nBut to show a very simple example of using the p.adjust() function, let’s use the lipocalin Fig3 data. First get the p values for the four simple effects on the cardiolipin in panel d. Be sure to use the adjust = \"none argument because we want to adjust unadjusted p-values!\n\nm1 &lt;- lm(cardiolipin_d ~ genotype * treatment, data = fig3)\nm1_emm &lt;- emmeans(m1, specs = c(\"genotype\", \"treatment\"))\n# compute the four simple effect p-values\nm1_simple &lt;- contrast(m1_emm,\n                      method = \"revpairwise\",\n                      simple = \"each\",\n                      combine = TRUE,\n                      adjust = \"none\") |&gt;\n  summary(infer = TRUE) |&gt;\n  data.table()\n# extract the p-values\n\np_fig3d &lt;- m1_simple[, p.value]\np_fig3d\n\n[1] 0.034802754 0.497327407 0.003147072 0.668086400\n\n\nNow, use the p.adjust() function\n\np_fdr_BH_fig3d &lt;- p.adjust(p_fig3d, \"BH\")\np_fdr_BH_fig3d\n\n[1] 0.06960551 0.66310321 0.01258829 0.66808640\n\n\nNotes:\n\nAgain, we wouldn’t typically do this, we would simply use the “adjust =” argument within the contrast() function and not bother to extract the p-values and then adjust these, as done here.\nWhen we would want to extract the p-values and adjust is when we have multiple responses for the same experiment and these responses answer the same question, which is common in experimental bench biology and is the case for all three examples above.\n\n\n12.6.4.1 Options in p.adjust()\n\n“BH” or “fdr” – controls the false discovery rate using the logic of Benjamini & Hochberg. The BH method assumes independent tests. This or BY should be used for most uses of p-value adjustment in experimental bench biology.\n“BY” – is a modification of BH using the logic of Benjamini & Yekutieli. The BY method relaxes the independent test assumption and consequently is more consertive than BH. This or BY should be used for most uses of p-value adjustment in experimental bench biology.\n“holm” – Holm-Bonferroni is the Holm modification of Bonferroni and is used to control the FWER. It is more powerful than Bonferroni while still holding FWER at alpha. For most uses of p-value adjustment in experimental bench biology, we are less interested in controling the FWER than the FDR.\n“bonferroni” – Bonferroni is the original, general-purpose adjustment for a set of p-values to control the FWER. It has less power than the Holm modification, so it’s use should be historic only.\n\n\n\n\n12.6.5 A Holm-Sidak function for reproducing GraphPad Prism results\n\nholm_sidak &lt;- function(p){\n  # assume alpha = 0.05\n  n &lt;- length(p)\n  rank_p &lt;- frank(p)\n  inv_rank_p &lt;- n - rank_p + 1\n  \n  # put pvalues and the inv rankings in rank order\n  p_order &lt;- p[order(p)]\n  inv_rank_order &lt;- inv_rank_p[order(p)]\n  \n  # compute the adjusted p-value\n  p_holm_sidak_order &lt;- (1 - (1 - p_order)^(inv_rank_order))\n  \n  # adjusted p-values of higher rank cannot be less than adjusted p-value of lower rank\n  # so find and adjust these\n  for(j in 2:n){\n    if(p_holm_sidak_order[j] &lt; p_holm_sidak_order[j-1]){\n      p_holm_sidak_order[j] &lt;- p_holm_sidak_order[j-1]\n    }\n  }\n  \n  # any adjusted &gt; 1 make 1\n  p_holm_sidak_order[p_holm_sidak_order &gt; 1.0] &lt;- 1.0\n  \n  # put back into original order\n  p_holm_sidak &lt;- p_holm_sidak_order[rank_p]\n  return(p_holm_sidak)\n}\n\nNotes:\n\nThis function only computes the Holm-Sidak p-value and not the adjusted CIs. The emmeans package uses Bonferroni intervals for the Holm adjustment, and these should be used for the Holm-Sidak adjustment. The function RHSDB::rh.sd.sidak computes CIs in addition to the p-values but I haven’t sleuthed out what these are.\nThis function is used exactly like any other R function. Here I compare both Holm and Holm-Sidak for the Lipocalin Fig3 dataset. Just like the Sidak adjustment is slightly more powerful than Bonferroni, the Holm-Sidak is slightly more powerful than Holm. These tiny differences are rather moot, given the TL;DR at the start of this chapter (Chapter 12).\n\n\nm1 &lt;- lm(cardiolipin_d ~ genotype * treatment, data = fig3)\nm1_emm &lt;- emmeans(m1, specs = c(\"genotype\", \"treatment\"))\n# compute the four simple effect p-values\nm1_simple &lt;- contrast(m1_emm,\n                      method = \"revpairwise\",\n                      simple = \"each\",\n                      combine = TRUE,\n                      adjust = \"none\") |&gt;\n  summary(infer = TRUE) |&gt;\n  data.table()\n\n# compute holm\nm1_simple[, holm := p.adjust(p.value, \"holm\")]\n\n# compute holm-sidak\nm1_simple[, holm_sidak := holm_sidak(p.value)]\nm1_simple |&gt;\n  kable(digits = c(1,1,1,3,3,1,3,3,1,4,4,4)) |&gt;\n  kable_styling()\n\n\n\n\ntreatment\ngenotype\ncontrast\nestimate\nSE\ndf\nlower.CL\nupper.CL\nt.ratio\np.value\nholm\nholm_sidak\n\n\n\n\nSaline\n.\nKO - WT\n-0.023\n0.010\n14\n-0.043\n-0.002\n-2.3\n0.0348\n0.1044\n0.1008\n\n\nCL\n.\nKO - WT\n0.006\n0.009\n14\n-0.012\n0.024\n0.7\n0.4973\n0.9947\n0.7473\n\n\n.\nWT\nCL - Saline\n-0.033\n0.009\n14\n-0.052\n-0.013\n-3.6\n0.0031\n0.0126\n0.0125\n\n\n.\nKO\nCL - Saline\n-0.004\n0.009\n14\n-0.024\n0.016\n-0.4\n0.6681\n0.9947\n0.7473",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Multiple tests -- why and when to adjust *p*-values</span>"
    ]
  },
  {
    "objectID": "chapters/multiple-testing.html#hidden-import-code",
    "href": "chapters/multiple-testing.html#hidden-import-code",
    "title": "12  Multiple tests – why and when to adjust p-values",
    "section": "12.7 Hidden Import Code",
    "text": "12.7 Hidden Import Code\n\n12.7.1 Example 1 (exfig2 data)\n\ndata_from &lt;- \"A brain-to-gut signal controls intestinal fat absorption\"\nfile_name &lt;- \"41586_2024_7929_MOESM13_ESM.xlsx\"\nfile_path &lt;- here(data_folder, data_from, file_name)\nexfig2_import &lt;- function(phenotype = \"xxx\", range_in = \"xxx\"){\n  exfig2_part &lt;- read_excel(file_path,\n                            sheet = \"Sheet1\",\n                            range = range_in,\n                            col_names = TRUE) |&gt;\n    data.table() |&gt;\n    melt(variable.name = \"treatment\",\n         value.name = phenotype)\n  return(exfig2_part)\n}\n\nexfig2 &lt;- exfig2_import(phenotype = \"nefa\",\n                        range = \"A47:C57\")\nexfig2 &lt;- cbind(exfig2,\n                \"fecal_tg\" = exfig2_import(phenotype = \"fecal_tg\",\n                                           range = \"A64:C74\")$fecal_tg)\nexfig2 &lt;- cbind(exfig2,\n                \"jejunal_tg\" = exfig2_import(phenotype = \"jejunal_tg\",\n                                             range = \"A81:C91\")$jejunal_tg)\nexfig2[, treatment := factor(treatment,\n                             levels = unique(treatment))]\n\n\n\n12.7.2 Example 2 (fig5l data)\n\ndata_from &lt;- \"A brain-to-gut signal controls intestinal fat absorption\"\nfile_name &lt;- \"41586_2024_7929_MOESM11_ESM.xlsx\"\nfile_path &lt;- here(data_folder, data_from, file_name)\nfig5l &lt;- read_excel(file_path,\n                    sheet = \"Sheet1\",\n                    range = \"A133:AG137\",\n                    col_names = FALSE) |&gt;\n  data.table() |&gt;\n  transpose(make.names = 1)\ncolnames(fig5l)[1] &lt;- c(\"treatment_orig\")\nfig5l[, ezrin := as.numeric(ezrin)]\nfig5l[, cdc42 := as.numeric(cdc42)]\nfig5l[, eps8 := as.numeric(eps8)]\nfig5l[, vil1 := as.numeric(vil1)]\nfig5l[, treatment_orig := fill_down(treatment_orig)]\nfig5l[treatment_orig == \"flox-veh\", group := \"WT Vehicle\"]\nfig5l[treatment_orig == \"flox-Pue\", group := \"WT Puerarin\"]\nfig5l[treatment_orig == \"KO-veh\", group := \"KO Vehicle\"]\nfig5l[treatment_orig == \"KO-Pue\", group := \"KO Puerarin\"]\nfig5l[, group := factor(group, levels = unique(group))]\n\nfig5l[, c(\"genotype\", \"treatment\") := tstrsplit(group, \" \")]\nfig5l[, genotype := factor(genotype, levels = c(\"WT\", \"KO\"))]\nfig5l[, treatment := factor(treatment, levels = c(\"Vehicle\", \"Puerarin\"))]\n\n\n\n12.7.3 Example 3 (fig3 data)\n\ndata_from &lt;- \"Lipocalin 2 regulates mitochondrial phospholipidome remodeling, dynamics, and function in brown adipose tissue in male mice\"\n\nfile_name &lt;- \"41467_2023_42473_MOESM7_ESM.xlsx\"\nfile_path &lt;- here(data_folder, data_from, file_name)\n\n# note I had to insert rows in the excel file \n# in between some of the\n# sets because of inconsistency in the file\nfig3_import &lt;- function(range_in = \"xxx\", lipid_id = \"xxx\"){\n  fig3_wide &lt;- read_excel(file_path,\n                          sheet = \"Fig. 3\",\n                          range = range_in) |&gt;\n    data.table()\n  fig3_long &lt;- melt(fig3_wide,\n                    measure.vars = colnames(fig3_wide),\n                    variable.name = \"group\",\n                    value.name = paste0(\"cardiolipin_\", lipid_id))\n  fig3_long[, group := factor(group,\n                              levels = colnames(fig3_wide))]\n  fig3_long[, mouse_id := .I]\n  return(fig3_long)\n}\n\nfig3 &lt;- data.table(NULL)\nrow_1 &lt;- 12\nfor(fig_id in letters[4:21]){\n  row_2 &lt;- row_1 + 5\n  range_in &lt;- paste0(\n    \"B\",row_1,\":E\",row_2\n  )\n  if(fig_id == letters[4]){\n    fig3 &lt;- fig3_import(range_in, fig_id)\n  }else{\n    fig3_part &lt;- fig3_import(range_in, fig_id)\n    fig3 &lt;- merge(fig3, fig3_part, by = c(\"mouse_id\", \"group\"))\n  }\n  # fig3 &lt;- rbind(\n  #   fig3,\n  #   data.table(\n  #     figure = fig_id,\n  #     fig3_import(range_in))\n  # )\n  row_1 &lt;- row_2 + 4\n}\n\n# row 5 and 15 have all missing data\nfig3 &lt;- fig3[-c(5,15), ]\n\nfig3[, c(\"genotype\", \"treatment\") := tstrsplit(group, \"-\")]\ny_cols &lt;- which(substr(names(fig3), 1, 11) == \"cardiolipin\")\nfig3 &lt;- fig3[, .SD, .SDcols = c(\"mouse_id\", \"group\", \"genotype\", \"treatment\", names(fig3)[y_cols])]\n\nfig3[, group := factor(group, levels = unique(group))]\nfig3[, genotype := factor(genotype, levels = c(\"WT\", \"KO\"))]\nfig3[, treatment := factor(treatment, levels = c(\"Saline\", \"CL\"))]",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Multiple tests -- why and when to adjust *p*-values</span>"
    ]
  },
  {
    "objectID": "chapters/two-factors.html",
    "href": "chapters/two-factors.html",
    "title": "13  Linear models with two categorical \\(X\\) – Factorial linear models (“two-way ANOVA”)",
    "section": "",
    "text": "13.1 A linear model with crossed factors estimates interaction effects\nA factorial experiment is one in which there are two or more factor variables (categorical \\(X\\)) that are crossed, resulting in a group for each combination of the levels of each factor. Each specific combination is a different treatment. A linear model with crossed factors is used to estimate interaction effects, which occur when the effect of the level of one factor is conditional on the level of the other factors. Estimation of the interaction effect is necessary for inferences about\nInferences like these are common in the experimental biology literature but they are made using the wrong statistics. The correct statistic – the interaction effect – is easy to compute but rarely computed.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Linear models with two categorical $X$ -- Factorial linear models (\"two-way ANOVA\")</span>"
    ]
  },
  {
    "objectID": "chapters/two-factors.html#a-linear-model-with-crossed-factors-estimates-interaction-effects",
    "href": "chapters/two-factors.html#a-linear-model-with-crossed-factors-estimates-interaction-effects",
    "title": "13  Linear models with two categorical \\(X\\) – Factorial linear models (“two-way ANOVA”)",
    "section": "",
    "text": "“Something different” – Estimation of a treatment effect relative to a control effect (Example 1 – TLR9-/- mice)\n“It depends” – Estimation of the effect of background condition on an effect (Example 2 – XX mice)\n“More than the sum of the parts” – Estimation of synergy, a non-additive effect (Example 3 – plant root growth)\n\n\n\n\n\n\n\n\nNHST Blues\n\n\n\nNHST encourages dichotomous thinking such as “there is or isn’t an interaction effect.” Dichotomous thinking can only lead to disasters of biblical proportion, dogs and cats living together. Interaction effects in biology are ubiquitous. Sometimes they are small enough to ignore.\n\n\n\n13.1.1 An interaction is a difference in simple effects\nIn this chapter, I’ll describe an interaction effect using different descriptions (Examples 1-3). Here, I want to emphasize that an interaction effect is a difference of differences. To clarify this, I’ll introduce a fake experiment. A research group has evidence that a certain gene product (CGP) is an intermediary between intestinal microbiota and obesity. The researchers transfer feces from either lean mice or obese mice into either wildtype mice or CGP-/- mice to investigate the effect of microbiota and CGP on body weight. The design has two factors (donor and genotype), each with two levels (donor: “Lean”, “Obese; genotype:”WT”, “KO”), which makes this a \\(2 \\times 2\\) (crossed or factorial) design. There are \\(n=6\\) replicates for each treatment.\nA good way to visualize the treatment combinations in a crossed design is with a \\(m \\times p\\) table showing all combinations of the \\(m\\) levels of factor 1 (donor) against the \\(p\\) levels of factor 2 (genotype) (Table 13.1).\n\n\n\n\nTable 13.1: Mean body weight of mice in four treatment groups of the 2 x 2 factorial experiment.\n\n\n\n\n\n\n\nWT\nKO\nEffect\n\n\n\n\nLean\n33.2\n34.1\n0.9\n\n\nObese\n41.9\n32.7\n-9.2\n\n\nEffect\n8.7\n-1.4\n-10.1\n\n\n\n\n\n\n\n\n\n\n\nThe upper-left \\(2 \\times 2\\) part of the table contains the fake mean body weight of each treatment group. These means are known as cell means.\nThe first two elements in the “Effect” column contains the difference of the two cells to the left – these are the effects of genotype conditional on the level of donor.\nThe first two elements in the “Effect” row contains the difference of the two cells above – these are the effects of donor conditional on the level of genotype.\nThe effects described in items 2 and 3 are known as the simple effects.\nThe value in red is the difference of the two simple effects above it. It is also the difference of the two simple effects to the left. These differences are equal. This is the interaction effect.\nIn an experiment with a single factor with four levels, all six pairwise comparisons may be of interest. In a \\(2 \\times 2\\) factorial experiment, it is the four simple effects and the interaction effect that should pique the interest of the researcher.\n\nIn this fake experiment, we want to know the effect of obese donor treatment in the KO mice compared to the effect of obese donor treatment in the WT mice. That is, we want the contrast of these two simple effects.\n\\[\n(\\operatorname{Obese KO} - \\operatorname{Lean KO}) - (\\operatorname{Obese WT} - \\operatorname{Lean WT})\n\\]\nThis contrast is the interaction effect. An interaction effect is a difference of differences.\n\n\n13.1.2 A linear model with crossed factors includes interaction effects\nThe factorial linear model for the fake data is\n\\[\n\\begin{align}\n\\texttt{body\\_weight} &= \\beta_0\\ + \\\\\n&\\quad \\ \\beta_1 (\\texttt{donor}_{\\texttt{Obese}})\\ + \\\\\n&\\quad \\ \\beta_2 (\\texttt{genotype}_{\\texttt{KO}})\\ + \\\\\n&\\quad \\ \\beta_3 (\\texttt{donor}_{\\texttt{Obese}} : \\texttt{genotype}_{\\texttt{KO}})\n\\end{align}\n\\]\nWhat are the variables?\n\n\\(\\texttt{donor}_{\\texttt{Obese}}\\) is the indicator variable for the “Obese” level of the factor \\(\\texttt{donor}\\). It contains the value 1 if the donor was “Obese” and 0 otherwise.\n\\(\\texttt{genotype}_{\\texttt{KO}}\\) is the indicator variable for the “KO” level of the factor \\(\\texttt{genotype}\\). It contains the value 1 if the genotype is “KO” and 0 otherwise.\n\\(\\texttt{donor}_{\\texttt{Obese}}\\) : _{is the indicator variable for the interaction between the \"Obese\" level of $\\texttt{donor and the “KO” level of \\(\\texttt{genotype}\\). This “:” character to indicate interaction follows the R formula convention in the LME4 package. Many sources use a \\(\\times\\) symbol instead of the colon. The variable contains the value 1 if the mouse is assigned to both “Obese” and “KO” and 0 otherwise. This value is the product of the values in \\(\\texttt{donor}_{\\texttt{Obese}}\\) and \\(\\texttt{genotype}_{\\texttt{KO}}\\).\n\nWhat are the parameters?\nThis linear model has set “Lean” as the reference level of donor and “WT” as the reference level of genotype. This make the “Lean WT” mice the control.\n\n\\(\\beta_0\\) is the true mean of the control, which is the “Lean WT” group.\n\\(\\beta_1\\) is the true effect of donor in the WT mice (the effect of manipulating the donor factor but not the genotype factor). It is the difference between the true means of the “Obese WT” group and the “Lean WT” group.\n\nThe mean of the “Obese WT” group is \\(\\beta_0 + \\beta_1\\). This is the expectation if we start with the control and then add the effect of Obese donor.\n\n\\(\\beta_2\\) is the true effect of genotype in the mice given feces from lean donors (the effect of manipulating the genotype factor but not the donor factor). It is the difference between the true means of the “Lean KO” group and the “Lean WT” group.\n\nThe mean of the “Lean KO” group is \\(\\beta_0 + \\beta_2\\). This is the expectation if we start with the control and then add the effect of KO genotype.\n\n\\(\\beta_3\\) is the true interaction effect of \\(\\texttt{donor}_{\\texttt{Obese}}\\) : _{}$\n\nThe mean of the “Obese KO” group is \\(\\beta_0 + \\beta_1 + \\beta_2 + \\beta_3\\). The expected mean of the “Obese KO” group, if the factors are additive, which means the interaction effect is zero, is \\(\\beta_0 + \\beta_1 + \\beta_2\\). The interaction effect is the difference between the actual mean of the “Obese KO” group and this additive mean. The interaction effect is what’s left-over, after you’ve added Obese effect and the KO effect to the control.\n\n\n13.1.3 factorial experiments are frequently analyzed as flattened linear models in the experimental biology literature\nOften, researchers analyze data from a factorial experiment with a one-way ANOVA followed by pairwise tests (or by a simple series of separate t-tests). For the fake experiment, this linear model is\n\\[\n\\begin{align}\n\\texttt{body\\_weight} &= \\beta_0 \\ + \\\\\n& \\quad \\ \\beta_1 (\\texttt{treatment}_{\\texttt{Obese}}) \\ + \\\\\n& \\quad \\ \\beta_2 (\\texttt{treatment}_{\\texttt{KO}}) \\ + \\\\\n& \\quad \\ \\beta_3 (\\texttt{treatment}_{\\texttt{Obese + KO}})\n\\end{align}\n\\]\nI refer to this as a flattened model (the table of treatment combinations has been flattened into a single row). Inference from a factorial or flattened model is the same. We can get the same pairwise contrasts or interaction contrasts from either model. That said, a factorial model nudges researchers to think about the analysis as a factorial design, which is good, because interaction effects are an explicit component of a factorial model.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Linear models with two categorical $X$ -- Factorial linear models (\"two-way ANOVA\")</span>"
    ]
  },
  {
    "objectID": "chapters/two-factors.html#example-1-estimation-of-a-treatment-effect-relative-to-a-control-effect-something-different-experiment-2j-glucose-uptake-data",
    "href": "chapters/two-factors.html#example-1-estimation-of-a-treatment-effect-relative-to-a-control-effect-something-different-experiment-2j-glucose-uptake-data",
    "title": "13  Linear models with two categorical \\(X\\) – Factorial linear models (“two-way ANOVA”)",
    "section": "13.2 Example 1 – Estimation of a treatment effect relative to a control effect (“Something different”) (Experiment 2j glucose uptake data)",
    "text": "13.2 Example 1 – Estimation of a treatment effect relative to a control effect (“Something different”) (Experiment 2j glucose uptake data)\nTo introduce a linear model with crossed factors (categorical \\(X\\) variables), I’ll use data from a set of experiments designed to measure the effect of the toll-like receptor protein TLR9 on the activation of excercise-induced AMP-activated protein kinase (AMPK) and downstream sequelae of this activation, including glucose transport (from outside to inside the cell) by skeletal muscle cells.\nArticle source: TLR9 and beclin 1 crosstalk regulates muscle AMPK activation in exercise\nPublic source\nThe data are from multiple experiments in Figure 2.\nData source\n\n13.2.1 Understand the experimental design\nBackground. Exercise (muscle activity) stimulates AMPK activated glucose uptake (transport from outside to inside the muscle cell). Instead of natural exercise, which can stimulate multiple systems, the researchers directly activate the muscle with electrical stimulation.\nResearch question There are two ways to think about the question – both produce the same answer.\n\nHow much does TLR9 knockout inhibit the expected increase of glucose uptake following electrical stimulation? The expected increase comes from the contrast of the positive and negative controls. Call this the knockout-induced stimulation effect.\nHow much does TLR9 knockout inhibit glucose uptake during muscle stimulation compared to effect of TLR9 knockout during muscle rest? Call this the stimulation-induced TLR9-/- effect.\n\nResponse variable glucose_uptake (nmol per mg protein per 15 min) – the rate of glucose transported into the cell.\nFactor 1 – genotype (“WT”, “KO”).\n\n“WT” (reference level) – C57BL/6J mice with intact TLR gene (TLR+/+)\n“KO” – TLR-/- mice on a C57BL/6J background\n\nFactor 2 stimulation (“Rest”, “Active”) – Two levels:\n\n“Rest” (reference level) – muscle that has not been stimulated.\n“Active” – electrical stimulation of muscle to induce contraction and contractile-related cell changes\n\nThese two factors create the three control treatments and the one focal treatment:\n\n“WT Rest” – Negative control. Expect non-exercise (low) level uptake.\n“WT Active” – Positive control. Expect high uptake.\n“KO Rest” – Method control. Unsure of KO effect on uptake in Rest, which is why we need this control.\n“KO Active” – Focal treatment. At about same level of KO Rest if TLR9-/- completely inhibits electrical stimulation of glucose uptake\n\nDesign – \\(2 \\times 2\\), that is, two crossed factors each with two levels. This results in four groups, each with a unique combination of the levels from each factor.\nPlanned Contrasts\nThe two ways of framing the research question suggest either of the following sets of contrasts. Both ways of framing generate a treatment contrast that includes the focal treatment and a control contrast that does not include the focal treatment. The question pursued by the experiment is addressed with the contrast of the treatment and control contrasts, which is is the estimate of the interaction effect. There is no difference in inference between the two ways of framing the question – the interaction effects are equivalent. The two framings simply give two different ways of viewing an interaction effect.\nFraming 1: How much does TLR9 knockout inhibit the expected increase of glucose uptake during stimulation?\n\n(KO Active - KO Rest) – effect of Stimulation in KO mice. This is our treatment contrast. If TLR9 is necessary for glucose uptake, then this should be zero. If positive, there are non-TLR9 paths.\n(WT Active - WT Rest) – This is the positive control contrast – it is what we know based on prior knowledge and what we want to compare the treatment effect to. This should be positive based on prior knowledge.\n\nWe need to control for the expected increase in 2 using the contrast:\n\n(KO Active - KO Rest) - (WT Active - WT Rest) – This contrast is the interaction effect. This focal contrast is what we need to estimate the knockout-induced stimulation effect.\n\nFraming 2: What is stimulation-induced TLR9-/- effect?\n\n(KO Active - WT Active) – effect of KO during muscle stimulation. This is our treatment contrast. If TLR9 is necessary for glucose uptake, then this should be big and negative if the KO effect at rest is small.\n(KO Rest - WT Rest) – effect of KO when the muscle is not stimulated. This is the methodological control contrast. If TLR9-/- KO has non-muscle-stimulation paths to glucose uptake, this will be something other than zero.\n\nWe need to control for any (KO Rest - WT Rest) effect using the contrast:\n\n(KO Active - WT Active) - (KO Rest - WT Rest) – This contrast is the interaction effect. This focal contrast is what we need to estimate the stimulation-induced TLR9-/- effect.\n\nOur planned contrasts are:\n\n(WT Active - WT Rest) – positive control contrast\n(KO Rest - WT Rest) – methodological control contrast\n(KO Active - WT Active) – treatment contrast\n(KO Active - WT Active) - (KO Rest - WT Rest) – interaction contrast\n\n\n\n13.2.2 Fit the linear model\n\nexp2j_m1 &lt;- lm(glucose_uptake ~ stimulation * genotype,\n               data = exp2j)\n\n\n\n13.2.3 Inference\nThe coefficient table\n\nexp2j_m1_coef &lt;- tidy(exp2j_m1, conf.int = TRUE)\n\nexp2j_m1_coef |&gt;\n  kable(digits = c(1,2,3,1,4,2,2)) |&gt;\n  kable_styling()\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\nconf.low\nconf.high\n\n\n\n\n(Intercept)\n6.75\n0.419\n16.1\n0.0000\n5.89\n7.61\n\n\nstimulationActive\n3.45\n0.592\n5.8\n0.0000\n2.23\n4.67\n\n\ngenotypeKO\n0.78\n0.639\n1.2\n0.2338\n-0.54\n2.10\n\n\nstimulationActive:genotypeKO\n-2.31\n0.885\n-2.6\n0.0152\n-4.13\n-0.48\n\n\n\n\n\n\n\nemmeans table\n\nexp2j_m1_emm &lt;- emmeans(exp2j_m1,\n                          specs = c(\"stimulation\", \"genotype\"))\nexp2j_m1_emm |&gt;\n  kable(digits = c(1,1,2,3,1,2,2)) |&gt;\n  kable_styling()\n\n\n\n\nstimulation\ngenotype\nemmean\nSE\ndf\nlower.CL\nupper.CL\n\n\n\n\nRest\nWT\n6.75\n0.419\n25\n5.89\n7.61\n\n\nActive\nWT\n10.20\n0.419\n25\n9.34\n11.06\n\n\nRest\nKO\n7.53\n0.483\n25\n6.53\n8.53\n\n\nActive\nKO\n8.67\n0.447\n25\n7.75\n9.59\n\n\n\n\n\n\n\nThe contrasts table\n\n# exp2j_m1_emm # print in console to get row numbers\n# set the mean as the row number from the emmeans table\nwt_rest &lt;- c(1,0,0,0)\nwt_active &lt;- c(0,1,0,0)\nko_rest &lt;- c(0,0,1,0)\nko_active &lt;- c(0,0,0,1)\n\n# contrasts are the difference in the vectors created above\n# these planned contrasts are described above\n# 1. (WT Active - WT Rest) -- positive control contrast\n# 2. (KO Rest - WT Rest) -- methodological control contrast\n# 3. (KO Active - WT Active) -- treatment contrast\n# 4. (KO Active - WT Active) - (KO Rest - WT Rest) -- interaction \n\nexp2j_m1_planned &lt;- contrast(\n  exp2j_m1_emm,\n  method = list(\n    \"(WT Active - WT Rest)\" = c(wt_active - wt_rest),\n    \"(KO Rest - WT Rest)\" = c(ko_rest - wt_rest),\n    \"(KO Active - WT Active)\" = c(ko_active - wt_active),\n    \"KO:Active Ixn\" = c(ko_active - wt_active) -\n      (ko_rest - wt_rest)\n  ),\n  adjust = \"none\"\n) |&gt;\n  summary(infer = TRUE)\n\n\nexp2j_m1_planned |&gt;\n  kable(digits = c(0,3,4,0,3,3,2,5)) |&gt;\n  kable_styling()\n\n\n\n\ncontrast\nestimate\nSE\ndf\nlower.CL\nupper.CL\nt.ratio\np.value\n\n\n\n\n(WT Active - WT Rest)\n3.448\n0.5919\n25\n2.229\n4.667\n5.83\n0.00000\n\n\n(KO Rest - WT Rest)\n0.780\n0.6393\n25\n-0.537\n2.097\n1.22\n0.23379\n\n\n(KO Active - WT Active)\n-1.527\n0.6127\n25\n-2.789\n-0.265\n-2.49\n0.01967\n\n\nKO:Active Ixn\n-2.307\n0.8855\n25\n-4.131\n-0.484\n-2.61\n0.01523\n\n\n\n\n\n\n# double check with automated contrasts\n# contrast(exp2b.2_m1_emm, method = c(\"revpairwise\"))\n\n\n\n13.2.4 Plot the model\n\nggplot_the_model(\n  exp2j_m1,\n  exp2j_m1_emm,\n  exp2j_m1_planned,\n  palette = pal_okabe_ito_blue,\n  legend_position = \"bottom\",\n  y_label = \"Glucose uptake\\n(nmol per mg protein\\nper 15 min)\",\n  effect_label = \"Difference in glucose uptake\",\n  rel_heights = c(0.44,1),\n)\n\n\n\n\n\n\n\n\n\n\n13.2.5 alternaPlot the model\n\nexp2j_m1_pairs &lt;- contrast(exp2j_m1_emm,\n                           method = \"revpairwise\",\n                           simple = \"each\",\n                           combine = TRUE,\n                           adjust = \"none\") |&gt;\n  summary(infer = TRUE)\nb &lt;- emm_b(exp2j_m1_emm)\ndodge_width &lt;- 0.4\ngg &lt;- ggplot_the_response(\n  exp2j_m1,\n  exp2j_m1_emm,\n  exp2j_m1_pairs[c(1,3,4),],\n  palette = pal_okabe_ito_blue,\n  legend_position = \"bottom\",\n  y_label = \"Glucose uptake\\n(nmol per mg protein\\nper 15 min)\",\n  y_pos = c(13.6, 13.2, 13.3)\n) +\n  geom_segment(x = 2 + dodge_width/2 - 0.05,\n               y = b[1] + b[2] + b[3],\n               xend = 2 + dodge_width/2 + 0.05,\n               yend = b[1] + b[2] + b[3],\n               linetype = \"dashed\",\n               color = \"gray\") +\n  geom_bracket(\n    x = 2.25,\n    y = b[1] + b[2] + b[3],\n    yend = b[1] + b[2] + b[3] + b[4],\n    label = paste0(\"ixn p = \",\n                  fmt_p_value_rmd(exp2j_m1_planned[4,\"p.value\"])),\n    text.size = 3,\n    text.hjust = 0,\n    color = \"black\") +\n  NULL\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\ngg\n\n\n\n\nDashed gray line is expected additive mean of KO Active",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Linear models with two categorical $X$ -- Factorial linear models (\"two-way ANOVA\")</span>"
    ]
  },
  {
    "objectID": "chapters/two-factors.html#understanding-the-linear-model-with-crossed-factors-1",
    "href": "chapters/two-factors.html#understanding-the-linear-model-with-crossed-factors-1",
    "title": "13  Linear models with two categorical \\(X\\) – Factorial linear models (“two-way ANOVA”)",
    "section": "13.3 Understanding the linear model with crossed factors 1",
    "text": "13.3 Understanding the linear model with crossed factors 1\nResearchers in experimental biology report almost exclusively the p-values of the simple effects (Table 13.1) in data from factorial experiments. These can be computed even if the data are flattened into a single “treatment” factor. But it is the interaction effect that is necessary to make many of the inferences made by the researchers. To see why, we need to understand what the coefficients in the coefficient table are.\n\n13.3.1 What the coefficients are\nTo understand the coefficients, it helps to use the means in the emmeans table to construct a factorial table of the cell means\n\n\n\n\nTable 13.2: Cell mean table\n\n\n\n\n\n\n\nRest\nActive\n\n\n\n\nWT\n6.75\n7.53\n\n\nKO\n10.20\n8.67\n\n\n\n\n\n\n\n\n\n\nThe linear model fit to the exp2j data is\n\\[\n\\begin{align}\n\\texttt{glucose\\_uptake} &= \\beta_{0} \\ + \\\\\n&\\quad \\ \\beta_{1}(\\texttt{stimulation}_{\\texttt{Active}}) \\ + \\\\\n&\\quad \\ \\beta_{2}(\\texttt{genotype}_{\\texttt{KO}}) \\ + \\\\\n&\\quad \\ \\beta_{3}(\\texttt{stimulation}_{\\texttt{Active}} : \\texttt{genotype}_{\\texttt{KO}})\n\\end{align}\n\\]\nand the fit coefficients are\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\nconf.low\nconf.high\n\n\n\n\n(Intercept)\n6.7501\n0.42\n16.1280\n0.000\n5.888\n7.612\n\n\nstimulationActive\n3.4482\n0.59\n5.8258\n0.000\n2.229\n4.667\n\n\ngenotypeKO\n0.7801\n0.64\n1.2202\n0.234\n-0.537\n2.097\n\n\nstimulationActive:genotypeKO\n-2.3072\n0.89\n-2.6056\n0.015\n-4.131\n-0.484\n\n\n\n\n\n\n\nExplainer\n\nUnderstand what the rows of the coefficient table are. There are four parameters in the fit linear model – the rows are the statistics for the estimates of these parameters. These estimates are the coefficients of the model.\nThe (Intercept) coefficient (\\(b_0\\)) is the mean glucose_uptake of the reference level, which was set to the “WT Rest” group (Figure 13.1). This is the mean in the upper left cell in Table 13.2.\nThe stimulationActive coefficient (\\(b_2\\)) is the estimate of the the added effect of Stimulation when the genotype factor is at its reference level, and so is the mean in the upper right cell minus the mean in the upper left cell in Table 13.2. This coefficient is not a mean – it is a difference of means (Figure 13.1). The mean of the “WT Active” group is \\(b_0 + b_2\\).\nThe genotypeKO coefficient (\\(b_1\\)) is the estimate of the the added effect of knocking out TLR9 when the stimulation factor is at its reference level, and so is the mean in the lower left cell minus the mean in the upper left cell in Table 13.2. This coefficient is not a mean – it is a difference of means (Figure 13.1). The mean of the “KO Rest” group is \\(b_0 + b_1\\).\nThe stimulationActive:genotypeKO coefficient (\\(b_3\\)) is the estimate of the interaction effect between genotype and stimulation. It is not the mean in the lower right cell minus the mean in the upper left cell. Instead, it is the mean in the lower right cell minus the expected mean in the lower right cell if the genotype and activity treatment effects were additive. The expected additive-mean in the lower right cell (“KO Active”) is \\(b_0 + b_1 + b_2\\). The mean of KO stimulation is \\(b_0 + b_1 + b_2 + b_3\\) (Figure 13.1).\nThe interaction effect is a non-additive effect. Think about this. Adding the Stimulation alone adds 3.45 nmol glucose per protein per 15 min to the uptake rate of the reference. Adding KO alone adds 0.78 glucose per protein per 15 min to the uptake rate of the reference. If these effects were purely additive, then adding both Stimulation and KO to the reference rate should result in a mean of 6.75 + 3.45 + 0.78 = 10.98 glucose per protein per 15 min. The modeled mean for “KO Active” is 8.67 glucose per protein per 15 min. The difference observed - additive is 8.67 - 10.98 = -2.31 glucose per protein per 15 min. Compare this difference to the interaction coefficient in the coefficient table.\nReinforce your understanding of non-additive in item 5. The interaction is a non-additive effect because the mean of the combined treatment is something different than if we were to just add the KO and Active effects. But this effect is additive in the linear model. This is what linear models are – a reference mean plus the sum of a bunch of effects.\nUnderstand what these rows are not. The stimulationActive row is not the same as the “stimulation” term in a Type III ANOVA table (the ANOVA table produced in GraphPad Prism or JMP). The p-values will be different because the p-values are testing different hypotheses. In the coefficient table, the stimulationActive p-value is testing the difference of the means of WT Active and WT Rest. The stimulation term in a Type III ANOVA table is testing if there is an overall stimulation effect, which is estimated as the average of the two stimulation contrasts (WT Active - WT Rest) and (KO Active - KO Rest). The average of these two contrasts is not often of interest (but sometimes is – see below).\n\n\n\n\n\n\n\n\n\nFigure 13.1: The coefficients of a linear model with two crossed factors, explained.\n\n\n\n\n\n\n\n13.3.2 The interaction effect is something different\nItem 6 in the coefficient table explainer stated “The interaction is a non-additive effect because the mean of the combined treatment is something different than if we were to just add the KO and Active effects. The something different is the interaction effect. If the interaction effect were zero, the expected effect of stimulation in the KO mice would be the same as the expected effect of stimulation in the WT mice (Figure 13.2). This would suggest the underlying physiological changes between Rest and Active in the KO mice is”more of the same” physiological changes in the WT mice. But, because of the interaction, the underlying physiological changes between Rest and Active in the KO mice is “something different” to that of physiological changes in the WT mice (Figure 13.2).\nThe biological reasons causing interaction effects are highly variable and are what makes Biology fun. Additive effects (no interaction) may occur when combined treatments act independently of each other. This might occur in the glucose uptake response if knocking out TLR9 opens a path to glucose uptake that is different from and independent of the paths activated by electrial stimulation. Positive, or synergistic interaction effects may occur when combined treatments augment each other’s ability to affect the response (see Example 3 below). This could occur in the glucose uptake response if knocking out TLR9 opens a path to glucose uptake that is different the paths activated by electrial stimulation but also makes the paths activated by stimulation more sensitive to stimulation. Negative, or antagonistic interaction effects may occur when combined treatments interfere with each other’s ability to affect the response. This could occur in the glucose uptake response if TLR9 is on the path from stimulation to glucose uptake. Knocking out TLR9 interferes with this path. If TLR9 is required, we’d expect the interaction effect to be the same magnitude but opposite sign of the control effect – that is, complete antagonism of the control effect. Previous experiments suggested this negative interaction. The measurement of this effect was the purpose of experiment exp2j.\n\n\n\n\n\n\n\n\nFigure 13.2: The interaction is something different, not more of the same. To get the observed difference (4), take the expected difference (3) and add something different (5).\n\n\n\n\n\n\n\n13.3.3 Why we want to compare the treatment effect to a control effect\nThe purpose of the experiment is to infer a TLR9 role in the regulation of muscle-stimulated glucose transport, that is, a stimulation-induced TLR9-/- effect. If TLR9 is in the pathway from muscle activity to glucose transport, we expect some kind of recovery to baseline (Rest) values in the KO Active group. But TLR9 may also (or alternatively) have a role in non-stimulation-induced glucose uptake.\nHow do we make an inference about a stimulation-induced TLR9-/- effect using this experiment? Researchers typically look at the treatment effect\n(KO Active - WT Active)\nand conclude a stimulation-induced TLR9-/- effect if it’s big (and negative)\nIf the treatment effect is the correct contrast for inference, why bother with the measures of glucose uptake during Rest? The reason the treatment effect alone is the wrong contrast for inferring the stimulation-induced TLR9 effect is because it is confounded by the control effect, which is the contrast (KO Rest - WT Rest) (Figure 13.3).\n\n\n\n\n\n\n\n\nFigure 13.3: Why the interaction effect is the stimulation-induced TLR9-/- effect\n\n\n\n\n\nTo unconfound the inference, subtract the confounder:\n(KO Active - WT Active) - (KO Rest - WT Rest)\nThis is the interaction effect. Consequences of interpreting the treatment effect KO Active - WT Active as the stimulation-induced TLR9-/- effect are highlighted in Figure 13.4. In these plots, the data are those from exp2j but with the values in the KO groups shifted up or down to create the scenarios.\nScenario 1. The positive control has big effect AND KO has no effect during rest AND the stimulation-induced TLR9-/- effect is equal in magnitude but opposite direction to the Active effect in WT (Figure 13.4 A). The stimulation-induced TLR9-/- effect is conspicuous from the plot, if our sample means are close to the true means and we have high precision. The simple effect measures the stimulation-induced TLR9-/- effect correctly – but this is because the simple effect is equal in magnitude (but opposite sign) to the interaction effect. Many experiments in the literature are pretty similar to this scenario.\nScenario 2. The positive control has big effect AND KO has a negative effect during rest BUT there is zero stimulation-induced TLR9-/- effect – that is, the interaction is zero (Figure 13.4 B). There is an effect of KO during stimulation but this effect is no different that that occuring during Rest. So this cannot be a contraction induced TLR9 effect. Simple effect 1 inflates the effect.\nScenario 3. The positive control has big effect AND KO has a positive effect during rest AND there is a stimulation-induced TLR9-/- effect that is equal to that in scenario 1 (Figure 13.4 C). The positive effect of KO during Rest masks the stimulation-induced TLR9-/- effect. The simple effect underestimates the stimulation-induced TLR9-/- effect. This is similar to what is happening in Experiment 2j.\n\n\n\n\n\n\n\n\nFigure 13.4: Scenarios to show the consequence of inferring the stimulation-induced TLR9-/- effect from the simple effect (KO Active - WT Active). The simple effect and interaction effect lines extend from the KO Active mean to either the KO Rest mean (simple) or the expected mean of KO Active if the two factors were additive (interaction).\n\n\n\n\n\n\n\n13.3.4 The order of the factors in the model tells the same story differently\nThe order of the factors in the model formula doesn’t matter for the coefficients, the estimated marginal means, or the contrasts. It can matter for ANOVA (more on this below) but not “tests after ANOVA”. But the order does matter to how the researchers communicates the results to themselves and in the report (Figure 13.5). The order is also a natural consequence of the two different ways of framing the research question.\n\n\n\n\n\n\n\n\nFigure 13.5: It may take some work but these plots show the same four means and effects. The only difference is how we communicate the story to ourselves and to others. A) the order of the factors in the model is stimulation * genotype. B) the order of the factors in the model is genotype * stimulation.\n\n\n\n\n\n\n\n13.3.5 Power for the interaction effect is less than that for simple effects\nThe interaction contrast is the difference of two simple contrasts and, consequently, the variance of the interaction contrast is twice that of the simple contrasts. And, consequently, the SE of the interaction estimate is \\(\\sim 1.4 \\times\\) larger than the precision of the two simple effects that form the contrast (1.4 is $sqrt{2}. The exact value will depend on differences in sample size among groups). The consequence of this is wider confidence intervals and larger p-values for the interaction contrast compared to a simple contrast of the same effect size.\n\n\n13.3.6 Planned comparisons vs. post-hoc tests\nThe contrasts computed above were planned based on questions motivating the experiment. The planned contrasts contain three of the four simple effects. The four simple effects are\n\nexp2j_m1_pairs &lt;- contrast(exp2j_m1_emm,\n                           method = \"revpairwise\",\n                           simple = \"each\",\n                           combine = TRUE,\n                           adjust = \"none\") |&gt;\n  summary(infer = TRUE)\nexp2j_m1_pairs |&gt;\n  kable(digits = c(1,1,1,2,3,1,2,2,1,6)) |&gt;\n  kable_styling()\n\n\n\n\ngenotype\nstimulation\ncontrast\nestimate\nSE\ndf\nlower.CL\nupper.CL\nt.ratio\np.value\n\n\n\n\nWT\n.\nActive - Rest\n3.45\n0.592\n25\n2.23\n4.67\n5.8\n0.000004\n\n\nKO\n.\nActive - Rest\n1.14\n0.659\n25\n-0.22\n2.50\n1.7\n0.095500\n\n\n.\nRest\nKO - WT\n0.78\n0.639\n25\n-0.54\n2.10\n1.2\n0.233785\n\n\n.\nActive\nKO - WT\n-1.53\n0.613\n25\n-2.79\n-0.27\n-2.5\n0.019668\n\n\n\n\n\n\n\nThere are six pairwise comparisons for this experiment. Two of these are not a simple effect:\n\nKO Active - WT Rest\nWT Active - KO Rest\n\nThese are the contrasts of the diagonal pairs in the cell-means table (Table 13.2). In a factorial design, we generally are not interested in these diagonal contrasts. They could be reported in a supplement. Recognize that if you are adjusting p-values for multiple tests, and you do not care about these contrasts but have included them in the computation of the adjustment, then your adjusted p-values are conservative.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Linear models with two categorical $X$ -- Factorial linear models (\"two-way ANOVA\")</span>"
    ]
  },
  {
    "objectID": "chapters/two-factors.html#example-2-estimation-of-the-effect-of-background-condition-on-an-effect-it-depends-experiment-3e-lesian-area-data",
    "href": "chapters/two-factors.html#example-2-estimation-of-the-effect-of-background-condition-on-an-effect-it-depends-experiment-3e-lesian-area-data",
    "title": "13  Linear models with two categorical \\(X\\) – Factorial linear models (“two-way ANOVA”)",
    "section": "13.4 Example 2: Estimation of the effect of background condition on an effect (“it depends”) (Experiment 3e lesian area data)",
    "text": "13.4 Example 2: Estimation of the effect of background condition on an effect (“it depends”) (Experiment 3e lesian area data)\n\n13.4.1 Understand the experimental design\nResearch question 1. What is the effect of the X chromosome complement on lipid-related disease markers? 2. What is the effect of sex (female or male gonad) on lipid-related disease markers? 3. How conditional is the sex effect on the chromosome complement? 4. How conditional is the effect of X chromosome complement on the level of sex?\nResponse variable lesian_area – atherosclerotic lesian area in aortic sinus.\nFactor 1 – sex (“Female”, “Male”). This is not the typical sex factor that is merely observed but is a manipulated factor. Sex is determined by the presence or absence of SRY on an autosome using the Four Core Genotype mouse model. SRY determines the gonad that develops (ovary or testis). “Female” does not have the autosome with SRY. “Male” has the autosome with SRY.\nFactor 2 – chromosome (“XX”, “XY”). The sex chromosome complement is not observed but manipulated. In “XX”, neither sex chromosome has SRY as the natural condition. In “XY”, SRY has been removed from the Y chromosome.\nDesign – \\(2 \\times 2\\), that is, two crossed factors each with two levels. This results in four groups, each with a unique combination of the levels from each factor. “Female XX” is the control. “Male XX” adds the autosomal SRY gene (and testes instead of an ovary). “Female XY” replaces the “X” complement with the engineered Y complement. “Male XY” has both the autosomal SRY and the engineered Y complement.\n\n\n\n\n\nsex\nchromosome\ntreatment\nchromosome complement\nautosome\ngonad\n\n\n\n\nFemale\nXX\nFXX\nX\nWT\novary\n\n\nMale\nXX\nMXX\nX\nSRY+\ntestis\n\n\nFemale\nXY\nFXY\nY (sry-)\nWT\novary\n\n\nMale\nXY\nMXY\nY (sry-)\nSRY+\ntestis\n\n\n\n\n\n\n\nThe research question suggest the following planned contrasts\nWhat is the effect of chromosome on lipid-related disease markers?\n\n(Female XX - Female XY) – effect of XX in mice with female gonad. If hypothesis is true, this should be large, negative.\n(Male XX - Male XY) – effect of XX in mice with male gonad. If hypothesis is true, this should be large, negative.\n\nWhat is the effect of sex on lipid-related disease markers?\n\n(Male XX - Female XX) – effect of sex in XX mice.\n(Male XY - Female XY) – effect of sex in XY in mice.\n\nHow conditional are the effects?\n\n(Male XX - Male XY) - (Female XX - Female XY) – Interaction.\n\nIn this experiment, there is no treatment contrast or control contrast. Instead, all four simple contrasts are of equal interest.\n\n\n13.4.2 Fit the linear model\n\nexp3e_m1 &lt;- lm(lesian_area ~ sex*chromosome, data = exp3e)\n\n\n\n13.4.3 Check the model\n\nggcheck_the_model(exp3e_m1)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n\n13.4.4 Inference from the model\nThe coefficient table\n\n# step 2 - get the coefficient table\nexp3e_m1_coef &lt;- tidy(exp3e_m1, conf.int = TRUE)\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\nconf.low\nconf.high\n\n\n\n\n(Intercept)\n0.540\n0.0353\n15.29\n0.0000\n0.464\n0.616\n\n\nsexMale\n-0.018\n0.0500\n-0.37\n0.7172\n-0.126\n0.089\n\n\nchromosomeXY\n-0.290\n0.0530\n-5.48\n0.0001\n-0.404\n-0.177\n\n\nsexMale:chromosomeXY\n0.143\n0.0749\n1.90\n0.0776\n-0.018\n0.303\n\n\n\n\n\n\n\nThe emmeans table\n\n# step 3 - get the modeled means\nexp3e_m1_emm &lt;- emmeans(exp3e_m1, specs = c(\"sex\", \"chromosome\"))\n\nexp3e_m1_emm |&gt;\n  summary() |&gt;\n  kable(digits = c(1,1,3,4,1,3,3)) |&gt;\n  kable_styling()\n\n\n\n\nsex\nchromosome\nemmean\nSE\ndf\nlower.CL\nupper.CL\n\n\n\n\nFemale\nXX\n0.540\n0.0353\n14\n0.464\n0.616\n\n\nMale\nXX\n0.522\n0.0353\n14\n0.446\n0.597\n\n\nFemale\nXY\n0.250\n0.0395\n14\n0.165\n0.335\n\n\nMale\nXY\n0.374\n0.0395\n14\n0.289\n0.459\n\n\n\n\n\n\n\nThe contrasts table\n\n# m1_emm # print in console to get row numbers\n# set the mean as the row number from the emmeans table\nfxx &lt;- c(1,0,0,0)\nmxx &lt;- c(0,1,0,0)\nfxy &lt;- c(0,0,1,0)\nmxy &lt;- c(0,0,0,1)\n\n# contrasts are the difference in the vectors created above\n# the focal contrasts are in the understand the experiment section\n# 1. (FXY - FXX) \n# 2. (MXY - MXX)\n# 3. (MXX - FXX)\n# 4. (MXY - FXY)\n# 5. Interaction\n\nexp3e_m1_planned &lt;- contrast(exp3e_m1_emm,\n                       method = list(\n                         \"FXY - FXX\" = c(fxy - fxx),\n                         \"MXY - MXX\" = c(mxy - mxx),\n                         \"MXX - FXX\" = c(mxx - fxx),\n                         \"MXY - FXY\" = c(mxy - fxy),\n                         \"Interaction\" = c(mxy - mxx) -\n                           c(fxy - fxx)\n                           \n                       ),\n                       adjust = \"none\"\n) |&gt;\n  summary(infer = TRUE)\n\n# check\n # exp3e_m1_ixn &lt;- contrast(exp3e_m1_emm,\n #                           interaction = c(\"revpairwise\"),\n #                           by = NULL)\n\nexp3e_m1_planned |&gt;\n  kable(digits = c(0,3,4,0,3,3,2,5)) |&gt;\n  kable_styling()\n\n\n\n\ncontrast\nestimate\nSE\ndf\nlower.CL\nupper.CL\nt.ratio\np.value\n\n\n\n\nFXY - FXX\n-0.290\n0.0530\n14\n-0.404\n-0.177\n-5.48\n0.00008\n\n\nMXY - MXX\n-0.148\n0.0530\n14\n-0.261\n-0.034\n-2.79\n0.01458\n\n\nMXX - FXX\n-0.018\n0.0500\n14\n-0.126\n0.089\n-0.37\n0.71716\n\n\nMXY - FXY\n0.124\n0.0559\n14\n0.004\n0.244\n2.22\n0.04309\n\n\nInteraction\n0.143\n0.0749\n14\n-0.018\n0.303\n1.90\n0.07762\n\n\n\n\n\n\n\nExplainer\n\nThe two simple effects and the interaction are computed separately. If we want to adjust for three comparisons, I would use the Holm method.\nThe magnitude of the estimated effect of XX in mice with male gonads is about 1/2 that in mice with female gonads. This difference is the magnitude of the interaction.\nDon’t infer “no effect” given the p-value of the interaction. The estimate of the interaction effect has the same magnitude as the estimated effect of XX in mice with male gonads and about 1/2 the magnitude as the estimated effect of XX in mice with female gonads. The interaction p-value suggests caution in our confidence of the sign of this effect.\n\n\n\n13.4.5 Plot the model\n\nexp3e_m1_plot &lt;- ggplot_the_model(\n  fit = exp3e_m1,\n  fit_emm = exp3e_m1_emm,\n  fit_pairs = exp3e_m1_planned,\n  palette = pal_okabe_ito_blue,\n  legend_position = \"bottom\",\n  y_label = expression(paste(\"Lesian area (\", mm^2, \")\")),\n  effect_label = expression(paste(\"Effect (\", mm^2, \")\")),\n  contrast_rows = \"all\",\n  rel_heights = c(0.5,1)\n)\nexp3e_m1_plot\n\n\n\n\n\n\n\n\n\n\n13.4.6 alternaPlot the model\n\nexp3e_m1_pairs &lt;- contrast(exp3e_m1_emm,\n                           method = \"revpairwise\",\n                           simple = \"each\",\n                           combine = TRUE,\n                           adjust = \"none\") |&gt;\n  summary(infer = TRUE)\nb &lt;- emm_b(exp3e_m1_emm)\ndodge_width &lt;- 0.4\ngg &lt;- ggplot_the_response(\n  exp3e_m1,\n  exp3e_m1_emm,\n  exp3e_m1_pairs[1:4,],\n  palette = pal_okabe_ito_blue,\n  legend_position = \"bottom\",\n  y_label = expression(paste(\"Lesian area (\", mm^2, \")\")),\n  y_pos = c(0.75, 0.78, 0.72, 0.72)\n) +\n  geom_segment(x = 2 + dodge_width/2 - 0.05,\n               y = b[1] + b[2] + b[3],\n               xend = 2 + dodge_width/2 + 0.05,\n               yend = b[1] + b[2] + b[3],\n               linetype = \"dashed\",\n               color = \"gray\") +\n  geom_bracket(\n    x = 2.25,\n    y = b[1] + b[2] + b[3],\n    yend = b[1] + b[2] + b[3] + b[4],\n    label = paste0(\"ixn p = \",\n                  fmt_p_value_rmd(exp3e_m1_planned[5,\"p.value\"])),\n    text.size = 3,\n    text.hjust = 0,\n    color = \"black\")\n\ngg\n\n\n\n\nDashed gray line is expected additive mean of KO Active",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Linear models with two categorical $X$ -- Factorial linear models (\"two-way ANOVA\")</span>"
    ]
  },
  {
    "objectID": "chapters/two-factors.html#understanding-the-linear-model-with-crossed-factors-2",
    "href": "chapters/two-factors.html#understanding-the-linear-model-with-crossed-factors-2",
    "title": "13  Linear models with two categorical \\(X\\) – Factorial linear models (“two-way ANOVA”)",
    "section": "13.5 Understanding the linear model with crossed factors 2",
    "text": "13.5 Understanding the linear model with crossed factors 2\n\n13.5.1 Conditional and marginal means\n\n\n\n\nTable 13.3: Conditional (white background) and marginal (color background) means from full factorial model fit to lesian area data\n\n\n\n\n\n\n\nFemale\nMale\nmean\n\n\n\n\nXX\n0.540\n0.522\n0.531\n\n\nXY\n0.250\n0.374\n0.312\n\n\nmean\n0.395\n0.448\n\n\n\n\n\n\n\n\n\n\n\nThe conditional means from the fit model are shown in the upper left \\(2 \\times 2\\) block (white background) of Table 13.3. These means are conditional on the level of sex and chromosome. For the linear model with two crossed factor here (with no continuous covariates), these conditional means are equal to the sample means of the treatment. The values in the last row and column are the marginal means, which are the means of the associated row or column cells (these values are in the margins of the table). More generally, marginal refers to a statistic averaged across multiple levels of another variable. The marginal means of the chromosome levels (orange background) are the means of the “XX” and “XY” rows. The marginal means of the sex levels (blue background) are the means of the “Female” and “Male” columns. Note that the marginal means are simple averages across cell means and not weighted averages where the weights are the sample size used to compute the conditional (cell) means.\n\n\n13.5.2 Simple (conditional) effects\nIn a factorial experiment with crossed A and B factors, there is an effect of factor A (relative to the reference, or another level of factor A) for each of the p levels of factor B. And, there is an effect of factor B (relative to the reference, or another level of factor B) for each of the m levels of factor A. These effects of one factor at each of the levels of the other factor are called the simple effects. I prefer conditional effects, since the value of the effect is conditional on the level of the other factor.\nFor the mouse lesian area experiment, there is a chromosome effect at sex = Female and a different effect at sex = Male. Similarly, there is a sex effect at chromosome = XX and a different effect at chromosome = XY.\n\n\n\nConditional (or \"simple\") effects of the factorial model.\n\n\nchromosome\nsex\ncontrast\nestimate\nSE\ndf\nlower.CL\nupper.CL\nt.ratio\np.value\n\n\n\n\nXX\n.\nMale - Female\n-0.018\n0.050\n14\n-0.126\n0.089\n-0.370\n0.71716\n\n\nXY\n.\nMale - Female\n0.124\n0.056\n14\n0.004\n0.244\n2.224\n0.04309\n\n\n.\nFemale\nXY - XX\n-0.290\n0.053\n14\n-0.404\n-0.177\n-5.479\n0.00008\n\n\n.\nMale\nXY - XX\n-0.148\n0.053\n14\n-0.261\n-0.034\n-2.786\n0.01458\n\n\n\n\n\n\n\nThe first two rows are the conditional effects of sex in each of the levels of chromosome. The last two rows are the conditional effects of chromosome in each of the levels of sex.\nTo help understand conditional effects, I add these to the \\(m \\times p\\) table of treatment combination means ((tab-twoway-simple-effects?)). The values in the right-side column (orange) are the conditional effects of sex at each level of chromosome These values are the difference of the means in the associated row. For example, the conditional effect of sex when chromosome = XY is 0.124 (second value in orange column). The values in the bottom row (blue) are the conditional effects of chromosome at each level of sex. These values are the difference of the means in the associated column. For example, the conditional effect of chromosome when sex = Female is -0.29 (first value in blue row). Note that the first conditional effect for each factor has a corresponding row in the table of coefficients of the fit model because these are the effects for that factor at the reference level of the other factor.\n\n\n\n\nTable 13.4: Conditional (\"simple\") effects of sex (orange background) and chromosome (blue background) on lesian area. The conditional means of each combination of the factor levels are in the cells with the white background. The simple effect is the difference in the means of the associated row or column.\n\n\n\n\n\n\n\nFemale\nMale\nsimple\n\n\n\n\nXX\n0.54\n0.522\n-0.018\n\n\nXY\n0.25\n0.374\n0.124\n\n\nsimple\n-0.29\n-0.148\n\n\n\n\n\n\n\n\n\n\n\n\n\n13.5.3 Marginal effects\nThe average of the conditional effects for a factor are the marginal effects, or the main effects in ANOVA terminology.\n\nemmeans(exp3e_m1, specs = \"sex\") |&gt;\n  contrast(method = \"revpairwise\")\n\nNOTE: Results may be misleading due to involvement in interactions\n\n\n contrast      estimate     SE df t.ratio p.value\n Male - Female   0.0529 0.0375 14   1.411  0.1800\n\nResults are averaged over the levels of: chromosome \n\nemmeans(exp3e_m1, specs = \"chromosome\") |&gt;\n  contrast(method = \"revpairwise\")\n\nNOTE: Results may be misleading due to involvement in interactions\n\n\n contrast estimate     SE df t.ratio p.value\n XY - XX    -0.219 0.0375 14  -5.844  &lt;.0001\n\nResults are averaged over the levels of: sex \n\n\nI’m showing the full output from the emmeans package output to highlight the warning that the inference “may be misleading” because of the interaction effect in the linear model. This is a healthy warning that I follow up on below.\nIn Table 13.5, I add the marginal effects to the table of conditional effects from above (Table 13.4).\n\n\n\n\nTable 13.5: Marginal effects of sex (orange) and chromosome (blue) on lesian area. Simple effects are in grey cells. The conditional means of each combination of the factor levels are in the white cells.\n\n\n\n\n\n\n\nFemale\nMale\nsimple\nmarginal\n\n\n\n\nXX\n0.54\n0.522\n-0.018\n\n\n\nXY\n0.25\n0.374\n0.124\n\n\n\nsimple\n-0.29\n-0.148\n\n-0.219\n\n\nmarginal\n\n\n0.053\n\n\n\n\n\n\n\n\n\n\n\n\n\n13.5.4 The additive model\nMarginal effects can be useful for summarizing a general trend, but, like any average, might not be especially meaningful if there is large heterogeneity of the simple effects, which occurs when the interaction effect is large.\nIf an interaction effect is small, and we want to summarize the results as general trends (“sex does this, chromosome does that”), then the best practice strategy is to refit a new linear model that estimates the effects of the two factors as if the interaction were equal to zero.\n\\[\n\\begin{align}\n\\texttt{lesian\\_area} &= \\beta_0 \\ + \\\\\n&\\quad \\ \\beta_1 \\texttt{sex}_\\texttt{Male} \\ + \\\\\n&\\quad \\ \\beta_2 \\texttt{chromosome}_\\texttt{XY}\n\\end{align}\n\\tag{13.1}\\]\nModel Equation 13.1 is a reduced model because one of the terms has been removed from the model. This particular reduced model is often referred to as the additive model, since it excludes the interaction term, which is non-additive effect (the indicator variable is the product of two “main” indicator variables). In R, this model is\n\nexp3e_m2 &lt;- lm(lesian_area ~ sex + chromosome, data = exp3e)\n\nThe model coefficients of the additive model are\n\n\n\nModel coefficients of additive model.\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\nconf.low\nconf.high\n\n\n\n\n(Intercept)\n0.508\n0.034\n15.06\n0.000\n0.436\n0.580\n\n\nsexMale\n0.045\n0.040\n1.11\n0.283\n-0.041\n0.131\n\n\nchromosomeXY\n-0.219\n0.041\n-5.39\n0.000\n-0.306\n-0.132\n\n\n\n\n\n\n\nExplainer\n\nsexMale is the average of the two conditional effects of “Male” (one at chromosome = \"XX\" and one at chromosome = \"XY).\nchromosomeXY is the average of the two conditional effects of “XY” (one at sex = \"Female\" and one at sex = \"Male).\n(Intercept) is the expected value without the added sexMale or chromosomeXY effects. This is a very abstract “average” of Female XX and is not the average value in the Female XX group.\n\nThe conditional effects of the reduced model are\n\n\n\n\nTable 13.6: Conditional effects of additive model.\n\n\n\n\n\n\nchromosome\nsex\ncontrast\nestimate\nSE\ndf\nlower.CL\nupper.CL\nt.ratio\np.value\n\n\n\n\nXX\n.\nMale - Female\n0.04495\n0.040\n15\n-0.041\n0.131\n1.11\n0.28292\n\n\nXY\n.\nMale - Female\n0.04495\n0.040\n15\n-0.041\n0.131\n1.11\n0.28292\n\n\n.\nFemale\nXY - XX\n-0.21895\n0.041\n15\n-0.306\n-0.132\n-5.39\n0.00007\n\n\n.\nMale\nXY - XX\n-0.21895\n0.041\n15\n-0.306\n-0.132\n-5.39\n0.00007\n\n\n\n\n\n\n\n\n\n\nExplainer\n\nIn an additive model, all conditional effects for one factor are the same for each level of the other factor. This makes sense. If the model fit is additive, the interaction effect is set to zero by the model and there cannot be differences in conditional effects among the contrasts at each of the levels of the other factor (otherwise, there would be an interaction). A more sensible way of thinking about this is, it doesn’t make sense to compute or discuss conditional effects in an additive model. Instead, an additive model automatically estimates marginal effects.\nCompare the table of marginal effects of the additive model to the table of marginal effects of the full model. The estimates for the chromosome effect are the same but the t-values and p-values differ because of different degrees of freedom (the full model estimates one more parameter, the interaction effect). The estimates for the sex effect are not the same between the two tables because of an imbalance of sample size. In the computation of the marginal effect of chromosome, the two simple effects both have sample size of 5 and 4. But in the computation of the marginal effect of sex, one simple effect has sample size of 5 and 5 while the other has a simple effect of 4 and 4.\n\n\n\n13.5.5 Reduce models for the right reason\nUnless one factor truly has no effect, there will always be an interaction. As stated above, interactions are ubiquitous. If an interaction is small, it can make sense to drop the interaction term and re-fit an additive model to estimate marginal effects in order to present a simplified picture of what is going on, with the recognition that these estimates are smoothing over the heterogenity in conditional (simple) effects that truly exist.\nAided and abetted by statistics textbooks for biologists, there is a long history of researchers dropping an interaction effect because the interaction \\(p&gt;0.05\\). A good rule of thumb is, don’t make model decisions based on p-values. It doesn’t make any sense.\n\nThe \\(p\\)-value is an arbitrary dichotomization of a continuous variable. Would it make sense to behave differently if the interaction were \\(p=0.06\\) vs. \\(p=0.04\\), given that these two p-values are effectively identical?\nA \\(p\\)-value is not evidence that an effect is zero, or “doesn’t exist”, or even that an effect is “trivially small”. This is because \\(p\\)-values are a function of measurement error, sampling error, and sample size, in addition to effect size.\n\nThe interaction p-value for the lesion-area data is 0.078. Should we refit the additive model and report a simpler story of “a” chromosome effect and “a” sex effect? This reduced model isn’t invalid and it is useful. Some considerations.\n\nthere is certainly a real interaction between these two factors and this interaction reflects interesting biology.\nFor this example, I might report both – the additive effect in the main paper (since the big chromosome complement effect is the story) and the conditional effects in the supplement, which might further work on investigating the underlying biology. Or maybe two sets of p-values on the plot? There are lots of unexplored ways to provide more “ways” of looking at the results.\nRegardless, for this example, I would not avoid reporting the interaction effect and the conditional effects somewhere. The estimated interaction effect (0.14 mm\\(^2\\)) is moderately large relative to the four simple effects. It’s much bigger than the marginal effect of sex and about 2/3 the size of the marginal effect of chromosome (Table 13.6).\nA response plot of both models (Figure 13.6) can help understanding and the decision of which model to report.\n\n\n\n\n\n\n\n\n\nFigure 13.6: A. Conditional means and p-values of conditional effects. B. Marginal means and p-values of marginal effects.\n\n\n\n\n\n\n\n13.5.6 The marginal means of an additive linear model with two factors can be weird\nTo better understand the marginal effects computed from the additive model, let’s compare the emmeans table of the factorial and additive models.\n\n\n\n\nTable 13.7: Conditional means of the lesian area data computed from the factorial model.\n\n\n\n\n\n\nsex\nchromosome\nemmean\nSE\ndf\nlower.CL\nupper.CL\n\n\n\n\nFemale\nXX\n0.540\n0.035\n14\n0.464\n0.616\n\n\nMale\nXX\n0.522\n0.035\n14\n0.446\n0.597\n\n\nFemale\nXY\n0.250\n0.039\n14\n0.165\n0.335\n\n\nMale\nXY\n0.374\n0.039\n14\n0.289\n0.459\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTable 13.8: Marginal means of the lesian area data computed from the additive model.\n\n\n\n\n\n\nsex\nchromosome\nemmean\nSE\ndf\nlower.CL\nupper.CL\n\n\n\n\nFemale\nXX\n0.508\n0.034\n15\n0.436\n0.580\n\n\nMale\nXX\n0.553\n0.034\n15\n0.481\n0.625\n\n\nFemale\nXY\n0.289\n0.036\n15\n0.212\n0.367\n\n\nMale\nXY\n0.334\n0.036\n15\n0.257\n0.412\n\n\n\n\n\n\n\n\n\n\nExplainer\n\nThe means in the conditional means table (Table 13.7) are equal to the sample means. These are conditional on sex and chromosome.\nThe means in the marginal means table (Table 13.8) are not equal to the sample means. These are modeled means of the four groups from a model in which there is no interaction effect, so all conditional effects for one factor are the same for each level of the other factor. If you take the difference of Male - Female for both the XX and the XY groups, these will be the same. All data has some measured interaction (even if there is no true interaction. But remember, interaction is ubiquitous in biology). The larger this interaction, the more weird the marginal means because these are less compatible with the data.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Linear models with two categorical $X$ -- Factorial linear models (\"two-way ANOVA\")</span>"
    ]
  },
  {
    "objectID": "chapters/two-factors.html#example-3-estimation-of-synergy-more-than-the-sum-of-the-parts-experiment-1c-ja-data",
    "href": "chapters/two-factors.html#example-3-estimation-of-synergy-more-than-the-sum-of-the-parts-experiment-1c-ja-data",
    "title": "13  Linear models with two categorical \\(X\\) – Factorial linear models (“two-way ANOVA”)",
    "section": "13.6 Example 3: Estimation of synergy (“More than the sum of the parts”) (Experiment 1c JA data)",
    "text": "13.6 Example 3: Estimation of synergy (“More than the sum of the parts”) (Experiment 1c JA data)\nTo explain what synergy is and why it is estimated by the interaction effect, this example uses data from an experiment measuring the effect of two defense signals on the defense response in Maize plants. In response to herbivory from insects, maize, and other plants, release multiple, chemical signals into the air (chemicals that evaporate into the air are known as volatile compounds). These chemicals signal the plant, and neighboring plants, to secrete anti-herbivory hormones, including abcisic acid and jasmonic acid. The researchers investigated the effects of two volatile compounds, (Z)‐3‐hexenyl acetate (HAC) and Indole, on the defense response both each without the other and in combination.\nThe example data come from Figure 1c, which is the effect of HAC and Indole on tissue concentrations of the hormone jasmonic acid (JA). The design is fully crossed with two factors, each with two levels: hac, with levels “HAC-” and “HAC+”, and indole, with levels (“Indole-” and “Indole+”).\n\n\n\n\n\n\nHAC-\nHAC+\n\n\n\n\nIndole-\nControl\nHAC\n\n\nIndole+\nIndole\nHAC+Indole\n\n\n\n\n\n\n13.6.1 Examine the data\n\nqplot(x = treatment, y = ja, data = exp1)\n\nWarning: `qplot()` was deprecated in ggplot2 3.4.0.\n\n\n\n\n\n\n\n\n\nToo few points for box plot. control variance is small. No obvious implausible points. fit with lm but recognize small n warning for any inference.\n\n\n13.6.2 Fit the model\n\nexp1c_m1 &lt;- lm(ja ~ hac * indole, data = exp1)\n\n\n\n13.6.3 Model check\n\nggcheck_the_model(exp1c_m1)\n\nWarning in rlm.default(x, y, weights, method = method, wt.method = wt.method, :\n'rlm' failed to converge in 20 steps\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nThe distribution looks like a sample from a Normal. The variance looks like it increases with the mean. This suggest gls modeling heterogeneity.\n\n\n13.6.4 Inference from the model\n\nexp1c_m1_coef &lt;- tidy(exp1c_m1, conf.int = TRUE)\n\nexp1c_m1_coef |&gt;\n  kable(digits = 3) |&gt;\n  kable_styling()\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\nconf.low\nconf.high\n\n\n\n\n(Intercept)\n43.862\n4.881\n8.986\n0.000\n33.515\n54.210\n\n\nhacHAC+\n15.615\n6.903\n2.262\n0.038\n0.982\n30.249\n\n\nindoleIndole+\n13.104\n6.903\n1.898\n0.076\n-1.529\n27.738\n\n\nhacHAC+:indoleIndole+\n29.813\n9.762\n3.054\n0.008\n9.118\n50.508\n\n\n\n\n\n\n\n\nexp1c_m1_emm &lt;- emmeans(exp1c_m1, specs = c(\"hac\", \"indole\"))\n\nexp1c_m1_emm |&gt;\n  kable(digits = c(1,1,1,2,1,1,1)) |&gt;\n  kable_styling()\n\n\n\n\nhac\nindole\nemmean\nSE\ndf\nlower.CL\nupper.CL\n\n\n\n\nHAC-\nIndole-\n43.9\n4.88\n16\n33.5\n54.2\n\n\nHAC+\nIndole-\n59.5\n4.88\n16\n49.1\n69.8\n\n\nHAC-\nIndole+\n57.0\n4.88\n16\n46.6\n67.3\n\n\nHAC+\nIndole+\n102.4\n4.88\n16\n92.0\n112.7\n\n\n\n\n\n\n\n\n# exp1c_m1_emm # print in console to get row numbers\n# set the mean as the row number from the emmeans table\nref &lt;- c(1,0,0,0)\nhac &lt;- c(0,1,0,0)\nindole &lt;- c(0,0,1,0)\nhac_indole &lt;- c(0,0,0,1)\n\n# contrasts are the difference in the vectors created above\n# these planned contrasts are described above\n# 1. (hac+/indole- - hac-/indole-) # add hac\n# 2. (hac-/indole+ -  hac-/indole-) # add indole\n# 3. (hac+/indole+ - hac-/indole+) - (hac+/indole- - hac-/indole-) # Interaction\n\nexp1c_m1_planned &lt;- contrast(\n  exp1c_m1_emm,\n  method = list(\n    \"hac+\" = c(hac - ref),\n    \"indole+\" = c(indole - ref),\n    \"HAC/Indole Ixn\" = c(hac_indole - indole) -\n      (hac - ref)\n  ),\n  adjust = \"none\"\n) |&gt;\n  summary(infer = TRUE)\n\nexp1c_m1_planned |&gt;\n  kable(digits = c(1,2,3,1,2,2,2,5)) |&gt;\n  kable_styling()\n\n\n\n\ncontrast\nestimate\nSE\ndf\nlower.CL\nupper.CL\nt.ratio\np.value\n\n\n\n\nhac+\n15.62\n6.903\n16\n0.98\n30.25\n2.26\n0.03796\n\n\nindole+\n13.10\n6.903\n16\n-1.53\n27.74\n1.90\n0.07583\n\n\nHAC/Indole Ixn\n29.81\n9.762\n16\n9.12\n50.51\n3.05\n0.00758\n\n\n\n\n\n\n\n\n\n13.6.5 Plot the model\n\n\n\n\n\n\n\n\n\n\n\n13.6.6 Alternative plot\n\n\n\n\n\nAn alternative plot for showing the estimate of synergy. Gray, dashed line is the expected mean of the HAC + Indole group if the interaction is zero.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Linear models with two categorical $X$ -- Factorial linear models (\"two-way ANOVA\")</span>"
    ]
  },
  {
    "objectID": "chapters/two-factors.html#understanding-the-linear-model-with-crossed-factors-3",
    "href": "chapters/two-factors.html#understanding-the-linear-model-with-crossed-factors-3",
    "title": "13  Linear models with two categorical \\(X\\) – Factorial linear models (“two-way ANOVA”)",
    "section": "13.7 Understanding the linear model with crossed factors 3",
    "text": "13.7 Understanding the linear model with crossed factors 3\n\n13.7.1 Thinking about the coefficients of the linear model\n\\[\n\\begin{align}\n\\texttt{ja} &= \\beta_0  \\ + \\\\\n&\\quad \\ \\beta_1 (\\texttt{hac}_\\texttt{HAC+}) \\ + \\\\  \n&\\quad \\ \\beta_2 (\\texttt{indole}_\\texttt{Indole+}) \\ + \\\\  \n&\\quad \\ \\beta_3 (\\texttt{hac}_\\texttt{HAC+}:\\texttt{indole}_\\texttt{Indole+})\n\\end{align}\n\\tag{13.2}\\]\nThe linear model makes it easy to understand synergy in the \\(2 \\times 2\\) design. If we start with the mean of the reference (the group without the added HAC or Indole), then \\(\\beta_1\\) is the extra bit due to adding HAC, \\(\\beta_2\\) is the extra bit due to adding Indole, and \\(\\beta_3\\) is the extra bit due to synergy between HAC and Indole. A positive \\(\\beta_3\\) means the combined treatment effect is more than the sum of the parts.\n\n\n\nCoefficient table of the factorial model\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\nconf.low\nconf.high\n\n\n\n\n(Intercept)\n43.86\n4.9\n8.986\n0.00\n33.51\n54.21\n\n\nhacHAC+\n15.62\n6.9\n2.262\n0.04\n0.98\n30.25\n\n\nindoleIndole+\n13.10\n6.9\n1.898\n0.08\n-1.53\n27.74\n\n\nhacHAC+:indoleIndole+\n29.81\n9.8\n3.054\n0.01\n9.12\n50.51\n\n\n\n\n\n\n\nAgain, the interaction is a non-additive effect. Adding HAC alone increases JA concentration by 15.6 ng per g FW. Adding Indole alone increases JA concentration by 13.1 ng per g FW. If these effects were purely additive, then adding both HAC and Indole to the Control mean should result in a mean of 43.9 + 15.6 + 13.1 = 72.6 ng per g FW in the HAC+Indole group. The modeled mean is 102.4 ng per g FW. The difference (observed - additive) is 102.4 - 72.6 = 29.8 ng per g FW. This is the estimated interaction effect in the coefficient table.\n\n\n\n\n\nSynergy is the bit needed to get to the HAC + Indole mean after adding the HAC effect and the Indole effect to the Control mean",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Linear models with two categorical $X$ -- Factorial linear models (\"two-way ANOVA\")</span>"
    ]
  },
  {
    "objectID": "chapters/two-factors.html#issues-in-inference",
    "href": "chapters/two-factors.html#issues-in-inference",
    "title": "13  Linear models with two categorical \\(X\\) – Factorial linear models (“two-way ANOVA”)",
    "section": "13.8 Issues in inference",
    "text": "13.8 Issues in inference\n\n13.8.1 For pairwise inference, it doesn’t matter if you fit a factorial or a flattened linear model, but…\nCompare the pairwise comparisons of the Experiment 2j glucose uptake data using a factorial linear model and a flattened linear model.\nFactorial:\n\nm1 &lt;- lm(glucose_uptake ~ stimulation * genotype, data = exp2j)\n\nm1_emm &lt;- emmeans(m1, specs = c(\"stimulation\", \"genotype\"))\n\nm1_pairs &lt;- contrast(m1_emm,\n                     method = \"revpairwise\",\n                     adjust = \"tukey\") |&gt;\n  summary(infer = TRUE)\n\nm1_pairs |&gt;\n  kable(digits = c(3)) |&gt;\n  kable_styling()\n\n\n\n\ncontrast\nestimate\nSE\ndf\nlower.CL\nupper.CL\nt.ratio\np.value\n\n\n\n\nActive WT - Rest WT\n3.448\n0.592\n25\n1.820\n5.076\n5.826\n0.000\n\n\nRest KO - Rest WT\n0.780\n0.639\n25\n-0.978\n2.539\n1.220\n0.620\n\n\nRest KO - Active WT\n-2.668\n0.639\n25\n-4.427\n-0.910\n-4.173\n0.002\n\n\nActive KO - Rest WT\n1.921\n0.613\n25\n0.236\n3.606\n3.136\n0.021\n\n\nActive KO - Active WT\n-1.527\n0.613\n25\n-3.212\n0.158\n-2.493\n0.086\n\n\nActive KO - Rest KO\n1.141\n0.659\n25\n-0.671\n2.953\n1.733\n0.329\n\n\n\n\n\n\n\nFlattened:\n\nm2 &lt;- lm(glucose_uptake ~ treatment, data = exp2j)\n\nm2_emm &lt;- emmeans(m2, specs = c(\"treatment\"))\n\nm2_pairs &lt;- contrast(m2_emm,\n                     method = \"revpairwise\",\n                     adjust = \"tukey\") |&gt;\n  summary(infer = TRUE)\n\nm2_pairs |&gt;\n  kable(digits = c(3)) |&gt;\n  kable_styling()\n\n\n\n\ncontrast\nestimate\nSE\ndf\nlower.CL\nupper.CL\nt.ratio\np.value\n\n\n\n\nWT Active - WT Rest\n3.448\n0.592\n25\n1.820\n5.076\n5.826\n0.000\n\n\nKO Rest - WT Rest\n0.780\n0.639\n25\n-0.978\n2.539\n1.220\n0.620\n\n\nKO Rest - WT Active\n-2.668\n0.639\n25\n-4.427\n-0.910\n-4.173\n0.002\n\n\nKO Active - WT Rest\n1.921\n0.613\n25\n0.236\n3.606\n3.136\n0.021\n\n\nKO Active - WT Active\n-1.527\n0.613\n25\n-3.212\n0.158\n-2.493\n0.086\n\n\nKO Active - KO Rest\n1.141\n0.659\n25\n-0.671\n2.953\n1.733\n0.329\n\n\n\n\n\n\n\n::: {.callout-tip, title=“Here’s the but…} But…just fit the factorial model. This makes it much easier to get the best practice mindless contrasts (the simple effects) and the interaction effect (either from the coefficient table or the emmeans package) and if Reviewer 2/PI/boss/old-school colleague demands p-value adjustment you will have slightly more power adjusting for a family of four simple effects than six pairwise comparisons (see below). :::\n\n\n13.8.2 For interaction interaction, it doesn’t matter if you fit a factorial or a flattened linear model, but…\nFactorial model:\n\n# using m1 from above\nm1_ixn &lt;- contrast(m1_emm,\n                     interaction = \"revpairwise\") |&gt;\n  summary(infer = TRUE)\n\nm1_ixn |&gt;\n  kable(digits = c(3)) |&gt;\n  kable_styling()\n\n\n\n\nstimulation_revpairwise\ngenotype_revpairwise\nestimate\nSE\ndf\nlower.CL\nupper.CL\nt.ratio\np.value\n\n\n\n\nActive - Rest\nKO - WT\n-2.307\n0.885\n25\n-4.131\n-0.484\n-2.606\n0.015\n\n\n\n\n\n\n\nFlattened model:\n\n# need to compute the interaction as a contrast\n# using m2 from the previous chunk\n\n# m2_emm # print in console to get row numbers\n# set the mean as the row number from the emmeans table\nwt_rest &lt;- c(1,0,0,0)\nwt_active &lt;- c(0,1,0,0)\nko_rest &lt;- c(0,0,1,0)\nko_active &lt;- c(0,0,0,1)\n\n# contrasts are the difference in the vectors created above\n# 4. (KO Active - WT Active) - (KO Rest - WT Rest) -- interaction \n\nm2_ixn &lt;- contrast(m2_emm,\n                   method = list(\n                     \"KO:Active Ixn\" = c(ko_active - wt_active) -\n                       (ko_rest - wt_rest)\n                   )) |&gt;\n  summary(infer = TRUE)\n\nm2_ixn |&gt;\n  kable(digits = c(3)) |&gt;\n  kable_styling()\n\n\n\n\ncontrast\nestimate\nSE\ndf\nlower.CL\nupper.CL\nt.ratio\np.value\n\n\n\n\nKO:Active Ixn\n-2.307\n0.885\n25\n-4.131\n-0.484\n-2.606\n0.015\n\n\n\n\n\n\n\n::: {.callout-tip, title=“Here’s the but…} But…see the previous tip :::\n\n\n13.8.3 Adjusting p-values for multiple tests\nFirst, you probably don’t want to adjust for multiple tests in a 2 x 2 (or larger) design (Chapter 12). Or, if you do, you want to adjust for multiple responses, not multiple contrasts within one response (Chapter 12). Understand that the norm in experimental bench biology misunderstands what a family of tests is (Chapter 12).\nBut, if Reviewer 2, or your PI, or an older colleague demands p-value adjustment, just say no. If you cannot, then read on.\nHere are seven different adjustment methods (including no adjustment) for the simple effects of the exp2j data.\n\n\n\n\nTable 13.9: P-values from seven different methods of adjustment. The Tukey-all p-value is from all six pairwise comparisons. All other p-values are from the four simple effects.\n\n\n\n\n\n\nContrast\nNone\nTukey-all\nBonferroni\nSidak\nHolm\nMvT\nBH\n\n\n\n\nActive WT - Rest WT\n0.00\n0.000\n0.000\n0.000\n0.000\n0.000\n0.000\n\n\nActive KO - Rest KO\n0.10\n0.329\n0.382\n0.331\n0.191\n0.275\n0.127\n\n\nRest KO - Rest WT\n0.23\n0.620\n0.935\n0.655\n0.234\n0.561\n0.234\n\n\nActive KO - Active WT\n0.02\n0.086\n0.079\n0.076\n0.059\n0.066\n0.039\n\n\n\n\n\n\n\n\n\n\n\nNone – no adjustment\nTukey – Tukey’s HSD method is the most common method used to compare all pairwise comparisons.\nBonferroni – Bonferroni is a general purpose method to compare any set of multiple tests. The test is conservative. Use the Holm modification instead.\nSidak – Sidak correction is a modification of Bonferroni with slightly more power.\nHolm – Holm-Bonferroni is a modification of Bonferroni with more power. Use this instead of Bonferroni.\nBH – controls the false discovery rate not the Type I error rate for a family of tests. See ?sec-mult-tests-fdr for full explanation of FDR and when we would want to use it.\nMvt – based on the multivariate t distribution and using covariance structure of the variables.\n\nFurther thoughts on multiple adjustment, focusing on the highlighted row in the table.\n\nReport the focal p-values on the figure. Then, report all pairwise and interaction, p-values in a supplement. Reporting all p-values on the figure overwhelms the principal message of a figure. There should be a table of these for every experiment in the supplement.\nIf you’re following best practice, report the focal none-adjusted p-values in the figure. Then, in the supplement, report the none-adjusted and adjusted value for all pairwise comparisons and interaction effects.\nIf you’ve adjusted for multiple response variables (Chapter 12), report the focal, adjusted values on the figure (adjusted for multple responses, not multiple tests within a response). Then, in the supplement, report the none-adjusted and adjusted value for all pairwise comparisons and interaction effects, again, using the adjustment for multiple responses not multiple tests within a response.\nIn a Fisherian world-view of p-values, there just isn’t any real difference in inference between \\(p = 0.02\\) (no adjustment) and \\(p = 0.086\\) (Tukey adjustment from all pairwise comparisons). Fisher expected decisions based on multiple replications of the experiment, all (or nearly) with \\(p &lt; 0.05\\). From a Fisherian-world-view, then, we have bigger how-to-do-science issues than multiple testing if we are making decisions entirely on \\(p = 0.02\\) versus \\(p = 0.086\\).\nFrom a Neyman-Pearson testing world-view, there is a consequential difference in inference between \\(p = 0.02\\) (no adjustment) and \\(p = 0.086\\) (Tukey adjustment from all pairwise comparisons) but I don’t think experimental bench biologists are practicing pure Neyman-Pearson.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Linear models with two categorical $X$ -- Factorial linear models (\"two-way ANOVA\")</span>"
    ]
  },
  {
    "objectID": "chapters/two-factors.html#two-way-anova",
    "href": "chapters/two-factors.html#two-way-anova",
    "title": "13  Linear models with two categorical \\(X\\) – Factorial linear models (“two-way ANOVA”)",
    "section": "13.9 Two-way ANOVA",
    "text": "13.9 Two-way ANOVA\nBoth wet-bench and ecology/evolution experimental biologists do ANOVA. Many textbooks, advisors, and colleagues tell researchers to report “tests after ANOVA” – what I call pairwise contrasts – but in modern computing, pairwise contrasts and ANOVA tables are both computed from the same underlying regression model. The ANOVA is not necessary for any inference, a point I return to below.\nLet’s use the glucose uptake experiment (Example 1) to explore the ANOVA table.\n\n13.9.1 How to read a two-way ANOVA table\n\n\n\n\nTable 13.10: ANOVA table of glucose uptake experiment (Example 1).\n\n\n\n\n\n\nEffect\ndf\nF\np.value\n\n\n\n\nstimulation\n1, 25\n26.86\n&lt;.0001\n\n\ngenotype\n1, 25\n0.71\n.41\n\n\nstimulation:genotype\n1, 25\n6.79\n.02\n\n\n\n\n\n\n\n\n\n\nExplainer\n\nThis ANOVA table is computed using Type III sum of squares and should match the output from most statistics software including Graphpad Prism. The relevance of this is addressed below.\nIn general, an ANOVA table has a row for each term in the underlying linear model. ANOVA is a method for decomposing the total variance into batches and each of these terms is a batch. For the linear model with two crossed factors, there is a term for each factor and a term for the interaction. stimulation and genotype are the main (or 1st order) effects. stimulation:genotype is the interaction (or 2nd order) effect. These terms are the row names of the table.\nSome ANOVA functions in R also include a row for the variance of the residuals from the fit model. This row can be useful for learning what the values in ANOVA table are but only one statistic from the row (the residual df) is especially useful for reporting. In Table 13.10, this residual df is included in the statistics for each term.\nMany ANOVA tables contain additional SS (sum of squares) and MSE (mean square error) columns. The MSE is the variance of the term and used to compute the F statistic. The SS is used to compute the MSE. Since these columns are used for computation but not reporting, Table 13.10 excludes these.\ndf (Degree of freedom) – If the term is a factor, the df will equal the number of levels (\\(k\\)) for the factor minus 1. Think of it this way: the contribution of the variance due to a factor is a function of the variability of the \\(k\\) level means around the grand mean. How many degrees of independent variation do these level means have, given that we know the grand mean? The answer is \\(k-1\\) – once the values of \\(k-1\\) level means are written down, the \\(k\\)th level mean has no freedom to vary; its value has to be \\(k\\bar{\\bar{Y}} - \\sum_i^{k-1}{Y_i}\\). For an interaction term, the df is the product of the df of each of the factors in the interaction.\nF (F-value or F-ratio) – This is the test statistic. It is the ratio of the MSE for the term divided by the MSE of the residual (this is strictly true for only “Fixed effects” ANOVA, which we have here).\np.value – the p-value for the test statistic. F is compared to an F-distribution, which is a distribution of F-values under the null.\n\n\n\n13.9.2 What do the main effects in an ANOVA table mean?\nIt is very common in the literature to see researchers report the rows of an ANOVA table by stating something like “We found an effect of stimulation on glucose uptake (\\(F_{1,25} = 26.9\\), \\(p &lt; 0.0001\\))”. Thinking of a main effect term in an ANOVA table as “an effect” can be misleading. What does “an effect” mean? Afterall, there are two effects of stimulation in this experiment, one in the wildtype mice and one in the TLR9-/- mice.\nMain effects in ANOVA tables are about average effects. A main effect in a two-way ANOVA table is an “overall” marginal effect. Recall that a marginal effect of the level of Factor A is the average of the conditional effects at each level of B (Section 13.5.3). For a \\(2 \\times 2\\) table, there is only one marginal effect for each factor and, as a consquence, the p value for the main effect term of stimulation in the ANOVA table (Table 13.11) is equal to the p-value of the marginal effect of stimulation in the contrast table (Table 13.12). The “main term” effect of stimulation is illustrated in Figure 13.7.\n\n\n\n\nTable 13.11: Reprinting the ANOVA table of the glucose uptake experiment (Example 1) to show more decimal places of the p-value.\n\n\n\n\n\n\n\nnum Df\nden Df\nF\nPr(&gt;F)\n\n\n\n\nstimulation\n1\n25\n26.86\n0.0000232\n\n\ngenotype\n1\n25\n0.71\n0.4068630\n\n\nstimulation:genotype\n1\n25\n6.79\n0.0152305\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTable 13.12: Marginal effect of stimulation on glucose uptake.\n\n\n\n\n\n\ncontrast\nestimate\nSE\ndf\nlower.CL\nupper.CL\nt.ratio\np.value\n\n\n\n\nActive - Rest\n2.29\n0.443\n25\n1.38\n3.21\n5.183\n0.0000232\n\n\n\n\n\n\n\n\n\n\nAgain, main effects in ANOVA tables are about average effects. Is the average effect what we want to report? The answer is, it depends. It does not depend on the p-value of the interaction (the answer in many textbooks and websites) although the p-value is, perhaps, not irrelevant. Rather, it depends on the research question motivating the experiment. The research question in Experiment 2j specifically predicted a different treatment (KO Active - KO Rest) and control effect (WT Active - WT Rest). The average of these two effects isn’t of interest. Any inference about the average effect of stimulation (the p-value from the ANOVA table or the marginal effect size or CIs) doesn’t answer any question motivating the experiment. A more general discussion of when we might be interested in the main effect term of an ANOVA table (or better, marginal effects) was in section xxx and below in section xxx.\n\n\n\n\n\n\n\n\nFigure 13.7: The main effect of stimulation in the ANOVA table tests the average of the conditional effects of stimulation. The average of the conditional effects is the difference between the marginal means of Active and of Rest.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Linear models with two categorical $X$ -- Factorial linear models (\"two-way ANOVA\")</span>"
    ]
  },
  {
    "objectID": "chapters/two-factors.html#more-issues-in-inference",
    "href": "chapters/two-factors.html#more-issues-in-inference",
    "title": "13  Linear models with two categorical \\(X\\) – Factorial linear models (“two-way ANOVA”)",
    "section": "13.10 More issues in inference",
    "text": "13.10 More issues in inference\n\n13.10.1 Longitudinal experiments – include Time as a random factor (better than repeated measures ANOVA)",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Linear models with two categorical $X$ -- Factorial linear models (\"two-way ANOVA\")</span>"
    ]
  },
  {
    "objectID": "chapters/two-factors.html#working-in-r",
    "href": "chapters/two-factors.html#working-in-r",
    "title": "13  Linear models with two categorical \\(X\\) – Factorial linear models (“two-way ANOVA”)",
    "section": "13.11 Working in R",
    "text": "13.11 Working in R\n\n13.11.1 Model formula\nA linear model with two crossed factors is specified in the model formula as y ~ A*B where A is the first factor, and B is the second factor. R expands this formula to y ~ 1 + A + B + A:B where the colon indicates an interaction (multiplicative) effect.\n\nm1 &lt;- lm(glucose_uptake ~ stimulation * genotype, data = exp2j)\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\nconf.low\nconf.high\n\n\n\n\n(Intercept)\n6.75\n0.419\n16.13\n0.0000000\n5.89\n7.61\n\n\nstimulationActive\n3.45\n0.592\n5.83\n0.0000045\n2.23\n4.67\n\n\ngenotypeKO\n0.78\n0.639\n1.22\n0.2337850\n-0.54\n2.10\n\n\nstimulationActive:genotypeKO\n-2.31\n0.885\n-2.61\n0.0152305\n-4.13\n-0.48\n\n\n\n\n\n\n\nThe order of the factors in the model formula doesn’t matter for the values of the coefficients, the estimated marginal means, or the contrasts. It can matter for ANOVA (more on this below) but not “tests after ANOVA”.\n\nm1_b &lt;- lm(glucose_uptake ~ genotype * stimulation, data = exp2j)\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\nconf.low\nconf.high\n\n\n\n\n(Intercept)\n6.75\n0.419\n16.13\n0.0000000\n5.89\n7.61\n\n\ngenotypeKO\n0.78\n0.639\n1.22\n0.2337850\n-0.54\n2.10\n\n\nstimulationActive\n3.45\n0.592\n5.83\n0.0000045\n2.23\n4.67\n\n\ngenotypeKO:stimulationActive\n-2.31\n0.885\n-2.61\n0.0152305\n-4.13\n-0.48\n\n\n\n\n\n\n\nThe additive model is specified by the formula y ~ A + B\n\nm2 &lt;- lm(glucose_uptake ~ stimulation + genotype, data = exp2j)\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\nconf.low\nconf.high\n\n\n\n\n(Intercept)\n7.27\n0.408\n17.82\n0.0000000\n6.43\n8.10\n\n\nstimulationActive\n2.42\n0.487\n4.97\n0.0000368\n1.42\n3.42\n\n\ngenotypeKO\n-0.42\n0.489\n-0.86\n0.3954703\n-1.43\n0.58\n\n\n\n\n\n\n\n\n\n13.11.2 Using the emmeans function\n\n13.11.2.1 Conditional means table\n\n# model m1 fit above\n# m1 &lt;- lm(glucose_uptake ~ stimulation * genotype, data = exp2j)\n\nm1_emm &lt;- emmeans(m1, specs = c(\"stimulation\", \"genotype\"))\n\nm1_emm\n\n stimulation genotype emmean    SE df lower.CL upper.CL\n Rest        WT         6.75 0.419 25     5.89     7.61\n Active      WT        10.20 0.419 25     9.34    11.06\n Rest        KO         7.53 0.483 25     6.53     8.53\n Active      KO         8.67 0.447 25     7.75     9.59\n\nConfidence level used: 0.95 \n\n\nNotes\n\nPrinting the emmeans object displays useful information. Here, this information includes the confidence level used. If the object is printed using kable() |&gt; kable_styling() (as in the “Inference” and “Understanding” sections above), only the table is printed and the additional information is lost.\nemmeans computes the modeled means of all combinations of the levels of the factor variables specified in specs.\nIf there are two factor variables in the model, and both are passed to specs, then the modeled means of all combinations of the levels of the two variables are computed. If only one factor variable is passed, then the marginal means (averaged over all levels of the missing factor) are computed (see below).\n\n\n\n13.11.2.2 Marginal means\n\nm1_emm_stimulation &lt;- emmeans(m1, specs = c(\"stimulation\"))\n\nNOTE: Results may be misleading due to involvement in interactions\n\nm1_emm_stimulation\n\n stimulation emmean    SE df lower.CL upper.CL\n Rest          7.14 0.320 25     6.48      7.8\n Active        9.43 0.306 25     8.80     10.1\n\nResults are averaged over the levels of: genotype \nConfidence level used: 0.95 \n\n\nNotes\n\nIn a model with two crossed factors, y ~ A * B, the marginal means of the levels of A, averaged over all levels of B are computed by setting specs = to A only.\nRemember that the specs argument sets the values of the predictors for which we want a mean. By excluding B, we don’t get the means of the levels of A at each level of B but averaged across the levels of B.emmeans “knows” to average across the levels of B because B is in the model.\n\n\n\n\n13.11.3 Contrasts\n\n13.11.3.1 Planned contrasts\n\nwt_rest &lt;- c(1,0,0,0)\nwt_active &lt;- c(0,1,0,0)\nko_rest &lt;- c(0,0,1,0)\nko_active &lt;- c(0,0,0,1)\n\nm1_planned &lt;- contrast(\n  m1_emm,\n  method = list(\n    \"(WT Active - WT Rest)\" = c(wt_active - wt_rest),\n    \"(KO Rest - WT Rest)\" = c(ko_rest - wt_rest),\n    \"(KO Active - WT Active)\" = c(ko_active - wt_active),\n    \"KO:Active Ixn\" = c(ko_active - wt_active) -\n      (ko_rest - wt_rest)\n  ),\n  adjust = \"none\"\n) |&gt;\n  summary(infer = TRUE)\n\n\n\n\n\n\ncontrast\nestimate\nSE\ndf\nlower.CL\nupper.CL\nt.ratio\np.value\n\n\n\n\n(WT Active - WT Rest)\n3.45\n0.592\n25\n2.23\n4.67\n5.83\n0.0000\n\n\n(KO Rest - WT Rest)\n0.78\n0.639\n25\n-0.54\n2.10\n1.22\n0.2338\n\n\n(KO Active - WT Active)\n-1.53\n0.613\n25\n-2.79\n-0.27\n-2.49\n0.0197\n\n\nKO:Active Ixn\n-2.31\n0.885\n25\n-4.13\n-0.48\n-2.61\n0.0152\n\n\n\n\n\n\n\n\n\n13.11.3.2 All pairwise effects\n\nm1_pairs &lt;- contrast(m1_emm,\n                     method = \"revpairwise\",\n                     adjust = \"tukey\",\n                     level = 0.95) |&gt;\n  summary(infer = TRUE) # add the 95% CIs\n\nm1_pairs\n\n contrast              estimate    SE df lower.CL upper.CL t.ratio p.value\n Active WT - Rest WT       3.45 0.592 25    1.820    5.076   5.826  &lt;.0001\n Rest KO - Rest WT         0.78 0.639 25   -0.978    2.539   1.220  0.6202\n Rest KO - Active WT      -2.67 0.639 25   -4.427   -0.910  -4.173  0.0017\n Active KO - Rest WT       1.92 0.613 25    0.236    3.606   3.136  0.0212\n Active KO - Active WT    -1.53 0.613 25   -3.212    0.158  -2.493  0.0857\n Active KO - Rest KO       1.14 0.659 25   -0.671    2.953   1.733  0.3287\n\nConfidence level used: 0.95 \nConf-level adjustment: tukey method for comparing a family of 4 estimates \nP value adjustment: tukey method for comparing a family of 4 estimates \n\n\nNotes\n\nNote that printing the contrast object displays useful information, including the method of adjustment for multiple tests. If the object is printed using kable() |&gt; kable_styling() (as in the “Inference” and “Understanding” sections above), only the table is printed and the additional information is lost.\nThe method argument is used to control the set of contrasts that are computed. See below.\nThe adjust argument controls if and how to adjust for multiple tests. Each method has a default adjustment method. See below.\nThe level argument controls the percentile boundaries of the confidence interval. The default is 0.95. Including this argument with this value makes this level transparent.\n\n\n\n13.11.3.3 Simple effects\n\nm1_simple &lt;- contrast(m1_emm,\n                     method = \"revpairwise\",\n                     simple = \"each\",\n                     combine = TRUE,\n                     adjust = \"fdr\") |&gt;\n  summary(infer = TRUE) # add the 95% CIs\n\n\n\n\n\n\ngenotype\nstimulation\ncontrast\nestimate\nSE\ndf\nlower.CL\nupper.CL\nt.ratio\np.value\n\n\n\n\nWT\n.\nActive - Rest\n3.45\n0.592\n25\n1.86\n5.04\n5.83\n0.0000\n\n\nKO\n.\nActive - Rest\n1.14\n0.659\n25\n-0.63\n2.91\n1.73\n0.1273\n\n\n.\nRest\nKO - WT\n0.78\n0.639\n25\n-0.94\n2.50\n1.22\n0.2338\n\n\n.\nActive\nKO - WT\n-1.53\n0.613\n25\n-3.18\n0.12\n-2.49\n0.0393\n\n\n\n\n\n\n\n\n\n13.11.3.4 Interaction contrasts\nThe interaction contrasts can be computed as in Planned contrasts above or using the argument “interaction =”\n\nm1_ixn &lt;- contrast(m1_emm,\n                     interaction = \"revpairwise\",\n                     adjust = \"none\") |&gt;\n  summary(infer = TRUE) # add the 95% CIs\n\n\n\n\n\n\nstimulation_revpairwise\ngenotype_revpairwise\nestimate\nSE\ndf\nlower.CL\nupper.CL\nt.ratio\np.value\n\n\n\n\nActive - Rest\nKO - WT\n-2.31\n0.885\n25\n-4.13\n-0.48\n-2.61\n0.0152\n\n\n\n\n\n\n\n\n\n13.11.3.5 Method argument\n\nm1_pairs &lt;- contrast(m1_emm,\n                     method = \"pairwise\",\n                     adjust = \"fdr\") |&gt;\n  summary(infer = TRUE) # add the 95% CIs\n\nm1_pairs |&gt;\n  kable() |&gt;\n  kable_styling()\n\n\n\n\ncontrast\nestimate\nSE\ndf\nlower.CL\nupper.CL\nt.ratio\np.value\n\n\n\n\nRest WT - Active WT\n-3.4482471\n0.5918936\n25\n-5.1439551\n-1.7525392\n-5.825789\n0.0000269\n\n\nRest WT - Rest KO\n-0.7800781\n0.6393182\n25\n-2.6116520\n1.0514958\n-1.220172\n0.2337850\n\n\nRest WT - Active KO\n-1.9211230\n0.6126681\n25\n-3.6763474\n-0.1658985\n-3.135667\n0.0086970\n\n\nActive WT - Rest KO\n2.6681690\n0.6393182\n25\n0.8365951\n4.4997430\n4.173460\n0.0009510\n\n\nActive WT - Active KO\n1.5271242\n0.6126681\n25\n-0.2281003\n3.2823486\n2.492580\n0.0295018\n\n\nRest KO - Active KO\n-1.1410449\n0.6585984\n25\n-3.0278545\n0.7457647\n-1.732535\n0.1145997\n\n\n\n\n\n\n\n\nm1_pairs &lt;- contrast(m1_emm,\n                     method = \"trt.vs.ctrl\",\n                     adjust = \"none\") |&gt;\n  summary(infer = TRUE) # add the 95% CIs\n\nm1_pairs |&gt;\n  kable() |&gt;\n  kable_styling()\n\n\n\n\ncontrast\nestimate\nSE\ndf\nlower.CL\nupper.CL\nt.ratio\np.value\n\n\n\n\nActive WT - Rest WT\n3.4482471\n0.5918936\n25\n2.2292194\n4.667275\n5.825789\n0.0000045\n\n\nRest KO - Rest WT\n0.7800781\n0.6393182\n25\n-0.5366223\n2.096779\n1.220172\n0.2337850\n\n\nActive KO - Rest WT\n1.9211230\n0.6126681\n25\n0.6593094\n3.182936\n3.135667\n0.0043485\n\n\n\n\n\n\n\nNotes\n\nmethod = \"pairwise\" and method = \"revpairwise\" compute all pairwise comparisons. I prefer “revpairwise” because the contrasts that include the reference are in the direction non-reference minus reference.\nmethod = \"trt.vs.ctrl\" gives a very flattened picture of the model results and constrains what we can infer from the results.\n\n\n\n13.11.3.6 Adjustment for multiple tests\n\n\n\n\n\n\nNHST Blues\n\n\n\nSee Section 12.3 for why Tukey (or any) p-value adjustment for multiple tests is unjustified in many bench biology experiments, even though adjustment is the norm\nSee Section 12.4 for why we often want to use a FDR method to adjust p-values in many bench biology experiments, even though not adjusting is the norm.\nSeriously, you’d be better off ignoring what’s below and clicking these links or just reading all of Chapter 12\n\n\nset the adjust = argument to\n\n“none” – no adjustment.\n“BH” or “fdr” – controls the false discovery rate using the logic of Benjamini & Hochberg. The BH method assumes independent tests. This or BY should be used for most uses of p-value adjustment in experimental bench biology.\n“BY” – is a modification of BH using the logic of Benjamini & Yekutieli. The BY method relaxes the independent test assumption and consequently is more consertive than BH. This or BY should be used for most uses of p-value adjustment in experimental bench biology.\n“holm” – Holm-Bonferroni is the Holm modification of Bonferroni and is used to control the FWER. It is more powerful than Bonferroni while still holding FWER at or below alpha. For most uses of p-value adjustment in experimental bench biology, we are less interested in controling the FWER than the FDR.\n“bonferroni” – Bonferroni is the original, general-purpose adjustment for a set of p-values to control the FWER. It has less power than the Holm modification, so it’s use should be historic only.\n“mvt” – based on the multivariate t distribution and using covariance structure of the variables.\n“dunnettx” – Dunnett’s test is a method used when comparing all treatments to a single control.\n“tukey” – Tukey’s HSD method is the most common method used to compare all pairwise comparisons.\n\n\n\n\n13.11.4 Practice safe ANOVA\nMany researchers are unaware of the different ways of computing the sum of squares in an ANOVA table. The F-value and p-value are both functions of the sum of squares. When the cells of a two-way ANOVA are not balanced (different sample sizes among the cells), the different ways of computing the sum of squares can matter. There is a great deal of sound and fury about which methods should be used and which avoided. For many research questions and the experiments used to address these, these arguments are moot because the ANOVA table is simply unnecessary for inference. Instead, fit the linear model and compute the contrasts of interest.\nIf your reviewer/advisor/boss wants an ANOVA table and wants this table to match that in Graphpad Prism or JMP, then you need a table computed from Type III sum of squares. If using a linear model to compute the ANOVA (remember that ANOVA was developed without fitting linear models), then the linear model needs to be fit using a model matrix constructed with indicator variables using effects coding. There are two safe ways to do this in R.\n\n13.11.4.1 The afex aov_4 function\n\n# .I is a data.table function that returns the row number\nexp2j[, fake_id := paste(\"mouse\", .I)]\n\nm1_aov4 &lt;- aov_4(glucose_uptake ~ stimulation * genotype +\n                   (1|fake_id),\n                 data = exp2j)\n\nnice(m1_aov4, MSE = FALSE)[,-4] |&gt; # delete ges column\n  kable() |&gt;\n  kable_styling()\n\n\n\n\nEffect\ndf\nF\np.value\n\n\n\n\nstimulation\n1, 25\n26.86 ***\n&lt;.0001\n\n\ngenotype\n1, 25\n0.71\n.41\n\n\nstimulation:genotype\n1, 25\n6.79 *\n.02\n\n\n\n\n\n\n\nNotes\n\nThe afex package has three function names for generating the same ANOVA table and statistics – here I’m using aov_4 because this functions uses a linear model formula argument (specifically, that used in the lme4 package), which is consistent with the rest of this text.\nThe formula includes the addition of a random factor ((1|id)) even though there really is no random factor in this model. See Models with random factors – Blocking and pseudoreplication for more information on random factors. The random factor (the factor variable “id” created in the line before the fit model line) identifies the individual mouse from which the response variable was measured. Because the response was only measured once on each individual mouse, “id” is not really a random factor but the addition of this in the model formula is necessary for the aov_4 function to work.\n\n\n\n13.11.4.2 The car Anova function\n\ntype3 &lt;- list(stimulation = contr.sum,\n              genotype = contr.sum)\nm1_type3 &lt;- lm(glucose_uptake ~ stimulation * genotype,\n               data = exp2j,\n               contrasts = type3)\nAnova(m1_type3, type=\"3\")[-1, -1] |&gt; # delete row 1 and col 1\n  kable(digits = c(1,2,5)) |&gt;\n  kable_styling()\n\n\n\n\n\nDf\nF value\nPr(&gt;F)\n\n\n\n\nstimulation\n1\n26.86\n0.00002\n\n\ngenotype\n1\n0.71\n0.40686\n\n\nstimulation:genotype\n1\n6.79\n0.01523\n\n\nResiduals\n25\n\n\n\n\n\n\n\n\n\nNotes\n\ncar::Anova has arguments for reporting the Type II and Type III sum of squares.\nBackground: The default model matrix in the lm function uses dummy (or treatment) coding. For a Type 3 SS ANOVA (the kind that matches that in Graphpad Prism or JMP), we need to tell lm to use sum (or deviation) coding.\nThe best practice method for changing the contrasts in the model matrix is using the contrasts argument within the lm function, as in the code above to fit m1_type3. This is the safest practice because this sets the contrasts only for this specific fit.\nThe coefficients of m1_type3 will be different from those using the default contrasts. Except when computing a Type III ANOVA, this text uses the default contrasts throughout because the coefficients make sense for the kinds of experiments in experimental biology. If using sum (“type III”) coding, the intercept will be the grand mean and the coefficients of the non-reference levels (the effects) will be their deviations from the grand mean. I don’t find this definition of “effects” very useful for most experiments in biology (but “useful” is largely a function of “used to”).\nThe contrasts (differences in the means among pairs of groups) in the contrast table will be the same, regardless of the contrast coding.\n\n\nDanger!. Many online sites suggest this bit of code before a Type III ANOVA using car::Anova:\noptions(contrasts = c(\"contr.sum\", \"contr.poly\")\nIf you’re reading this book, you almost certainly don’t want to do this because this code resets how R computes coefficients of linear models and SS of ANOVA tables. This will effect all future analysis until the contrasts are set to something else or a new R session is started.\n\n\n\n\n13.11.5 Better to avoid these\n\nm1_aov &lt;- aov(glucose_uptake ~ stimulation * genotype,\n               data = exp2j)\nsummary(m1_aov)\n\n                     Df Sum Sq Mean Sq F value   Pr(&gt;F)    \nstimulation           1  41.75   41.75  29.796 1.14e-05 ***\ngenotype              1   1.28    1.28   0.913   0.3485    \nstimulation:genotype  1   9.51    9.51   6.789   0.0152 *  \nResiduals            25  35.03    1.40                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n# same as exp2j_m1 in the Example 1 section\nm1 &lt;- lm(glucose_uptake ~ stimulation * genotype,\n               data = exp2j)\nanova(m1)\n\nAnalysis of Variance Table\n\nResponse: glucose_uptake\n                     Df Sum Sq Mean Sq F value    Pr(&gt;F)    \nstimulation           1 41.755  41.755 29.7959 1.143e-05 ***\ngenotype              1  1.279   1.279  0.9128   0.34853    \nstimulation:genotype  1  9.514   9.514  6.7890   0.01523 *  \nResiduals            25 35.034   1.401                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nNotes\n\nNote the differences from the the afex table and the car Anova table. The p-values for the interaction term is the same – this will always be the case for the highest order interaction term in the model. The p-values for stimulation and genotype terms are different. The difference will be a function of the degree of imbalance. Here, this difference doesn’t make a difference for inference. In some data sets, the difference is catastrophic.\nMany introduction to statistics textbooks and websites teach the base R aov function. This function is only useful if 1) the data are balanced and 2) we care about ANOVA. Don’t use this. If you need the ANOVA table use either afex or car Anova.\nThe base R anova is useful if you know what you are doing with it. Otherwise, use either afex or car Anova.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Linear models with two categorical $X$ -- Factorial linear models (\"two-way ANOVA\")</span>"
    ]
  },
  {
    "objectID": "chapters/two-factors.html#hidden-code",
    "href": "chapters/two-factors.html#hidden-code",
    "title": "13  Linear models with two categorical \\(X\\) – Factorial linear models (“two-way ANOVA”)",
    "section": "13.12 Hidden Code",
    "text": "13.12 Hidden Code\n\n13.12.1 Import exp2j (Example 1)\n\ndata_from &lt;- \"TLR9 and beclin 1 crosstalk regulates muscle AMPK activation in exercise\"\nfile_name &lt;- \"41586_2020_1992_MOESM4_ESM.xlsx\"\n\nfile_path &lt;- here(data_folder, data_from, file_name)\n\ntreatment_levels  &lt;- c(\"WT Rest\",\n                       \"WT Active\",\n                       \"KO Rest\",\n                       \"KO Active\")\nexp2j_wide &lt;- read_excel(file_path,\n                         sheet = \"2j\",\n                         range = \"A5:D13\",\n                         col_names = TRUE) |&gt;\n  data.table()\n\ncolnames(exp2j_wide) &lt;- treatment_levels\n\nexp2j &lt;- melt(exp2j_wide,\n              measure.vars = treatment_levels,\n              variable.name = \"treatment\",\n              value.name = \"glucose_uptake\") |&gt;\n  na.omit() # danger!\n\nexp2j[, c(\"genotype\", \"stimulation\") := tstrsplit(treatment,\n                                                  \" \",\n                                                  fixed = TRUE)]\n\ngenotype_levels &lt;- c(\"WT\", \"KO\")\nstimulation_levels &lt;- c(\"Rest\", \"Active\")\nexp2j[, genotype := factor(genotype,\n                           levels = genotype_levels)]\nexp2j[, stimulation := factor(stimulation,\n                              levels = stimulation_levels)]\n# View(exp2j)\n\n\n\n13.12.2 Import exp3e lesian area data (Example 2)\n\ndata_from &lt;- \"XX sex chromosome complement promotes atherosclerosis in mice\"\nfile_name &lt;- \"41467_2019_10462_MOESM6_ESM.xlsx\"\nfile_path &lt;- here(data_folder, data_from, file_name)\n\n# fig 3e\nexp3e_wide &lt;- read_excel(file_path,\n                  sheet = \"Figure 3E\",\n                  range = \"A3:K4\",\n                  col_names = FALSE) |&gt;\n  data.table() |&gt;\n  transpose(make.names = 1)\n\nsex_levels &lt;- colnames(exp3e_wide)\nexp3e &lt;- melt(exp3e_wide,\n              measure.vars = sex_levels,\n              variable.name = \"sex\",\n              value.name = \"lesian_area\")\n\n# convert lesian_area to mm^2 from µm^2\nexp3e[, lesian_area := lesian_area/10^6]\n\n# convert sex variable to factor\nexp3e[, sex := factor(sex,\n                        levels = sex_levels)]\n\n# create chromosome column and convert to factor\nchromosome_levels &lt;- c(\"XX\", \"XY\")\nexp3e[, chromosome := rep(rep(chromosome_levels, each = 5), 2)]\nexp3e[, chromosome := factor(chromosome,\n                             levels = chromosome_levels)]\n\n# researchers treatment levels\nexp3e[, treatment := rep(c(\"FXX\", \"FXY\", \"MXX\", \"MXY\"), each = 5)]\n\n# two rows with missing response so delte these rows as there is \n# no useful information in them\nexp3e &lt;- na.omit(exp3e) # be careful with a global na.omit\n\n# View(exp3e) # highlight and run to see data\n\n\n\n13.12.3 Import Exp1c JA data (Example 3)\n\ndata_from &lt;- \"Integration of two herbivore-induced plant volatiles results in synergistic effects on plant defense and resistance\"\nfile_name &lt;- \"Data for Dryad.xlsx\"\nfile_path &lt;- here(data_folder, data_from, file_name)\n\nexp1 &lt;- read_excel(file_path,\n                     sheet = \"Fig. 1\",\n                     range = \"A3:K23\", # 1 blank column\n                     col_names = TRUE) |&gt;\n  data.table() |&gt;\n  clean_names()\n\nsetnames(exp1, old = \"treat\", new = \"treatment\")\n\nexp1[treatment == \"Control\", c(\"hac\", \"indole\") := list(\"HAC-\", \"Indole-\")]\nexp1[treatment == \"HAC\", c(\"hac\", \"indole\") := list(\"HAC+\", \"Indole-\")]\nexp1[treatment == \"Indole\", c(\"hac\", \"indole\") := list(\"HAC-\", \"Indole+\")]\nexp1[treatment == \"HAC+Indole\", c(\"hac\", \"indole\") := list(\"HAC+\", \"Indole+\")]\n\ntreatment_levels &lt;- c(\"Control\", \"HAC\", \"Indole\", \"HAC+Indole\")\nexp1[, treatment := factor(treatment,\n                           levels = treatment_levels)]\nexp1[, hac := factor(hac)] # levels in correct order\nexp1[, indole := factor(indole)] # levels in correct order\n# View(exp1)",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Linear models with two categorical $X$ -- Factorial linear models (\"two-way ANOVA\")</span>"
    ]
  },
  {
    "objectID": "chapters/lmm.html",
    "href": "chapters/lmm.html",
    "title": "14  Models for non-independence – linear mixed models",
    "section": "",
    "text": "14.1 Liberal inference from pseudoreplication\nResearchers are interested in regulation and repair of DNA double-stranded breaks and use a proximity ligation assay (PLA) of HeLa cells to investigate the number of damage response events (“foci”) per cell with and without an inhibitor of transcription elongation (DRB). The number of foci in each of fifty cells per treatment is measured. The experiment is replicated three times. The researchers use a t-test to compare the effect of DRB on foci count and naively include all measures in the analysis.\nWhat is naive about the analysis? The fifty measures per cell are technical replicates and the values within a cell are not independent of each other because they share aspects of cell environment not shared by values in other cells. Including technical replicates in an analysis without accounting for this non-independence is a kind of pseudoreplication\nTo show how this naive analysis results in extremely liberal inference and an increase in false discovery, I simulate this experiment using a case in which there is no effect of DRB treatment, so a low p-value indicates a false-discovery. The simulation is simplified with the two following conditions: 1) the model pretends that count data are normally distributed (this is because we want to focus on pseudoreplication and not a misspecified distribution) and 2) the model pretends that values from each treatment within an experiment are independent (this is because we want to focus on pseudoreplication).\nIn this naive analysis of the experiment, the researcher finds an effect of treatment with a p-value of 0.000073 and uses this small p-value to justify a decision to move forward with follow-up experiments. But this low p-value is not supported by the data – this discovery is false. This very small p-value is not an example of a “rare event”. In fact, if the researcher repeats the experiment 1000 times, then the median p-value is 0.000195 and 72.3% of the 1000 p-values lead to the same false discovery if 0.05 is used to make the decision to move forward.\nWhile this example is fake, I see this naive analysis a lot: tumor area of multiple tumors per mouse, islet area of multiple islets per mouse, number of vesicles docked to a membrane in multiple cells per mouse, the number of mitochondria in multiple cells per mouse, the number of neurites in multiple neurons in multiple cells per mouse, etc. etc. Indeed, when I’m looking for examples of pseudoreplication to teach, I just look for figures with a bunch of points per treatment – something similar to this plot of the fake data experiment. Regardless, this is a huge source of false discovery that could disappear overnight.\nFigure 14.1: A fake example of pseudoreplication motivated by a real experiment.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Models for non-independence -- linear mixed models</span>"
    ]
  },
  {
    "objectID": "chapters/lmm.html#conservative-inference-from-failure-to-identify-blocks",
    "href": "chapters/lmm.html#conservative-inference-from-failure-to-identify-blocks",
    "title": "14  Models for non-independence – linear mixed models",
    "section": "14.2 Conservative inference from failure to identify blocks",
    "text": "14.2 Conservative inference from failure to identify blocks\nResearchers are investigating the regulation of the differentiation of stem blood cells into osteoclasts, which can cause osteoporosis if overactivated. The researchers randomly sample three mice from 6 litters and use a littermate control design: within a litter, one sib is assigned to control (CN), one to glucortacoid (GC) treatment, and one to buthionine sulphoximine (BSO) treatment. The researchers are investigating the mechanism of GC-induced osteoporosis and if the researcher’s model is correct, then BSO should block this mechanism. The response variable is Bone Mineral Density. Following an ANOVA, the researchers report the unadjusted p-value for each pairwise comparison with the expectation that BSO will reverse the effect of GC on Bone Mineral Density.\n\n\n\n\n\n\n\n\nFigure 14.2: Results of the fake experiment. A) results using ANOVA using on initial six litters, B) published results with four additional litters, C) results using LMM on initial six litters\n\n\n\n\n\nWhat is naive about the analysis? Any value measured from sibling mice within a litter are not independent of each other because the sibling mice share aspects of genetics and maternal environment not shared by mice in other litters. This shared variance adds correlated noise to data and failure to account for the shared variance will almost always lead to more conservative inference.\nIn the experiment, the results “looked” like the expected results if the model GC-induced osteoporosis were correct, but the p-value of the focal contrast (BSO - GC) was not &lt; 0.05 (Figure 14.2 A). The researchers expanded the experiment, adding data from four more litters (Figure 14.2 B). Had the researchers analyzed the initial set of data using a statistical model that accounts for the shared variance within a litter (Figure 14.2 C), the researchers would have been satisfied and fewer mice would have been killed, fewer resources used, and more time to pursue continued probing of the mechanism. The statistical model used to take advantage of the littermate control design is a linear-mixed model. An experiment with littermate control is known as a blocked design.\n\n\n\n\n\n\nNHST Blues\n\n\n\nNHST encourages peeking at the data to see if p &lt; 0.05 for a focal contrast, and collecting more data if this isn’t the case. Don’t do this. Peeking makes the p-value non-valid and will increase the false discovery rate. Peeking is a variation of the multiple test problem. In clinical trials there are statistically rigorous methods for peeking, which allows a trial to stop early.\n\n\nThe small p-value in the BSO - GC contrast using a linear mixed model is not an example of a “rare event”. In fact, if the researchers repeated the experiment 1000 times, 89.1% of the BSO - GC p-values using the linear mixed model are less than 0.05 while only 29.9% of the p-values following classical ANOVA are less than 0.05.\nWhile this example is fake, I see this naive analysis a lot – littermate controls is very, very common but other examples are too, including replicated experiments (each experiment is a block). There are certain instances where researchers do recognize non-independence and do use a paired t-test but the vast majority of blocked designs (occurring in almost all experimental biology papers) go unrecognized. This is a huge source of failed discovery that could disappear overnight.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Models for non-independence -- linear mixed models</span>"
    ]
  },
  {
    "objectID": "chapters/lmm.html#introduction-to-models-for-non-independent-data-linear-mixed-models",
    "href": "chapters/lmm.html#introduction-to-models-for-non-independent-data-linear-mixed-models",
    "title": "14  Models for non-independence – linear mixed models",
    "section": "14.3 Introduction to models for non-independent data (linear mixed models)",
    "text": "14.3 Introduction to models for non-independent data (linear mixed models)\nThis chapter is about models for correlated error, including linear models with added random factors, which are known as linear mixed models. In classical hypothesis testing, a paired t-test, repeated measures ANOVA, and mixed-effect ANOVA are equivalent to specific cases of linear mixed models. Linear mixed models are used for analyzing data composed of subsets – or batches – of data that were measured from the “same thing”, such as multiple measures within a mouse, or multiple mice within a litter. Batched data results in correlated error, which violates a key assumption of linear models (and their “which test” equivalents) and muddles statistical inference unless the correlated error is modeled, explicitly or implicitly. In some experimental designs (blocked designs), failure to model the correlated error reduces precision and power, contributing to reduced rates of discovery or confirmation. In other designs (nested designs), failure to model the correlated error results in falsely high precision and low p-values, leading to increased rates of false discovery. The falsely high precision is due to pseudoreplication. I think it’s fair to infer from the experimental biology literature, that experimental biologists don’t recognize the ubiquitousness of batched data and correlated error. This is probably the biggest issue in inference in the field (far more of an issue than say, a t-test on non-normal data).\nWhat do I mean by “batch” and how can correlated error both increase and decrease false discovery? Consider an experiment to measure pancreatic islet area with the experimental factor \\(\\texttt{treatment}\\) with levels “Cn” and “Tr”, where “Cn” is the administration of saline and “Tr” is the administration of a drug believed to disrupt the system. While it may seem like the data from this experiment should be analyzed using a t-test, the best practice statistical model actually depends on the experimental design. Experimental design matters because different designs introduce different patterns of correlated error due to shared genetics and environment. Recall that inference from a linear model (including t-tests and ANOVA) assumes independence (Chapter 8) – that is, each response value has no relationship to any other value, other than that due to treatment. Lack of independence results in patterns of correlation among the residuals, or correlated error.\nSomething like the first experiment below (Design 1) is the necessary design to use the statistics that have been covered in this book to this point, without extreme violation of the independence assumption. But many (most?) experiments in experimental bench biology do not look like the design in Design 1 below. Instead, many (most?) experiments are variants of Designs 2 and 3, both of which have extreme violations of the independence assumption. Design 2 and its variants result in liberal statistics and increased, false discovery rate but, interestingly, Design 3 and its variants (generally) result in conservative statistics and reduced, true discovery rate.\n\n\n\n\n\n\nFigure 14.3: Design 1: Completely Randomized Design. Each treatment replicate (mouse) is separately housed. There is a single measure of islet area per mouse. Image created with BioRender.com\n\n\n\nDesign 1. The design in Figure 14.3 has a single factor \\(\\texttt{genotype}\\) with two levels (Cn and Tr). Five mice of the same sex, each from a different litter from a unique dam and sire mating, are randomly sampled and assigned to either Cn or Tr. All mice are housed individually (10 cages). The pancreatic tissue from all mice is prepared in a single batch and the area of a single islet is measured from each mouse. The entire experiment is carried out at the same time and each component (tissue preparation, measuring) is carried out by the same person (these could be different people for each component). This is a Completely Randomized Design (CRD). The five replicate mice per treatment are treatment replicates (often called biological replicates in experimental biology). A CRD does not have batched data.\n\n\n\n\n\n\nFigure 14.4: Design 2: Completely Randomized Design with subsampling. Each treatment replicate (mouse) is separately housed. There are ten measures of islet area per mouse. Image created with BioRender.com\n\n\n\nDesign 2. The design in Figure 14.4 is exactly like that in Design 1, except that the researchers take ten measures of iselet area per mouse. The ten measures are subsampled replicates. Experimental biologists often call these technical replicates, especially when the multiple measures are taken from the same preparation. Subsampling is a kind of nested design in which one variable is nested within (as opposed to crossed with) another variable. Here, the subsampled variable (subsample_id) is nested within the mouse_id variable. Each mouse is a batch. Each mouse has a unique set of factors that contribute to the error variance of the measures of the response in that mouse. All response measures within a mouse share the component of the error variance unique to that mouse and, as a consequence, the error (residuals) within a mouse are more similar to each other than they are to the residuals between mice\n\n\n\n\n\n\nFigure 14.5: Design 3: Randomized Complete Block Design. Pairs of treatment replicates (mouse) are siblings and housed together with no other mice. There is a single measure of islet area per mouse. Image created with BioRender.com\n\n\n\nDesign 3. In the design in Figure 14.5, two littermates are randomly sampled from five litters, each with a different dam and sire. Within each litter, one mouse is assigned to Cn and a second is assigned to Tr. Each litter is randomly assigned to cage with only a single litter per cage. All other aspects of this design are as in Design 1. This is a Randomized Complete Block Design. The five replicate mice per treatment are the treatment replicates. Each litter/cage combination is a type of batch called a block. A blocked design typically functions to reduce noise in the model fit (this increases power) and to reduce the number of litters and cages needed for an experiment. The two measures of Islet Area within a litter/cage (one per mouse) are not independent of each other. Each cage has two mice from the same litter and these mice share genetic and maternal factors that contribute to mouse anatomy and physiology that are not shared by mice in other litters. Additionally, each cage has a unique set of environmental factors that contribute to the error variance of the measure of the response. Each cage shares a cage-specific history of temperature, humidity, food, light, interactions with animal facilities staff, and behavioral interactions among the mice. All response measures within a litter/cage share the component of the error variance unique to that litter/cage and, as a consequence, the error (residuals) within a litter/cage are more similar to each other than they are to the residuals among litters/cages.\nIn each of these experiments, there is systematic variation at multiple levels: among treatments due to treatment effects and among batches due to batch effects. Batches come in lots of flavors, including experiment, litter, cage, flask, plate, slide, donor, and individual. The among-treatment differences in means are the fixed effects. The among-batch differences are the random effects. An assumption of modeling random effects is that the batches are a random sample of the batches that could have been sampled. This is often not strictly true as batches are often convenience samples (example: the human donors of the Type 2 diabetes beta cells are those that were in the hospital).\nThe variation among batches/lack of independence within batches has different consequences on the uncertainty of the estimate of a treatment effect. The batches in Experiment 3 contain both treatments (Cn and Tr). The researcher is interested in the treatment effect but not the variation due to differences among the batches. The batches are nuissance factors that add additional variance to the response, with the consequence that estimates of treatment effects are less precise, unless the variance due to the batches is explicitly modeled. Modeling a batch that contains some or all treatment combinations will increase precision and power.\nBatches that contain more than one treatment combination are known as blocks. A block that contains all treatment combinations is a complete block. A block that contains fewer than all combinations is an incomplete block. Including block structure in the design is known as blocking. Blocks are non-experimental factors. Adding a blocking factor to a statistical model is used to increase the precision of an estimated treatment effect. Design 3 is an example of a randomized complete block design.\nIn Design 2, there are multiple measures per mouse and the design is a Completely Randomized Design with subsampling. The subsampling is not the kind of replication that can be used to infer the among treatment effect because the treatment assignment was not at the level of the subsamples. The treatment replicates are the mice, because it was at this level that treatment assignment was randomized. A statistical analysis of all measures from a subsampled design without modeling the correlated error due to the subsampling is a kind of pseudoreplication. Pseudoreplication results in falsely precise standard errors and false small p-values and, consequently, increased rates of false discovery.\nIn all of these designs, it is important for the researcher to identify the experimental unit and the measurement unit. The experimental unit is the entity that was randomly assigned the treatment. In all the designs above, the experimental unit is the mouse. In designs 1 and 3, the measurement unit is the mouse. In design 2, the measurement unit is the specific islet that was measured.\n\n\n\n\n\n\nPseudoreplication\n\n\n\nIn pseudoreplication, the degrees of freedom used to compute the test statistic and the p-value are inflated given the experimental design and research question. An example: A researcher wants to investigate the effect of some protein on mitochondrial biogenesis and designs an experiment with a wildtype (WT) and a conditional knockout (KO) mouse. Mitochondrial counts from twenty cells in one WT mouse and one KO mouse are measured and the researcher uses a t-test to compare counts. The sample size used to compute the standard error in the denominator of the t-value is 20. The t-distribution used to compute the p-value uses 38 df (20 measures times two groups minus two estimated parameters). This is wrong. The df are inflated and the estimate of the standard error of the difference (the denominator of the t-value) is falsely small. The correct sample size for this design is 1 and the correct df is zero. The sample size and df are inflated for this design because the treatment was randomized to mouse and not to cell. Mouse is the experimental unit – the number of experimental units is what gives the degrees of freedom. The df are correct for inference about the two individuals (how compatible are the data and a model of sampling from the same individual?), but not for inference about the effect of genotype. We cannot infer anything about genotype with a sample size of 1, even with 20 measures per mouse, because any effect of treatment is completely confounded with other differences between the two mice.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Models for non-independence -- linear mixed models</span>"
    ]
  },
  {
    "objectID": "chapters/lmm.html#experimental-designs-in-experimental-bench-biology",
    "href": "chapters/lmm.html#experimental-designs-in-experimental-bench-biology",
    "title": "14  Models for non-independence – linear mixed models",
    "section": "14.4 Experimental designs in experimental bench biology",
    "text": "14.4 Experimental designs in experimental bench biology\nGiven the basic principles above, let’s consider the kinds of experimental designs seen in experimental bench biology (Figure 14.6). For each of the experimental designs, I give examples with a certain number of treatments (t), blocks (b), treatment replications (r), and subsampled replications (s).\nNotation for models\ni = 1..t (treatments)\nj = 1..b (blocks)\nk = 1..r (treatment replications within a block or within a CRD with no block structure)\nm = 1..s (subsamples or technical replicates)\n\n\n\n\n\n\n\nFigure 14.6: Experimental designs in experimental bench biology. Images created with BioRender.com\n\n\n\n\n14.4.1 Completely Randomized Design (CRD)\nThe Completely Randomized Design experiment in Figure 14.6 A has a single factor, \\(\\texttt{treatment}\\) with two levels (“Cn” and “Tr”). Five mice are randomly assigned to each treatment level. Each mouse is bred from a different litter and housed in a separate cage. The researchers measure a single value of the response variable from each mouse. The five replicate mice per treatment are the treatment (biological) replicates. The design is completely randomized because there is no subgrouping due to batches. What kinds of subgrouping does this design avoid?\nBy using a single mouse per litter, there are no litter batches and subsets of mice don’t share litter effects – common litter responses to the Cn or Tr treatments. Each litter has a unique set of factors that contribute to the error variance of the measure of the response. Siblings from the same dam and sire share more genetic variation than non-siblings and this shared genetic variation contributes to phenotypes (including the response to treatment) that are more likely to be similar to each other than to non-siblings. Siblings from the same litter share the same history of maternal factors (maternal effects, including epigenetic effects) specific to the pregnancy and even the history of events leading up to the pregnancy. This shared non-genetic and epigenetic variation contributes to phenotypes (including the response to treatment) that are more likely to be similar to each other than to non-siblings. All response measures within a litter share the genetic, maternal environmental, and epigenetic components of the error variance unique to that litter and, as a consequence, the error (residuals) within a litter are more similar to each other than they are to the residuals between litters.\nBy housing each mouse in it’s own cage, there is no cage batch and subsets of mice don’t share cage effects – common cage responses to the Cn or Tr treatments. As stated earlier, each cage has a unique set of factors that contribute to the error variance of the measure of the response. Each cage shares a cage-specific history of temperature, humidity, food, light, interactions with animal facilities staff and behavioral interactions among the mice. All response measures within a cage share the component of the error variance unique to that cage and, as a consequence, the error (residuals) within a cage are more similar to each other than they are to the residuals between cages.\nExamples:\n\nTen mice from separate litters are sampled. Five mice are randomly assigned to control. Five mice are randomly assigned to treatment. A single measure per mouse is taken. Mouse is the experimental unit. \\(t=2\\), \\(b=0\\), \\(r=5\\), and \\(s=1\\).\nTen cell cultures are created. Five cultures are randomly assigned to control and five to treatment. A single measure per culture is taken. Culture is the experimental unit. \\(t=2\\), \\(b=0\\), \\(r=5\\), and \\(s=1\\).\n\n\n\n\n\n\n\nKnown Unknowns\n\n\n\nWhile most data from experiments in bench biology are analyzed as if the experimental design is a CRD, a good question is, what fraction of these actually are CRD? We know that many (most?) mouse experiments will have batched measures and correlated responses because most experiments are conducted with multiple mice per litter and/or cage (or equivalents in other model systems). Many cell culture data come from multiple replicates of the whole experiment – the experiment functions as a block. And time series experiments include multiple measures on the same experimental unit over time. Except for time series experiments, researchers in experimental bench biology are using almost exclusively statistical tests that assume independence of errors (the tests appropriate for CRDs). How this mismatch between experimental design and statistical practice affects the rate of false and true discovery in cell and molecular biology is a known unknown.\n\n\n\n\n14.4.2 Completely Randomized Design with Subsampling (CRDS) or “Nested Design”\nThe Completely Randomized Design with Subsampling experiment in Figure 14.6 B is exactly like the CRD except that the researchers measure multiple values of the response variable from each mouse under the same condition (that is, not different in treatment or time). The multiple measures are subsampled (technical) replicates.\n\nDo not confuse subsampled replicates with measures of the response under different conditions in the same mouse, for example a measure from one brain slice under the control treatment and a measure from a second brain slice under the drug treatment. This example is a kind of Randomized Complete Block Design, which is outlined next and the core design in this chapter.\nDo not confuse subsampled replicates with a measures of the response at different times in the same mouse, for example, the plasma glucose levels at baseline and at five post-baseline time points. This example is a kind of longitudinal design, which is outlined below and more thoroughly in the chapter Linear models for longitudinal experiments.\nDo not confuse subsampled replicates with measures of different response variables from the same mouse, for example measures of the weights of five different skeletal muscles. This example is a kind of multiple response which is addressed xxx.\n\nThe technical replicates are a kind of pseudoreplication. The general linear model y ~ treatment fit to these data, including t-tests and traditional ANOVA, will have falsely high precision and falsely low p-values.\nExamples:\n\nTen mice from separate litters are sampled. Five mice are randomly assigned to control. Five mice are randomly assigned to treatment. Multiple measures per mouse are taken. Example: five measures of Islet Area are measured in each pancreas. Mouse is the experimental unit. Each of the measures is a technical replicate because the treatment is not randomly assigned to each islet but to the whole mouse. \\(t=2\\), \\(b=0\\), \\(r=5\\), and \\(s=5\\).\nTen cell cultures are created. Five cultures are randomly assigned to control and five to treatment. Multiple measures per culture are taken. Example: Mitochondrial counts are measured from five cells in each culture. Culture is the experimental unit. Each of the counts is a technical replicate because the treatment is not randomly assigned to each cell but to the whole culture. \\(t=2\\), \\(b=0\\), \\(r=5\\), and \\(s=5\\).\n\nNotes:\n\nSubsampling can occur at multiple levels. Example: Ten mice from separate litters are sampled. Five mice are randomly assigned to control. Five mice are randomly assigned to treatment. Five neurons in a slice of brain tissue are identified in each mouse. From each neuron, the length of five dendrite spines are measured. The five measures of spine length are “nested within” neuron and the five neurons are “nested within” mouse. Nested subsampling can quickly lead to massive pseudoreplication and false discovery.\n\n\n\n14.4.3 Randomized Complete Block Design (RCBD)\nThe Randomized Complete Block Design experiment in Figure 14.6 C is similar to the CRD except that all treatment combinations (two here) are randomly assigned to sibling mice within a litter. Here, two mice from each litter are randomly selected and one is randomly assigned to “Cn” and the other to “Tr”. Each litter is randomly assigned to a unique cage. The researchers measure a single value of the response variable from each mouse. The five replicate mice per treatment are the treatment replicates. The litters (or cage) are the blocks. In this design, litter and cage effects are confounded but this has no consequence on the statistical model and inference unless the researchers want to explicitly estimate these effects separately. Compared to the CRD, this design requires fewer resources (five litters instead of ten, five cages instead of ten). Compared to the general linear model fit to data from the CRD (y ~ treatment), including t-tests and traditional ANOVA, the linear mixed model fit to the RCBD has increased precision and power. While many researchers seem to be designing experiments similar to this (“littermate controls”), most are failing to fit a statistical model that accounts for the batching and taking advantage of the increased precision and power.\nExamples:\n\nTen mice are sampled. In each mouse, one forelimb is assigned to control and the other forelimb is assigned to treatment. Only a single measure on each side is taken. Limb is the experimental unit. Mouse is a block. \\(t=2\\), \\(b=10\\), \\(r=1\\), and \\(s=1\\).\nTen litters are sampled. In each litter, one sib is assigned to control and the other sib is assigned to treatment. Only a single measure on each sib is taken. Mouse is the experimental unit. Litter is a block. \\(t=2\\), \\(b=10\\), \\(r=1\\), and \\(s=1\\).\nTwo mice, each from a separate litter are sampled. One is randomly assigned to control and the other to treatment. Only a single measure of the response variable is taken per mouse. The experiment is replicated five times (five different days, each with a newly made set of reagents and machine calibrations). Mouse is the experimental unit. Experiment is a block. \\(t=2\\), \\(b=5\\), \\(r=1\\), and \\(s=1\\).\n\n\n\n14.4.4 Randomized Split Plot Design (RSPD)\nThe Randomized Split Plot Design experiment in Figure 14.6 D is similar to the RCBD except that there is now a second experimental factor that is crossed with the first experimental factor. An individual mouse acts as single experimental unit for one factor (here, \\(\\texttt{genotype}\\) with levels “WT” and “KO”) but acts as a block for the second experimental factor (here, \\(\\texttt{treatment}\\) with levels “Cn” and “Tr”). The first factor (\\(\\texttt{genotype}\\)) is the main plot – the levels of the factor are randomly assigned to the main plots. The second factor (\\(\\texttt{treatment}\\)) is the subplot – the levels of this factor are randomly assigned to the subplots. \\(\\texttt{Litter}\\) is a replicated block. Also in Figure 14.6 D, a 2 x 2 RCBD with the same two experimental factors is shown for comparison. In the 2 x 2 RCBD, four mice per block (litter) are each randomly assigned one of the 2 x 2 combinations of \\(\\texttt{genotype}\\) and \\(\\texttt{treatment}\\)).\n\n\n14.4.5 Generalized Randomized Complete Block Design (GRCBD)\nThe Generalized Randomized Complete Block Design experiment in Figure 14.6 E is similar to the RCBD except that two treatment replicates per block (litter/cage) are assigned.\nImportant and somewhat not intuitive Because the treatment replicates within a litter share common error variance, these do not act like independent replicates. One consequence of this is, the sample size (\\(n\\)) is five and not ten (\\(litters \\times treatment replicates\\)). The general linear model y ~ treatment fit to these data, including t-tests and traditional ANOVA, will generally have falsely high precision and falsely low p-values.\n\n\n14.4.6 Nested Randomized Complete Block Design (NRCBD)\nThe Nested Randomized Complete Block Design experiment in Figure 14.6 F is similar to the RCBD except that there are now replicated experiments. This is a nested block design with litter (a block) nested within experiment (a block).\n\n\n14.4.7 Longitudinal Randomized Complete Block Design (LRCBD)\nThe Longitudinal Randomized Complete Block Design experiment in Figure 14.6 G is similar to the RCBD except that there are multiple measures of the response variable taken, each taken at a different time point, including baseline (time zero).\n\n\n14.4.8 Variations due to multiple measures of the response variable\nSimilar to the CRDS above, the other basic designs can include subsampling, resulting in, for example, RCBDS or RSPDS. If there is subsampling within subsampled units, then we can designated these with “SS”, for example RCBDSS.\nSimilar to the LRCBD above, the other basic designs can include longitudinal sampling, resulting in, for example, LCRD or LRSPD.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Models for non-independence -- linear mixed models</span>"
    ]
  },
  {
    "objectID": "chapters/lmm.html#building-the-linear-mixed-model-for-clustered-data",
    "href": "chapters/lmm.html#building-the-linear-mixed-model-for-clustered-data",
    "title": "14  Models for non-independence – linear mixed models",
    "section": "14.5 Building the linear (mixed) model for clustered data",
    "text": "14.5 Building the linear (mixed) model for clustered data\nNotation\n\ni = 1..t (treatments)\nj = 1..b (blocks)\nk = 1..r (experimental replications within a block)\nm = 1..s (subsamples or technical replicates)\n\n\\[y_i = \\beta_0 + \\beta_i treatment_i + (\\gamma_j block_j) + (\\gamma_{ij} block_j treatment_i) + \\varepsilon\\]",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Models for non-independence -- linear mixed models</span>"
    ]
  },
  {
    "objectID": "chapters/lmm.html#statistical-models-for-experimental-designs",
    "href": "chapters/lmm.html#statistical-models-for-experimental-designs",
    "title": "14  Models for non-independence – linear mixed models",
    "section": "14.6 Statistical models for experimental designs",
    "text": "14.6 Statistical models for experimental designs\n\n14.6.1 Models for Completely Randomized Designs (CRD)\n\nlm0 &lt;- lm(y ~ treatment,\n             data = figx)\n\n\n\n14.6.2 Models for batched data (CRDS, RCBD, RSPD, GRCBD, NRCBD)\n\n14.6.2.1 Linear models\n\n14.6.2.1.1 fixed effect model\n\nlm1 &lt;- lm(y ~ treatment + block,\n             data = figx)\n\n\n\n14.6.2.1.2 linear model of batch means\n\nfigx_means &lt;- figx[, .(y = mean(y)),\n               by = .(treatment, batch)]\nlm2 &lt;- lm(y ~ treatment,\n             data = figx_means)\n\n\n\n\n14.6.2.2 linear mixed models (“random effects” models)\n\n14.6.2.2.1 Random intercept model\n\nlmm1 &lt;- lmer(y ~ treatment + (1 | block),\n              data = figx)\n\n\n\n14.6.2.2.2 lmm for correlated error\n\nlmm2 &lt;- lme(y ~ treatment,\n             random = ~1 | block,\n             correlation = corSymm(form = ~ 1 | block),\n             weights = varIdent(form = ~ 1 | treatment),\n             data = figx)\n\n\n\n14.6.2.2.3 random intercept and slope model\n\nlmm3 &lt;-  lmer(y ~ treatment + (treatment | block),\n               data = figx)\n\n\n\n14.6.2.2.4 random interaction intercept model\n\nlmm4 &lt;-  lmer(y ~ treatment + (1 | block) + (1 | block:treatment),\n               data = figx)\n\n\n\n14.6.2.2.5 random interaction model with subsampling\n\nlmm5 &lt;-  lmer(y ~ treatment + (1 | block) + (1 | block:treatment) + (1 | block:treatment:replicate),\n               data = figx)\n\n\n\n14.6.2.2.6 CRD split plot model\n\n# tr1 is the main plot\n# tr2 is the subplot\n# main_plot is tr1:rep, where rep is the rep id\nlmm6 &lt;-  lmer(y ~ tr1 * tr2 + (1 | main_plot),\n               data = figx)\n\n\n\n14.6.2.2.7 RCBD split plot model\n\n# tr1 is the main plot\n# tr2 is the subplot\n# main_plot is tr1:block\nlmm7 &lt;-  lmer(y ~ tr1 * tr2 + (1 | block) + (1 | block:tr1),\n               data = figx)\n\nlmm7 &lt;-  lmer(y ~ tr1 * tr2 + block + (1 | block:tr1),\n               data = figx)\n# the two versions are numerically equivalent\n\n\n\n\n14.6.2.3 ANOVA models (“mixed models”, “repeated measures ANOVA”)\n\n14.6.2.3.1 Multivariate repeated measures ANOVA\n\naov1 &lt;- aov_4(y ~ treatment + (treatment | block),\n             data = figx)\n\n\n\n14.6.2.3.2 Univariate repeated measures ANOVA\n\naov2 &lt;- aov_4(y ~ treatment + (treatment | block),\n           include_aov = TRUE,\n             data = figx)\n\n\n\n14.6.2.3.3 Pairwise paired, t-tests\n\npptt &lt;- pairwise_t_tests(y_col = \"y\",\n                         g_col = \"treatment\",\n                         id_col = \"block\",\n                         data = figx)",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Models for non-independence -- linear mixed models</span>"
    ]
  },
  {
    "objectID": "chapters/lmm.html#which-model-and-why",
    "href": "chapters/lmm.html#which-model-and-why",
    "title": "14  Models for non-independence – linear mixed models",
    "section": "14.7 Which model and why?",
    "text": "14.7 Which model and why?\n\n14.7.1 CRD\n\nNo batch to model!\n\n\n\n14.7.2 CRDS\n\nlmm1 – random intercept model\nlm2 – linear model of batch means\n\nNotes\n\nThese are numerically equivalent and should result in same estimates, SE, CIs, and p-values\nThese are equivalent to a Nested t-test or Nested ANOVA\n\n\n\n14.7.3 RCBD\nReasonable models:\n\nlmm1 – random intercept model\nlmm2 – lmm for correlated error\nlm1 – fixed effect model\naov1 – multivariate RM-ANOVA\naov2 – univariate RM-ANOVA\npptt – pairwise, paired t-test\n\n(Cannot use lmm3 or lmm4 because there is no replication of each block:treatment combination.)\nAssumptions:\n\nlmm1, lm1, and aov2 assume\n\ncompound symmetry.\nsphericity.\n\nlmm2, aov1, and pptt do not assume compound symmetry and sphericity.\n\nNotes:\n\nlmm1 is the standard. If the number of treatments = 2, then lmm1 is eqivalent with a paired t-test.\nIf the design is balanced AND the number of treatments = 2 then all six methods are numerically equivalent.\nIf the design is balanced AND the number of treatments &gt; 2, then\n\nlmm1, lm1, and aov2 are numerically equivalent\nlmm2, aov1, pptt result in the same estimates and SE but lmm2 has more df, so the CIs, and p-values of lmm2 are less conservative.\n\nIf the number of treatments = 2 AND the design is not balanced (at least one block is missing value for one treatment level) then the linear models (lm1, aov1, aov2, pptt) are equivalent but the linear mixed models (lmm1, lmm2) differ from each other and from the linear models.\nIf a treatment within a block is missing, the whole block is deleted in the RM-ANOVA models. This reduces power (the loss of power depends partly on the number of missing values).\nIf a treatment within a block is missing, the block is deleted only in the comparisons including the missing treatment in the pairwise, paired t-tests. This makes pptt more powerful than aov1.\nthe lmm models are very flexible – covariates can be added or these can be used as generalized lmms for non-normal distributions.\nthe lmm models and especially lmm2 sometimes (often?) fail to converge, especially with small samples (small number of blocks) or if the among-block component of variance is small.\n\n\n14.7.3.1 So, which model, and why? A simulation to help decide\nTwo linear mixed modeling strategies would be\n\nFit the most complete model first, if the model fails or has boundary warning because of too little variance at some level, then fit the next most complete model, and so on. This order would be lmm2, lmm1, lm1 (lmm1 and lm1 are equivalent if the design is balanced).\nFit all linear mixed models (lmm2, lmm1) and use a model selection method to choose the best model that balances completeness (controling type I error) and overfitting.\n\nI suggest a third strategy, which I prefer. This is,\n\nuse simulation to compare the performance (Type I error and Power) of each model under differ combinations of the structure of the variances of the levels of the random factor. Use these results to guide the strategy\n\nHere I use a simulation to measure the performance of the RCBD statistical models under four conditions of correlated error among the blocks between two treatment levels.\n\nHigh correlated error. There is heterogenous correlated error among the three pairs of treatments, with one high, one intermediate, and one low correlation\nLow correlated error. There is heterogenous correlated error among the three pairs of treatments, all with low correlation.\nHomogenous correlated error. There is high correlated errror among the three pairs of treatment, all with the same expected level\nThere is no correlated error.\n\n\n\n\n\nTable 14.1: Type I error rate the RCBD statistical models under different levels of correlation structure among the observations. The Cor Error column is the average correlated error of the residuals fit by a simple linear model y ~ treatment. Model abbreviations as in text.\n\n\n\n\n\n\nContrast\nCor Error\nlm0\nlm1\naov1\naov2\npptt\nlmm2\n\n\n\n\nsim 1 - high correlated error\n\n\nTr1 - Cn\n0.300\n0.044\n0.106\n0.057\n0.106\n0.057\n0.0640\n\n\nTr2 - Cn\n0.416\n0.024\n0.082\n0.048\n0.082\n0.048\n0.0635\n\n\nTr2 - Tr1\n0.662\n0.000\n0.004\n0.050\n0.004\n0.050\n0.0600\n\n\nsim 2 - low correlated error\n\n\nTr1 - Cn\n0.161\n0.048\n0.070\n0.054\n0.070\n0.054\n0.0645\n\n\nTr2 - Cn\n0.178\n0.048\n0.072\n0.055\n0.072\n0.055\n0.0705\n\n\nTr2 - Tr1\n0.230\n0.014\n0.025\n0.052\n0.025\n0.052\n0.0705\n\n\nsim 3 - equal correlated error\n\n\nTr1 - Cn\n0.771\n0.002\n0.052\n0.052\n0.052\n0.052\n0.0635\n\n\nTr2 - Cn\n0.774\n0.001\n0.050\n0.052\n0.050\n0.052\n0.0685\n\n\nTr2 - Tr1\n0.771\n0.002\n0.048\n0.048\n0.048\n0.048\n0.0620\n\n\nsim 4 - zero correlated error\n\n\nTr1 - Cn\n0.011\n0.046\n0.043\n0.040\n0.043\n0.040\n0.0560\n\n\nTr2 - Cn\n0.016\n0.049\n0.049\n0.046\n0.049\n0.046\n0.0580\n\n\nTr2 - Tr1\n-0.005\n0.052\n0.050\n0.051\n0.050\n0.051\n0.0710\n\n\n\n\n\n\n\n\n\n\nNotes\n\nWhen there is zero correlated error, all methods have reasonable type I error control. Perhaps lmm2 is very slightly anticonservative (one of the error rates is 0.07).\nWhen there is equal correlated error, all methods have reasonable type I error control (lmm2 is a very slightly anti-conservative).\nWhen there is heterogenous correlated error, only aov1/pptt/lmm2 have reasonable type I error control (again, lmm2 is very slightly anticonservative). lmm1/aov1 have liberal Type I control in contrasts with low correlated error and conservative Type I control in contrasts with high correlated error.\n\nCombined, these results suggest that aov1/pptt should be our preferred model, if we only take Type I control into consideration.\n\n\n\n\nTable 14.2: Power of the RCBD statistical models under different levels of correlation structure among the observations. The Cor Error column is the average correlated error of the residuals fit by a simple linear model y ~ treatment. Model abbreviations as in text.\n\n\n\n\n\n\nContrast\nCor Error\nlm0\nlm1\naov1\naov2\npptt\nlmm2\n\n\n\n\nsim 1 - high correlated error\n\n\nTr1 - Cn\n0.300\n0.044\n0.106\n0.057\n0.106\n0.057\n0.0645\n\n\nTr2 - Cn\n0.416\n0.352\n0.501\n0.396\n0.501\n0.396\n0.4500\n\n\nTr2 - Tr1\n0.662\n0.256\n0.516\n0.904\n0.516\n0.904\n0.9315\n\n\nsim 2 - low correlated error\n\n\nTr1 - Cn\n0.161\n0.048\n0.070\n0.054\n0.070\n0.054\n0.0645\n\n\nTr2 - Cn\n0.178\n0.680\n0.738\n0.666\n0.738\n0.666\n0.7155\n\n\nTr2 - Tr1\n0.230\n0.720\n0.782\n0.812\n0.782\n0.812\n0.8580\n\n\nsim 3 - equal correlated error\n\n\nTr1 - Cn\n0.771\n0.002\n0.052\n0.052\n0.052\n0.052\n0.0635\n\n\nTr2 - Cn\n0.774\n0.262\n0.918\n0.880\n0.918\n0.880\n0.9095\n\n\nTr2 - Tr1\n0.771\n0.269\n0.923\n0.885\n0.923\n0.885\n0.9190\n\n\nsim 4 - zero correlated error\n\n\nTr1 - Cn\n0.011\n0.046\n0.043\n0.040\n0.043\n0.040\n0.0560\n\n\nTr2 - Cn\n0.016\n0.264\n0.252\n0.238\n0.252\n0.238\n0.2850\n\n\nTr2 - Tr1\n-0.005\n0.278\n0.264\n0.230\n0.264\n0.230\n0.2765\n\n\n\n\n\n\n\n\n\n\nNotes\n\nWhen there is zero correlated error, lmm2 has slightly higher power than lmm1/aov2/lm1. And lmm1/aov2/lm1 have slightly higher power than aov1/pptt.\nWhen there is equal correlated error, lmm2 and lmm1/aov2/lm1 have slightly higher power than aov1/pptt.\nWhen there is heterogenous correlated error, lmm1/aov2/lmm1 have non-trivially higher power than aov1/pptt in the contrast with the lower correlated error is but non-trivially lower power in the contrast with the higher correlated error. lmm2 has relatively high power regardless of the correlated error of the contrast.\n\nThese results suggest lmm2 is the preferred model. If lmm2 fails, then it’s a toss-up between aov1/pptt and lmm1/aov2/lmm1.\nBut, combining the Type I and power simulations, lmm1/aov2/lmm1 have poor type I error control under some conditions while lmm2 and aov1/pptt have good error control under all conditions.\n\n\n14.7.3.2 So, which model, and why? A simulator’s guide to best practices\n\nIf the number of treatments = 2 and\n\nthe design is balanced: it doesn’t matter which method you use.\nthe design is not balanced: use lmm1, since this will use all of the data\n\nIf the number of treatments &gt; 2\n\nuse lmm2. If the model fit fails then\nif the design is balanced: use aov1/pptt.\nthe design is not balanced: pptt is attractive if there are few missing values.\n\nIf there are covariates, then start with lmm2. Use lmeControl if lmm2 fails. If this fails, use pairwise lmm1, which is the same as pairwise t-tests but allows covariates to be added to the model. Note that the covariate will be modeled independently in each pair, which effectively models the consequence of a treatment:covariate interaction.\n\n\n\n\n14.7.4 RSPD\nReasonable models:\n\nlmm4 – random interaction intercept model\n\nAssumptions:\n\n\n\nNotes:\n\n\n\nPerformance:\n\n\n\nType I error rate for the RSPD statistical models under different models of random variance. The design is 2 (main plot: WT, KO) x 3 (subplot: Cn, Tr1, Tr2) with ten blocks. All four simulations have a component of variance due to block. Sim 1 includes a block:main plot component, Sim 2 includes a block:sub plot component, Sim 3 includes both block:main and block:sub components. Type I error rates for the 9 simple effect contrasts were averaged within the two sets: contrast = main is the aggregate of the single main plot contrast (KO - WT) at each level of the subplot factor. contrast = sub is the aggregate of the three subplot contrasts (Tr1 - Cn, Tr2 - Cn, Tr2 - Tr1) at each level of the main plot factor. The Cor Error column is the average correlated error of the residuals fit by a simple linear model y ~ treatment. Model abbreviations as in text.\n\n\nsim_id\nsig_block\nsig_ss\nsig_main\nsig_sub\ncontrast\nr\nlm0\nlm1\nlmm1\nlmm3\nlmm4\nlmm5\nlmm6\n\n\n\n\nsim 1 - block:main plot\n\n\n1\n0.8\n0.5\n0.6\n0.0\nmain\n0.494\n0.010\n0.199\n0.087\n0.051\n0.052\n0.083\n0.054\n\n\n1\n0.8\n0.5\n0.6\n0.0\nsub\n0.783\n0.000\n0.052\n0.011\n0.052\n0.052\n0.011\n0.052\n\n\nsim 2 - block:sub plot\n\n\n2\n0.8\n0.5\n0.0\n0.6\nmain\n0.782\n0.000\n0.002\n0.003\n0.002\n0.003\n0.042\n0.041\n\n\n2\n0.8\n0.5\n0.0\n0.6\nsub\n0.497\n0.008\n0.053\n0.066\n0.068\n0.065\n0.050\n0.053\n\n\nsim 3 - block:main + block:sub\n\n\n3\n0.8\n0.5\n0.6\n0.4\nmain\n0.549\n0.007\n0.111\n0.062\n0.038\n0.039\n0.068\n0.054\n\n\n3\n0.8\n0.5\n0.6\n0.4\nsub\n0.689\n0.001\n0.056\n0.026\n0.057\n0.056\n0.024\n0.054\n\n\nsim 4 - block only\n\n\n4\n0.8\n0.5\n0.0\n0.0\nmain\n0.707\n0.002\n0.049\n0.049\n0.044\n0.045\n0.056\n0.054\n\n\n4\n0.8\n0.5\n0.0\n0.0\nsub\n0.704\n0.002\n0.057\n0.055\n0.061\n0.060\n0.052\n0.059\n\n\n\n\n\n\n\n\n\n14.7.5 RCBDS\n\nlmm3 – random intercept and slopes model\nlmm4 – random interaction intercept model\nlmm1 – random intercept model on batch means\nlmm2 – lmm for correlated error on batch means\naov1 – multivariate RM-ANOVA\naov2 – univariate RM-ANOVA\npptt – pairwise, paired t-test\n\nNotes\n\nlmm3 is the “maximal model” – it fits the most parameters and has the fewest assumptions. lmm1 on the full data is the minimal model. I’m not recommending this at all as this is pseudoreplication and will result in very anti-conservative inference. lmm4 is less conservative than lmm3 (and makes more assumptions).\nlmm1 and lmm2 here are the models for RCBD designs but using the aggregated data - that is the subsampled replicates within a batch (block:treatment combination) are averaged.\nIf design is balanced\n\nlmm3 and aov1 result in same estimates, SE, CIs, and p-values\nlmm4, lmm1, and aov2 result in same estimates, SE, CIs, and p-values\nlmm2 has same estimates and SE as lmm3/aov1 but more df so less conservative\n\nIf a treatment within a block is missing, the whole block is deleted in the RM-ANOVA models. This reduces power\nthe lmm models are very flexible – covariates can be added or these can be used as generalized lmms for non-normal distributions.\nthe lmm models, and especially lmm2 and lmm3, sometimes (often?) fail to converge, especially with small samples (small number of blocks) or if within block correlation is small\nlmm1, lmm4, and aov2 will be less conservative (more power but at higher type I error/false positive rate)\n\nBest practices\n\nIf balanced and no covariates then use lmm3/lmm2/aov1 if discovery is expensive (want to avoid false positives) or lmm4/aov2/lmm1 if discovery is cheap (can afford false positives). lmm2 is less conservative than lmm3/aov1. Use lmm3 if interested in the variance at different levels and/or covariance structure.\nIf unbalanced, or if there are covariates, then start with lmm3 or lmm2. Use lmeControl if lmm2 fails. If these fail, go to 1.\n\n\n\n14.7.6 GRCBD\n\nlmm3 – random intercept and slopes model\nlmm4 – random interaction intercept model\nlmm1 – random intercept model on block:treatment means\nlmm2 – lmm for correlated error on block:treatment means\naov1 – multivariate RM-ANOVA\naov2 – univariate RM-ANOVA\nlm1 – fixed effect model on all data\npptt – pairwise, paired t-test on block:treatment means\n\n\n\n14.7.7 GRCBDS\n\nlmm3 – random intercept and slopes model\nlmm4 – random interaction intercept model\nlmm1 – random intercept model on block:treatment means\nlmm2 – lmm for correlated error on block:treatment means\naov1 – multivariate RM-ANOVA\naov2 – univariate RM-ANOVA\nlm1 – fixed effect model on all block:treatment:rep means\npptt – pairwise, paired t-test on block:treatment:rep means",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Models for non-independence -- linear mixed models</span>"
    ]
  },
  {
    "objectID": "chapters/lmm.html#example-1-crd-with-subsampling-classical-equivalent-nested-anova",
    "href": "chapters/lmm.html#example-1-crd-with-subsampling-classical-equivalent-nested-anova",
    "title": "14  Models for non-independence – linear mixed models",
    "section": "14.8 Example 1: CRD with subsampling (classical equivalent: Nested ANOVA)",
    "text": "14.8 Example 1: CRD with subsampling (classical equivalent: Nested ANOVA)\nSource article: A new mouse model of Charcot-Marie-Tooth 2J neuropathy replicates human axonopathy and suggest alteration in axo-glia communication\nPublished methods: Nested ANOVA\nFig: 5b\nDesign: 3 x 1 CRDS\n\n\n\nNumber of paranodes measured per mouse. Each measurement is a subsampled replicate. Each mouse is a treatment replicate.\n\n\ngenotype\nmouse\nN\n\n\n\n\nWT\nWT 1\n90\n\n\nWT\nWT 2\n58\n\n\nWT\nWT 3\n60\n\n\nWT\nWT 4\n92\n\n\nTM/+\nTM/+ 1\n76\n\n\nTM/+\nTM/+ 2\n52\n\n\nTM/+\nTM/+ 3\n66\n\n\nTM/TM\nTM/TM 1\n78\n\n\nTM/TM\nTM/TM 2\n74\n\n\nTM/TM\nTM/TM 3\n70\n\n\nTM/TM\nTM/TM 4\n48\n\n\nTM/TM\nTM/TM 5\n54\n\n\n\n\n\n\n\n\n14.8.1 Model fit and inference\n\n14.8.1.1 Model fit\n\nfig5b_lmm &lt;- lmer(caspr_length ~ genotype + (1 | mouse), data = fig5b)\n\nNotes\n\nThis model is called a “random intercept” model. The factor \\(\\texttt{mouse}\\) is a random factor and is added to the model as a random intercept. Random factors are added inside parentheses. A random intercept is specified with a 1 in front of the bar.\n\n\n\n14.8.1.2 Inference\n\nfig5b_lmm_emm &lt;- emmeans(fig5b_lmm, specs = \"genotype\")\nfig5b_lmm_pairs &lt;- contrast(fig5b_lmm_emm, method = \"revpairwise\", adjust = \"tukey\") |&gt;\n  summary(infer = TRUE)\nfig5b_lmm_pairs |&gt;\n  kable(digits = c(1,2,2,1,2,2,1,4)) |&gt;\n  kable_styling()\n\n\n\n\ncontrast\nestimate\nSE\ndf\nlower.CL\nupper.CL\nt.ratio\np.value\n\n\n\n\n(TM/+) - WT\n0.48\n0.16\n8.9\n0.03\n0.93\n3.0\n0.0387\n\n\n(TM/TM) - WT\n0.49\n0.14\n8.8\n0.09\n0.88\n3.4\n0.0192\n\n\n(TM/TM) - (TM/+)\n0.01\n0.16\n9.2\n-0.43\n0.44\n0.0\n0.9990\n\n\n\n\n\n\n\nNotes\n\n. I would not adjust the p-values (see Chapter 12) but I have adjusted using Tukey HSD to compare to researcher’s results in next section\n\n\n\n14.8.1.3 Plot the model\n\nset.seed(1)\nggptm(fig5b_lmm, fig5b_lmm_emm, fig5b_lmm_pairs,\n      show_nest_data = TRUE,\n      nest_id = \"mouse\",\n      hide_pairs = 3,\n      y_label = \"Caspr Length (µm)\",\n      palette = \"pal_okabe_ito_blue\")\n\n\n\n\n\n\n\n\n\n\n\n14.8.2 What the researchers did: Classical nested ANOVA\n\nfig5b_aov &lt;- aov_4(caspr_length ~ genotype + (1 | mouse), \n                   fun_aggregate = mean,\n                   data = fig5b)\nfig5b_aov_emm &lt;- emmeans(fig5b_aov, specs = \"genotype\")\nfig5b_aov_pairs &lt;- contrast(fig5b_aov_emm, method = \"revpairwise\", adjust = \"tukey\") |&gt;\n  summary(infer = TRUE) |&gt;\n  data.table()\nfig5b_aov_pairs |&gt;\n  kable(digits = c(1,2,2,1,2,2,1,4)) |&gt;\n  kable_styling()\n\n\n\n\ncontrast\nestimate\nSE\ndf\nlower.CL\nupper.CL\nt.ratio\np.value\n\n\n\n\n(TM/+) - WT\n0.49\n0.17\n9\n0.02\n0.95\n2.9\n0.0401\n\n\n(TM/TM) - WT\n0.49\n0.15\n9\n0.08\n0.90\n3.4\n0.0204\n\n\n(TM/TM) - (TM/+)\n0.00\n0.16\n9\n-0.44\n0.45\n0.0\n0.9996\n\n\n\n\n\n\n\nNotes\n\nIf the design is balanced (all units have the same sample size), classical nested ANOVA and the linear mixed model with a random intercept are equivalent.\nIn general, ANOVA doesn’t handle lack of balance well (different sample sizes per unit). If there were the same number of measures per mouse, The t and p values of this nested anova would be the same as these values fit by the linear mixed model. As is, the t value is slightly smaller and the p-value is slightly larger in the nested ANOVA.\n\n\n\n14.8.3 What if the researchers had ignored the non-independence?\n\nfig5b_lm &lt;- lm(caspr_length ~ genotype, data = fig5b)\nfig5b_lm_emm &lt;- emmeans(fig5b_lm, specs = \"genotype\")\nfig5b_lm_pairs &lt;- contrast(fig5b_lm_emm, method = \"revpairwise\", adjust = \"tukey\") |&gt;\n  summary(infer = TRUE) |&gt;\n  data.table()\nfig5b_lm_pairs[, p.value := format(p.value, digits = 3)]\nfig5b_lm_pairs |&gt;\n  kable(digits = c(1,2,2,1,2,2,1,2)) |&gt;\n  kable_styling()\n\n\n\n\ncontrast\nestimate\nSE\ndf\nlower.CL\nupper.CL\nt.ratio\np.value\n\n\n\n\n(TM/+) - WT\n0.46\n0.08\n815\n0.27\n0.66\n5.6\n7.29e-08\n\n\n(TM/TM) - WT\n0.48\n0.07\n815\n0.31\n0.65\n6.7\n1.15e-10\n\n\n(TM/TM) - (TM/+)\n0.02\n0.08\n815\n-0.17\n0.21\n0.2\n9.79e-01\n\n\n\n\n\n\n\nNotes\n\nThe p-values are extremely small because the model “thinks” the data are independent. This is an example of pseudoreplication – using technical replicates as treatment replicates.\n\n\n\n14.8.4 A deeper dive into the random intercepts\nRemember, intercepts are means. A random intercept model computes a mean for each level of the random factor, so here, the model is computing a mean for each mouse. The mouse means are not the mean you would compute using a calculator. Instead the modeled mean of each mouse within a treatment is a little closer to the treatment mean than the sample mean of each mouse. That is, the deviation between the mouse mean and the treatment mean are “shrunk toward zero”.\nIt is easy to show these shrunken estimates of the cluster means. Here are the modeled means of each WT mouse, and the deviation from the modeled treatment mean.\n\nlmm_means_table &lt;- data.table(\n  mouse = unique(fig5b[genotype == \"WT\", mouse]),\n  intercept = coef(fig5b_lmm)$mouse[1:4, \"(Intercept)\"])\nlmm_means_table[, deviation := intercept - coef(summary(fig5b_lmm))[1, \"Estimate\"]]\nlmm_means_table\n\n    mouse intercept   deviation\n   &lt;fctr&gt;     &lt;num&gt;       &lt;num&gt;\n1:   WT 1  3.872542 -0.08586488\n2:   WT 2  4.137722  0.17931496\n3:   WT 3  3.839612 -0.11879463\n4:   WT 4  3.983751  0.02534454\n\n\nHere are the sample means of each WT mouse and the deviation from the treatment mean\n\nsample_means_table &lt;- fig5b[genotype == \"WT\", .(mean = (mean(caspr_length))), by = .(mouse)]\nsample_means_table[, deviation := mean - mean(mean)]\nsample_means_table\n\n    mouse     mean   deviation\n   &lt;fctr&gt;    &lt;num&gt;       &lt;num&gt;\n1:   WT 1 3.850525 -0.11038077\n2:   WT 2 4.209068  0.24816221\n3:   WT 3 3.793921 -0.16698438\n4:   WT 4 3.990108  0.02920293",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Models for non-independence -- linear mixed models</span>"
    ]
  },
  {
    "objectID": "chapters/lmm.html#example-2-rcbd-with-2-groups-classical-equivalent-paired-t-test",
    "href": "chapters/lmm.html#example-2-rcbd-with-2-groups-classical-equivalent-paired-t-test",
    "title": "14  Models for non-independence – linear mixed models",
    "section": "14.9 Example 2: RCBD with 2 groups (classical equivalent: paired t-test)",
    "text": "14.9 Example 2: RCBD with 2 groups (classical equivalent: paired t-test)\nsource: Chen, J., López-Moyado, I.F., Seo, H., Lio, C.W.J., Hempleman, L.J., Sekiya, T., Yoshimura, A., Scott-Browne, J.P. and Rao, A., 2019. NR4A transcription factors limit CAR T cell function in solid tumours. Nature, 567(7749), pp.530-534.\nSource Figure: Fig 2C upper left (NR4A1 using CAR)\nResponse variable: NR4A expression measured as MFI (mean flourescence intensity)\nTreatment: CAR PD-1hi TIM3hi TILs, CAR PD-1hi TIM3lo TILs (so differ in density of TIM3 receptors)\nExperimental Design: Randomized Complete Block Design (RCBD). Three preparations (experiments). Experiment is the block.\nBackground: CAR-T are T cells expressing chimeric antigen receptors and target/kill solid tumor cells. TILs are tumor infiltrating lymphocytes. NR are nuclear receptors that function as transcription factors. PD-1 and TIM3 are inhibitory receptors. CAR PD-1hi TIM3hi TILs are “highly exhausted”, CAR PD-1hi TIM3lo TILs are “antigen-specific memory precursor”\n\n14.9.1 lmm1 – random intercept model\n\nlmm1 &lt;- lmer(nr4a1 ~ genotype + (1 | experiment_id), data = fig2c)\nlmm1_emm &lt;- emmeans(lmm1, specs = \"genotype\")\nlmm1_pairs &lt;- contrast(lmm1_emm, method = \"revpairwise\") |&gt;\n  summary(infer = TRUE)\nlmm1_pairs\n\n contrast          estimate   SE df lower.CL upper.CL t.ratio p.value\n (TIM3-) - (TIM3+)    -81.3 12.2  2     -134      -29  -6.683  0.0217\n\nDegrees-of-freedom method: kenward-roger \nConfidence level used: 0.95 \n\n\n\n\n14.9.2 aov1 – repeated measures ANOVA (RM-ANOVA)\n\naov1 &lt;- aov_4(nr4a1 ~ genotype + (genotype | experiment_id), data = fig2c)\naov1_emm &lt;- emmeans(aov1, specs = \"genotype\")\naov1_pairs &lt;- contrast(aov1_emm, method = \"revpairwise\") |&gt;\n  summary(infer = TRUE)\naov1_pairs\n\n contrast        estimate   SE df lower.CL upper.CL t.ratio p.value\n TIM3..1 - TIM3.    -81.3 12.2  2     -134      -29  -6.683  0.0217\n\nConfidence level used: 0.95 \n\n\n\n\n14.9.3 pptt – pairwise paired t-test\n\n# t.test(fig2c[genotype == \"TIM3-\", nr4a1], fig2c[genotype == \"TIM3+\", nr4a1], paired = TRUE)\npptt1 &lt;- pptt(nr4a1 ~ genotype + (genotype | experiment_id), data = fig2c)\nprint(pptt1)\n\n        contrast  estimate       SE    df lower.CL  upper.CL   t.ratio\n          &lt;char&gt;     &lt;num&gt;    &lt;num&gt; &lt;num&gt;    &lt;num&gt;     &lt;num&gt;     &lt;num&gt;\n1: TIM3- - TIM3+ -81.33333 12.17009     2 -133.697 -28.96966 -6.683051\n      p.value\n        &lt;num&gt;\n1: 0.02166486\n\n\n\n\n14.9.4 Notes on the three models\n\nAll inferential statistics are the same in all three models.\nA RM-ANOVA with two groups is equivalent to a random intercept linear mixed model. A paired t-test is a special case of a linear mixed model.\naov1 fits an ANOVA model even though there are only two groups. Yes ANOVA can be used with only two groups, and the statistics are exactly the same as with a t-test, but most people choose t-test and not ANOVA when there are only two groups.\nThe random component of the model formula for the linear mixed model using lmer is (1 | experiment_id). This is the notation for adding a random intercept. \\(\\texttt{experiment\\_id\\) is a random factor and is added as a random intercept.\nThere is potential for confusion in the formula for the RM-ANOVA model using aov_4. The afex package has three equivalent functions for fitting repeated measures ANOVA models, each function is targeted to users with different statistics backgrounds. This text uses the aov_4 function which uses the lmer formula notation. For an RCBD experimental design, aov_4 uses the lmer formula notation for a random intercept AND a random slope, which is (genotype | experiment_id). No random intercept or slope is estimated in an ANOVA model but the notation is necessary to aggregate multiple values of \\(\\texttt{experiment\\_id}\\) only within and not among \\(\\texttt{genotype}\\).\n\n\n\n14.9.5 Plot the model\n\nplot_response(lmm1, lmm1_emm, lmm1_pairs,\n              join_blocks = TRUE,\n              block_id = \"experiment_id\")\n\n\n\n\n\n\n\n\n\n\n14.9.6 Better Know the linear mixed model\nIntraclass correlation\n\nVarCorr(lmm1)\n\n Groups        Name        Std.Dev.\n experiment_id (Intercept) 52.602  \n Residual                  14.905  \n\n\n\nlmm1_vc &lt;- VarCorr(lmm1) |&gt;\n  data.frame() |&gt;\n  data.table()\namong_var &lt;- lmm1_vc[1, vcov]\nwithin_var &lt;- lmm1_vc[2, vcov]\nicc &lt;- among_var/(among_var + within_var)\nicc\n\n[1] 0.9256761",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Models for non-independence -- linear mixed models</span>"
    ]
  },
  {
    "objectID": "chapters/lmm.html#sec-lmm-example2",
    "href": "chapters/lmm.html#sec-lmm-example2",
    "title": "14  Models for non-independence – linear mixed models",
    "section": "14.11 Example 2: RCBD with >2 groups (classical equivalent: repeated measures ANOVA) (fig6g)",
    "text": "14.11 Example 2: RCBD with &gt;2 groups (classical equivalent: repeated measures ANOVA) (fig6g)\nThis example introduces linear mixed models for batches that contain all treatment levels of a single factor but no subsampling replication. In this example, the batch is the individual mouse (\\(\\texttt{mouse\\_id}\\)). There are four measures of the response variable on each mouse, one measure per treatment level. When there is no subsampling replication, we cannot add a random slope to the model because there is only a single observation at each treatment level and a slope would fit the point at the reference level and the point at the non-reference level perfectly. However, we can explicitly model variation in the correlated error and heterogeneity in the variances among treatments as an alternative to modeling a random slope.\nReversing a model of Parkinson’s disease with in situ converted nigral neurons\nPublic source\nSource figure: Fig. 6g\nSource data: Source Data Fig. 6\n\n14.11.1 Understand the data\nIn this study, the researchers investigated the effectiveness of knocking down the protein PTBP1 to induce astrocytes to convert to neurons in a motor processing region of the brain. Experimental lesions of this region of the brain is a model of Parkinson’s disease. In Experiment 6g, the researchers\n\nGenerated a lesion in the motor processing region using 6-hydroxydopamine (6-OHDA). The lesion disrupts the ability to control the contralateral (opposite side) forelimb.\nOne month after the lesion, measured the percent of ipsilateral (same side) forepaw touches (the forelimb extending out and touching the surface) in a test of exploration in a new environment (the “cylinder test”). The expected percent in an intact mouse is 50%. In a lesioned mouse, the percent should be much greater than 50% since there is less control of the contralateral limb. The measure at this point is in the treatment “Lesion”. This is the positive control.\nConverted astrocytes in the lesion to functional neurons by knocking down PTBP1.\nTwo months after knockdown, gave the mouse saline and remeasured percent ipsilateral touches in a cylinder test. If the knockdown worked as expected, there should be closer to 50% ipsilateral touches. The measure at this point is in the treatment “Saline”. The comparison with Lesion is a focal test.\nInhibited neuron action in the converted neurons using clozapine-N-oxide (CNO), which suppresses neuron electrical activity. Then, remeasured percent ipsilateral touches in a cylinder test. If the CNO worked as expected, there should be much greater than 50% ipsilateral touches since there should be re-loss of control of the contralateral limb. The measure at this point is in the treatment “CNO”. The comparison with Saline is a focal test.\nAllowed three days for the CNO to degrade, then, remeasured percent ipsilateral touches in a cylinder test. If the CNO degraded as expected, the converted neurons should be functional and there should be closer to 50% ipsilateral touches. The measure at this point is in the treatment “Post_CNO”. The comparison with CNO is a focal test.\n\nThe design is \\(4 \\times 1\\) – a single treatment with four levels (“Lesion”, “Saline”, “CNO”, “Post_CNO”)\nThe planned contrasts are\n\nSaline - Lesion. This measures the effect of the knockdown and conversion of astrocytes to functional neurons.\nCNO - Saline. This measures the effect of inhibiting the converted neurons to test if it was these and not some other neurons that account for the effect in contrast 1.\nPost_CNO - CNO. This is probing the same expectation as contrast 2.\n\n\n\n14.11.2 Model fit and inference\n\n14.11.2.1 Fit the model\n\nfig6g_m1a &lt;- lmer(touch ~ treatment + (1|mouse_id), data = fig6g)\n\n# alt model\nfig6g_m1b &lt;- lme(touch ~ treatment,\n                random = ~1|mouse_id,\n                correlation = corSymm(form = ~ 1 | mouse_id),\n                weights = varIdent(form = ~ 1 | treatment),\n                data = fig6g)\n\nAIC(fig6g_m1a, fig6g_m1b)\n\nWarning in AIC.default(fig6g_m1a, fig6g_m1b): models are not all fitted to the\nsame number of observations\n\n\n          df      AIC\nfig6g_m1a  6 191.3367\nfig6g_m1b 15 198.3888\n\n# report model a\nfig6g_m1 &lt;- fig6g_m1a\n\nfig6g_m1b overparameterizes, report fig6g_m1a (see Alternative models for fig6g below)\n\n\n14.11.2.2 Inference from the model\n\nfig6g_m1_coef &lt;- cbind(coef(summary(fig6g_m1)),\n                       confint(fig6g_m1)[-c(1:2),])\n\n# exp5c_m1_coef |&gt;\n#   kable(digits = c(2,3,1,1,4,2,2)) |&gt;\n#   kable_styling()\n\n\nfig6g_m1_emm &lt;- emmeans(fig6g_m1, specs = c(\"treatment\"))\n\n\n\n\n\n\ntreatment\nemmean\nSE\ndf\nlower.CL\nupper.CL\n\n\n\n\nLesion\n83.3\n3.44\n18.9\n76.1\n90.47\n\n\nSaline\n62.8\n3.44\n18.9\n55.5\n69.96\n\n\nCNO\n81.4\n3.44\n18.9\n74.2\n88.63\n\n\nPost_CNO\n58.8\n3.44\n18.9\n51.6\n66.00\n\n\n\n\n\n\n\n\n# fig6g_m1_emm # print in console to get row numbers\n# set the mean as the row number from the emmeans table\nlesion &lt;- c(1,0,0,0)\nsaline &lt;- c(0,1,0,0)\ncno &lt;- c(0,0,1,0)\npost_cno &lt;- c(0,0,0,1)\n\nfig6g_m1_planned &lt;- contrast(fig6g_m1_emm,\n                       method = list(\n                         \"Saline - Lesion\" = c(saline - lesion),\n                         \"CNO - Saline\" = c(cno - saline),\n                         \"Post_CNO - CNO\" = c(post_cno - cno)\n                       ),\n                       adjust = \"none\"\n) |&gt;\n  summary(infer = TRUE)\n\n\n\n\n\n\ncontrast\nestimate\nSE\ndf\nlower.CL\nupper.CL\nt.ratio\np.value\n\n\n\n\nSaline - Lesion\n-20.52\n4.071\n18\n-29.07\n-11.96\n-5.04\n0.00009\n\n\nCNO - Saline\n18.67\n4.071\n18\n10.12\n27.23\n4.59\n0.00023\n\n\nPost_CNO - CNO\n-22.63\n4.071\n18\n-31.18\n-14.08\n-5.56\n0.00003\n\n\n\n\n\n\n\n\n\n14.11.2.3 Plot the model\n\n\n\n\n\nTreatment effect on ipsilateral touch, as percent of all touches. Gray dots connected by lines are individual mice.\n\n\n\n\n\n\n14.11.2.4 Alternaplot the model\n\n\n\n\n\nIpsilateral touch (percent of all touches) response to different treatments. Gray dots connected by lines are individual mice.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Models for non-independence -- linear mixed models</span>"
    ]
  },
  {
    "objectID": "chapters/lmm.html#example-3-rcbd-with-two-factors-exp5c",
    "href": "chapters/lmm.html#example-3-rcbd-with-two-factors-exp5c",
    "title": "14  Models for non-independence – linear mixed models",
    "section": "14.12 Example 3 – RCBD with two factors (exp5c)",
    "text": "14.12 Example 3 – RCBD with two factors (exp5c)\nExample 3 is similar to example 2 in that there is no subsampling replication and we cannot add random slopes to the linear mixed model. Example 3 differs in that the design is factorial – there are two, crossed fixed factors. Consequently, there are several alternative models with different sets of random intercepts. The reported model includes two random intercepts, one of which models differences in batch effects among treatment levels (treatment by batch interactions). This interaction intercept is an alternative to random slope for modeling treatment by batch interactions.\nTranscriptomic profiling of skeletal muscle adaptations to exercise and inactivity\nSource figure: Fig. 5c\nSource data: Source Data Fig. 5\n\n\nNew names:\n• `` -&gt; `...1`\n• `` -&gt; `...2`\n• `` -&gt; `...3`\n• `` -&gt; `...4`\n• `` -&gt; `...5`\n• `` -&gt; `...6`\n• `` -&gt; `...7`\n• `` -&gt; `...8`\n• `` -&gt; `...9`\n• `` -&gt; `...10`\n• `` -&gt; `...11`\n• `` -&gt; `...12`\n• `` -&gt; `...13`\n\n\n\n14.12.1 Understand the data\nThe data for Example 1 are from Figure 5c. Six muscle source cells were used to start six independent cultures. Cells from each culture were treated with either a negative control (“Scr”) or a siRNA (“siNR4A3”) that “silences” expression of the NR4A3 gene product by cleaving the mRNA. Glucose uptake in the two cell types was measured at rest (“Basal”) and during electrical pulse stimulation (“EPS”).\nThe design is a \\(2 \\times 2\\) Randomized complete block with no subsampling. There are two factors each with two levels: \\(\\texttt{treatment}\\) (“Scr”, “siNR4A3”) and \\(\\texttt{activity}\\) (“Basal”, “EPS”). Each source cell is a block. All four treatment combinations were measured once per block.\n\n\n14.12.2 Examine the data\n\n\n\n\n\n\n\n\n\nThe plot shows a strong donor effect.\n\n\n14.12.3 Model fit and inference\n\nexp5c_m1a &lt;- lmer(glucose_uptake ~ treatment * activity +\n                   (1 | donor),\n                 data = exp5c)\n\nexp5c_m1b &lt;- lmer(glucose_uptake ~ treatment * activity +\n                   (1 | donor) +\n                   (1 | donor:treatment) +\n                   (1 | donor:activity),\n                 data = exp5c)\n\nboundary (singular) fit: see help('isSingular')\n\nexp5c_m1c &lt;- lmer(glucose_uptake ~ treatment * activity +\n                   (1 | donor) +\n                   (1 | donor:treatment),\n                 data = exp5c)\n\nexp5c_m1d &lt;- lme(glucose_uptake ~ treatment * activity,\n                 random =  ~ 1 | donor,\n                 correlation = corSymm(form = ~ 1 | donor),\n                 weights = varIdent(form = ~ 1|t.by.a),\n                 data = exp5c)\n\n# check AIC\nAIC(exp5c_m1a, exp5c_m1b, exp5c_m1c, exp5c_m1d)\n\nWarning in AIC.default(exp5c_m1a, exp5c_m1b, exp5c_m1c, exp5c_m1d): models are\nnot all fitted to the same number of observations\n\n\n          df       AIC\nexp5c_m1a  6  8.153813\nexp5c_m1b  8 10.052676\nexp5c_m1c  7  8.052676\nexp5c_m1d 15 14.763024\n\n# check VarCorr model c\nVarCorr(exp5c_m1c) # fine\n\n Groups          Name        Std.Dev.\n donor:treatment (Intercept) 0.089519\n donor           (Intercept) 0.352097\n Residual                    0.090685\n\n# report 1c (based on AIC and VarCorr check)\nexp5c_m1 &lt;- exp5c_m1c\n\nexp5c_m1b (equivalent to univariate repeated measures ANOVA) is singular fit. don’t use. Trivial difference in AIC between exp5c_m1a and exp5c_m1c. Nothing in VarCorr with exp5c_m1c raises red flags. Report exp5c_m1c.\n\n14.12.3.1 Check the model\n\nggcheck_the_model(exp5c_m1)\n\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\n\n\nWarning in rlm.default(x, y, weights, method = method, wt.method = wt.method, :\n'rlm' failed to converge in 20 steps\n\n\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\n\n\nWarning in rlm.default(x, y, weights, method = method, wt.method = wt.method, :\n'rlm' failed to converge in 20 steps\n\n\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\n\n\nWarning: Model failed to converge with 1 negative eigenvalue: -8.3e-02\n\n\nboundary (singular) fit: see help('isSingular')\n\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nfine.\n\n\n14.12.3.2 Inference from the model\n\nexp5c_m1_coef &lt;- cbind(coef(summary(exp5c_m1)),\n                       confint(exp5c_m1)[-c(1:3),])\n\n\nexp5c_m1_coef |&gt;\n  kable(digits = c(2,3,1,1,4,2,2)) |&gt;\n  kable_styling()\n\n\n\n\n\nEstimate\nStd. Error\ndf\nt value\nPr(&gt;&#124;t&#124;)\n2.5 %\n97.5 %\n\n\n\n\n(Intercept)\n1.34\n0.153\n5.8\n8.8\n0.0001\n1.02\n1.66\n\n\ntreatmentsiNR4A3\n0.08\n0.074\n8.5\n1.1\n0.2994\n-0.07\n0.23\n\n\nactivityEPS\n0.15\n0.052\n10.0\n2.9\n0.0163\n0.05\n0.25\n\n\ntreatmentsiNR4A3:activityEPS\n-0.27\n0.074\n10.0\n-3.6\n0.0048\n-0.41\n-0.12\n\n\n\n\n\n\n\n\nexp5c_m1_emm &lt;- emmeans(exp5c_m1, specs = c(\"treatment\", \"activity\"))\n\n\n\n\n\n\ntreatment\nactivity\nemmean\nSE\ndf\nlower.CL\nupper.CL\n\n\n\n\nScr\nBasal\n1.34\n0.153\n5.8\n0.96\n1.72\n\n\nsiNR4A3\nBasal\n1.42\n0.153\n5.8\n1.04\n1.80\n\n\nScr\nEPS\n1.49\n0.153\n5.8\n1.11\n1.87\n\n\nsiNR4A3\nEPS\n1.31\n0.153\n5.8\n0.93\n1.68\n\n\n\n\n\n\n\n\n# exp5c_emm # print in console to get row numbers\n# set the mean as the row number from the emmeans table\nscr_basal &lt;- c(1,0,0,0)\nsiNR4A3_basal &lt;- c(0,1,0,0)\nscr_eps &lt;- c(0,0,1,0)\nsiNR4A3_eps &lt;- c(0,0,0,1)\n\nexp5c_m1_planned &lt;- contrast(exp5c_m1_emm,\n                       method = list(\n                         \"(Scr EPS) - (Scr Basal)\" = c(scr_eps - scr_basal),\n                         \"(siNR4A3 EPS) - (siNR4A3 Basal)\" = c(siNR4A3_eps - siNR4A3_basal),\n                         \"Interaction\" = c(siNR4A3_eps - siNR4A3_basal) -\n                           c(scr_eps - scr_basal)\n                           \n                       ),\n                       adjust = \"none\"\n) |&gt;\n  summary(infer = TRUE)\n\n\n\n\n\n\ncontrast\nestimate\nSE\ndf\nlower.CL\nupper.CL\nt.ratio\np.value\n\n\n\n\n(Scr EPS) - (Scr Basal)\n0.15\n0.052\n10\n0.03\n0.27\n2.88\n0.016\n\n\n(siNR4A3 EPS) - (siNR4A3 Basal)\n-0.12\n0.052\n10\n-0.23\n0.00\n-2.22\n0.051\n\n\nInteraction\n-0.27\n0.074\n10\n-0.43\n-0.10\n-3.61\n0.005\n\n\n\n\n\n\n\n\n\n14.12.3.3 Plot the model\n\n\n\n\n\nTreatment effect on glucose uptake. Gray dots connected by lines are individual donors.\n\n\n\n\n\n\n14.12.3.4 Alternaplot the model\n\n\n\n\n\nGlucose response to different treatments. Gray dots connected by lines are individual donors. Dashed gray line is expected additive mean of “siNR4A3 EPS”.\n\n\n\n\nNotes\n\nMany researchers might look at the wide confidence intervals relative to the short distance between the means and think “no effect”. The confidence intervals are correct, they simply are not meant to be tools for inferring anything about differences in means. This is one of many reasons why plots of means and error bars can be misleading for inference, despite the ubiquity of their use for communicating results. And, its why I prefer the effects-and-response plots, which explicitly communicate correct inference about effects.\n\n\n\n\n14.12.4 Why we care about modeling batch in exp5c\nFigure @ref(fig:lmm-exp5c-why) shows the modeled means of the four treatment combinations and the individual values colored by donor. It is pretty easy to see that the glucose uptake values for donors 4 and 5 are well above the mean for all four treatments. And, the values for donors 1, 2, and 3 are well below the mean for all four treatments. The values for donor 6 are near the mean for all four treatments.\n\n\n\n\n\nWhy we care about blocking. The black dots are the modeled means of each treatment combination. The colored dots are the measured values of the response for each donor. The position of a donor relative to the mean is easy to see with these data.\n\n\n\n\nLet’s compare the effects estimated by the linear mixed model exp5c_m1 with a linear model that ignores donor.\n\nexp5c_m2 &lt;- lm(glucose_uptake ~ treatment * activity,\n           data = exp5c)\n\n\n\n\n\n\nA. Inference from a linear mixed model with blocking factor (donor) added as a random intercept. B. Inference from a fixed effects model.\n\n\n\n\nFigure @ref(fig:lmm-exp5c-why2)A is a plot of the effects from the linear mixed model that models the correlated error due to donor. Figure @ref(fig:lmm-exp5c-why2)B is a plot of the effects from the fixed effect model that ignores the correlated error due to donor. Adding \\(\\texttt{donor}\\) as a factor to the linear model increases the precision of the estimate of the treatment effects by eliminating the among-donor component of variance from the error variance.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Models for non-independence -- linear mixed models</span>"
    ]
  },
  {
    "objectID": "chapters/lmm.html#sec-lmm-example4",
    "href": "chapters/lmm.html#sec-lmm-example4",
    "title": "14  Models for non-independence – linear mixed models",
    "section": "14.13 Example 4 – RCBDS Experiments with subsampling replication (exp1g)",
    "text": "14.13 Example 4 – RCBDS Experiments with subsampling replication (exp1g)\nThis example is from a design with batches (independent experiments) that contain all treatment levels of a single factor and subsampling replication. These data were used to introduce linear mixed models in Example 1. The design of the experiment is \\(2 \\times 2\\) factorial. Example 1 flattened the analysis to simplify explanation of random intercepts and random slopes. Here, the data are analyzed with a factorial model.\nA GPR174–CCL21 module imparts sexual dimorphism to humoral immunity\nPublic source\nSource figure: Fig. 1g\nSource data: Source Data Fig. 1\n\n14.13.1 Understand the data\nThe researchers in this paper are interested in discovering mechanisms causing the lower antibody-mediated immune response in males relative to females. The data in Fig. 1 are from a set of experiments on mice to investigate how the G-protein coupled receptor protein GPR174 regulates formation of the B-cell germinal center in secondary lymph tissue. GPR174 is a X-linked gene.\nResponse variable \\(\\texttt{gc}\\) – germinal center size (%). The units are the percent of cells expressing germinal center markers.\nFactor 1 – \\(\\texttt{sex}\\) (“M”, “F”). Male (“M”) is the reference level.\nFactor 2 – \\(\\texttt{chromosome}\\) (“Gpr174+”, “Gpr174-”). “Gpr174-” is a GPR174 knockout. The wildtype (“Gpr174+”) condition is the reference level.\nDesign – \\(2 \\times 2\\), that is, two crossed factors each with two levels. This results in four groups, each with a unique combination of the levels from each factor. “M Gpr174+” is the control. “M Gpr174+” is the knockout genotype in males (“knockout added”). “F Gpr174+” is the wildtype female (“X chromosome added”). “F Gpr174-” is the knockout female (“knockout and X chromosome added”.\n\n\n14.13.2 Examine the data\n\nggplot(data = exp1g,\n       aes(x = treatment,\n           y = gc,\n           color = experiment_id)) +\n  geom_point(position = position_dodge(0.4))\n\n\n\n\n\n\n\n\n\n\n14.13.3 Fit the model\n\n# three slope parameters\nexp1g_m1a &lt;- lmer(gc ~ genotype * sex +\n                   (genotype * sex | experiment_id),\n                 data = exp1g)\n\nboundary (singular) fit: see help('isSingular')\n\nVarCorr(exp1g_m1a) # looks fine\n\n Groups        Name                 Std.Dev. Corr                \n experiment_id (Intercept)          1.77088                      \n               genotypeGpr174-      0.96193  -0.023              \n               sexF                 2.90154   0.459  0.878       \n               genotypeGpr174-:sexF 1.33170  -0.314 -0.625 -0.706\n Residual                           1.93155                      \n\n# one slope parameter but capturing all treatment combinations\nexp1g_m1b &lt;- lmer(gc ~ genotype * sex +\n                   (treatment | experiment_id),\n                 data = exp1g)\n\n# intercept interactions\nexp1g_m1c &lt;- lmer(gc ~ genotype * sex +\n                   (1 | experiment_id) +\n                    (1 | experiment_id:genotype) +\n                    (1 | experiment_id:sex) +\n                    (1 | experiment_id:genotype:sex),\n                 data = exp1g)\n\nboundary (singular) fit: see help('isSingular')\n\nVarCorr(exp1g_m1c) # id:genotype is low\n\n Groups                     Name        Std.Dev. \n experiment_id:genotype:sex (Intercept) 0.4046670\n experiment_id:sex          (Intercept) 1.6927499\n experiment_id:genotype     (Intercept) 0.0001216\n experiment_id              (Intercept) 2.5487069\n Residual                               1.9792428\n\n# drop id:genotype which has low variance\nexp1g_m1d &lt;- lmer(gc ~ genotype * sex +\n                   (1 | experiment_id) +\n                    (1 | experiment_id:sex) +\n                    (1 | experiment_id:sex:genotype),\n                 data = exp1g)\n\nAIC(exp1g_m1a, exp1g_m1b, exp1g_m1c, exp1g_m1d)\n\n          df      AIC\nexp1g_m1a 15 344.1579\nexp1g_m1b 15 344.1529\nexp1g_m1c  9 337.7602\nexp1g_m1d  8 335.7602\n\n# go with exp1g_m1d.\nexp1g_m1 &lt;- exp1g_m1d\n\n\n\n14.13.4 Inference from the model\n\nexp1g_m1_coef &lt;- coef(summary(exp1g_m1))\n\n\nexp1g_m1_coef\n\n                      Estimate Std. Error       df   t value    Pr(&gt;|t|)\n(Intercept)           8.435718   1.612602 4.638595  5.231123 0.004202015\ngenotypeGpr174-       2.568365   0.712122 5.760582  3.606637 0.012094327\nsexF                  5.583860   1.397388 4.024010  3.995927 0.015995349\ngenotypeGpr174-:sexF -5.304752   1.013552 5.918159 -5.233823 0.002033657\n\n\n\n# order of factors reversed in specs because I want sex to be\n# main x-axis variable in plot\nexp1g_m1_emm &lt;- emmeans(exp1g_m1, specs = c(\"sex\", \"genotype\"))\n\n\n# exp1g_m1_emm # print in console to get row numbers\n# set the mean as the row number from the emmeans table\nwt_m &lt;- c(1,0,0,0)\nwt_f &lt;- c(0,1,0,0)\nko_m &lt;- c(0,0,1,0)\nko_f &lt;- c(0,0,0,1)\n\n# simple effects within males and females + interaction \n# 1. (ko_m - wt_m) \n# 2. (ko_f - wt_f)\n\nexp1g_contrasts &lt;- list(\n  \"(Gpr174- M) - (Gpr174+ M)\" = c(ko_m - wt_m),\n  \"(Gpr174- F) - (Gpr174+ F)\" = c(ko_f - wt_f),\n  \"Interaction\" = c(ko_f - wt_f) -\n    c(ko_m - wt_m)\n)\nexp1g_m1_planned &lt;- contrast(exp1g_m1_emm,\n                       method = exp1g_contrasts,\n                       adjust = \"none\"\n) |&gt;\n  summary(infer = TRUE)\n\n\n\n\n\n\ncontrast\nestimate\nSE\ndf\nlower.CL\nupper.CL\nt.ratio\np.value\n\n\n\n\n(Gpr174- M) - (Gpr174+ M)\n2.57\n0.714\n5.784\n0.81\n4.33\n3.60\n0.012\n\n\n(Gpr174- F) - (Gpr174+ F)\n-2.74\n0.722\n6.100\n-4.50\n-0.98\n-3.79\n0.009\n\n\nInteraction\n-5.30\n1.016\n5.942\n-7.80\n-2.81\n-5.22\n0.002\n\n\n\n\n\n\n\nNotes\n\nThe direction of the estimated effect is opposite in males and females\n\n\n\n14.13.5 Plot the model\n\n\n\n\n\nTreatment effect on germinal center (GC) formation. Small, pale, colored dots are independent experiments. Intermediate size colored dots are experiment means.\n\n\n\n\n\n\n14.13.6 Alternaplot the model\n\n\n\n\n\nGerminal center (GC) formation in response to treatment. Small, pale, colored dots are independent experiments. Intermediate size colored dots are experiment means. Dashed gray line is expected additive mean of Gpr174- F\n\n\n\n\n\n\n14.13.7 Understanding the alternative models\n\nexp1g_m1a &lt;- lmer(gc ~ genotype * sex +\n                   (genotype * sex | experiment_id),\n                 data = exp1g)\n\nboundary (singular) fit: see help('isSingular')\n\nexp1g_m1b &lt;- lmer(gc ~ genotype * sex +\n                   (treatment | experiment_id),\n                 data = exp1g)\n\nexp1g_m1c &lt;- lmer(gc ~ genotype * sex +\n                   (1 | experiment_id) +\n                    (1 | experiment_id:genotype) +\n                    (1 | experiment_id:sex) +\n                    (1 | experiment_id:genotype:sex),\n                 data = exp1g)\n\nboundary (singular) fit: see help('isSingular')\n\nexp1g_m1d &lt;- lmer(gc ~ genotype * sex +\n                   (1 | experiment_id) +\n                    (1 | experiment_id:sex) +\n                    (1 | experiment_id:genotype:sex),\n                 data = exp1g)\n\nNotes\n\nAll four models model the same fixed effects. The models differ only in how they model the random effects.\nModel exp1g_m1a fits one random intercept, two random slopes, and one random interaction.\n\n\n\\(\\gamma_{0j}\\) – a random intercept modeling batch effects of \\(\\texttt{experiment\\_id}\\) on the intercept\n\\(\\gamma_{1j}\\) – a random slope modeling the effect of the non-reference level of “Gpr174-” on the batch effect of \\(\\texttt{experiment\\_id}\\). This is a \\(\\texttt{experiment\\_id} \\times \\texttt{genotype}\\) interaction.\n\\(\\gamma_{2j}\\) – a random slope modeling the effect of “F” (female) on the batch effect of \\(\\texttt{experiment\\_id}\\). This is a \\(\\texttt{experiment\\_id} \\times \\texttt{sex}\\) interaction.\n\\(\\gamma_{3j}\\) – a random slope modeling the effect of the interaction effect of “Gpr174-” and “F” on the batch effect of \\(\\texttt{experiment\\_id}\\). This is a \\(\\texttt{experiment\\_id} \\times \\texttt{sex}  \\times \\texttt{genotype}\\) interaction.\n\n\nModel exp1g_m1b fits one random intercept and three random slopes\n\n\n\\(\\gamma_{0j}\\) – a random intercept modeling batch effects of \\(\\texttt{experiment\\_id}\\) on the intercept. This is modeling the same thing as \\(\\gamma_{0j}\\) in Model exp1g_m1a.\n\\(\\gamma_{1j}\\) – a random slope modeling the effect of “Gpr174- M” on the batch effect of \\(\\texttt{experiment\\_id}\\). This is modeling the same thing as \\(\\gamma_{1j}\\) in Model exp1g_m1a.\n\\(\\gamma_{2j}\\) – a random slope modeling the effect of “Gpr174+ F” on the batch effect of \\(\\texttt{experiment\\_id}\\). This is modeling the same thing as \\(\\gamma_{2j}\\) in Model exp1g_m1a.\n\\(\\gamma_{3j}\\) – a random slope modeling the effect of “M Gpr174- F” on the batch effect of \\(\\texttt{experiment\\_id}\\). This is modeling the added variance accounted for by the random interaction \\(\\gamma_{2j}\\) in Model exp1g_m1a but in a different way.\n\n\nModel exp1g_m1c fits four random intercepts\n\n\n\\(\\gamma_{0j}\\) – a random intercept modeling batch effects of \\(\\texttt{experiment\\_id}\\) on the intercept. This is modeling the same thing as \\(\\gamma_{0j}\\) in Model exp1g_m1a.\n\\(\\gamma_{0jk}\\) – a random intercept modeling the effects of the combination of \\(\\texttt{experiment\\_id}\\) and \\(\\texttt{genotype}\\). This is very similar to the variance modeled by \\(\\gamma_{1j}\\) in Model exp1g_m1a except the draws from \\(\\gamma_{0jk}\\) are independent (uncorrelated) of draws from the other random intercepts.\n\\(\\gamma_{0jl}\\) – a random intercept modeling the effects of the combination of \\(\\texttt{experiment\\_id}\\) and \\(\\texttt{sex}\\). This is very similar to the variance modeled by \\(\\gamma_{2j}\\) in Model exp1g_m1a except the draws from \\(\\gamma_{0jl}\\) are independent (uncorrelated) of draws from the other random intercepts (review The correlation among random intercepts and slopes if this doesn’t make sense).\n\\(\\gamma_{0jkl}\\) – a random intercept modeling the effects of the combination of \\(\\texttt{experiment\\_id}\\), \\(\\texttt{genotype}\\) and \\(\\texttt{sex}\\). This is very similar to the variance modeled by \\(\\gamma_{3j}\\) in Model exp1g_m1a except the draws from \\(\\gamma_{0jkl}\\) are independent (uncorrelated) of draws from the other random intercepts.\n\n\nModel exp1g_m1d fits the same random intercepts as Model exp1g_m1c but excludes \\(\\gamma_{0jl}\\) (the random intercept for the experiment_id by genotyp combination. This was excluded because of the low variance of this component in the fit model.\n\n\n\n14.13.8 The VarCorr matrix of models exp1g_m1a and exp1g_m1b\nThe random effect similarity of models exp1g_m1a and exp1g_m1b can be seen in the estimated variance components and correlations among the random effects.\n\n\n\nThe Varcorr matrix. Standard deviations of random effects on the diagonal. Correlations of random effects on the off-diagonal.\n\n\n\n\n\n\n\n\n\n\n\nexp1g_m1a\n\n\n(Intercept)\n1.7709\n\n\n\n\n\ngenotypeGpr174-\n-0.0232\n0.9619\n\n\n\n\nsexF\n0.4589\n0.8776\n2.9015\n\n\n\ngenotypeGpr174-:sexF\n-0.3137\n-0.6247\n-0.7056\n1.3317\n\n\nexp1g_m1b\n\n\n(Intercept)\n1.7711\n\n\n\n\n\ntreatmentGpr174- M\n-0.0250\n0.9641\n\n\n\n\ntreatmentGpr174+ F\n0.4585\n0.8761\n2.9024\n\n\n\ntreatmentGpr174- F\n0.2999\n0.8793\n0.9384\n2.9909\n\n\n\n\n\n\n\n\n\n14.13.9 The linear mixed model has more precision and power than the fixed effect model of batch means\n\n# means pooling model\nexp1g_m2 &lt;- lm(gc ~ sex * genotype,\n                data = exp1g_means)\n\n\n\n\nPlanned contrasts for the linear mixed model exp1g_m1 and the fixed effects model of experiment means exp1g_m2.\n\n\ncontrast\nestimate\nSE\ndf\nlower.CL\nupper.CL\nt.ratio\np.value\n\n\n\n\nexp1g_m1 (lmm)\n\n\n(Gpr174- M) - (Gpr174+ M)\n2.57\n0.714\n5.8\n0.81\n4.33\n3.60\n0.012\n\n\n(Gpr174- F) - (Gpr174+ F)\n-2.74\n0.722\n6.1\n-4.50\n-0.98\n-3.79\n0.009\n\n\nInteraction\n-5.30\n1.016\n5.9\n-7.80\n-2.81\n-5.22\n0.002\n\n\nexp1g_m2 (lm means pooling)\n\n\n(Gpr174- M) - (Gpr174+ M)\n2.50\n2.257\n12.0\n-2.41\n7.42\n1.11\n0.289\n\n\n(Gpr174- F) - (Gpr174+ F)\n-2.73\n2.257\n12.0\n-7.65\n2.18\n-1.21\n0.249\n\n\nInteraction\n-5.24\n3.192\n12.0\n-12.20\n1.72\n-1.64\n0.127\n\n\n\n\n\n\n\nNotes\n\nA fixed effects model fit to batch-means pooled data is strongly conservative and will result in less discovery.\nMeans pooling does not make the data independent in a randomized complete block design. A linear mixed model of batch-means pooled data is a mixed-effect ANOVA (Next section. Also see Section @ref(issues-exp4d) in the Issues chapter).\n\n\n\n14.13.10 Fixed effect models and pseudoreplication\n\n# complete pooling model\nexp1g_m3 &lt;- lm(gc ~ sex * genotype,\n                data = exp1g)\n\n\n\n\nPlanned contrasts for the linear mixed model exp1g_m1 and the fixed effects model exp1g_m3 with complete pooling.\n\n\ncontrast\nestimate\nSE\ndf\nlower.CL\nupper.CL\nt.ratio\np.value\n\n\n\n\nexp1g_m1 (lmm)\n\n\n(Gpr174- M) - (Gpr174+ M)\n2.57\n0.714\n5.8\n0.81\n4.33\n3.60\n0.012\n\n\n(Gpr174- F) - (Gpr174+ F)\n-2.74\n0.722\n6.1\n-4.50\n-0.98\n-3.79\n0.009\n\n\nInteraction\n-5.30\n1.016\n5.9\n-7.80\n-2.81\n-5.22\n0.002\n\n\nexp1g_m3 (lm complete pooling)\n\n\n(Gpr174- M) - (Gpr174+ M)\n2.44\n1.111\n69.0\n0.22\n4.65\n2.20\n0.032\n\n\n(Gpr174- F) - (Gpr174+ F)\n-2.53\n1.125\n69.0\n-4.77\n-0.28\n-2.25\n0.028\n\n\nInteraction\n-4.97\n1.581\n69.0\n-8.12\n-1.81\n-3.14\n0.002\n\n\n\n\n\n\n\nNotes\n\nComplete pooling is strongly anti-conservative and will result in increased false discovery. Complete pooling is a type of pseudoreplication – the subsamples have been analyzed as if they are independent replicates. Subsamples are not independent.\nFor the Experiment 1g data, the 95% confidence intervals are wider and the p-values are larger in the complete pool model exp1g_m3 compared to the linear mixed model exp1g_m1. This is an unusual result.\n\n\n\n14.13.11 Mixed-effect ANOVA\n\nexp1g_m1_aov &lt;- aov_4(gc ~ sex * genotype +\n                  (sex * genotype | experiment_id),\n                data = exp1g)\n\nWarning: More than one observation per design cell, aggregating data using `fun_aggregate = mean`.\nTo turn off this warning, pass `fun_aggregate = mean` explicitly.\n\n\nNotes\n\nThe formula has the same format as that in Example 3 for repeated measures ANOVA on RCB designs with no subsampling. In biology, this is often called a mixed effect ANOVA with two fixed factors and one random factor.\nThe data are aggregated prior to fitting the model – this means the subsamples are averaged within each batch by treatment combination.\nThe mixed-effect ANOVA is equivalent to the linear mixed model exp1g_m1c if there are the same number of replicates in each treatment combination and the same number of subsamples in all treatment by \\(\\texttt{experiment\\_id}\\) combinations. The design is not balanced in this example.\n\n\n\n\nPlanned contrasts from mixed ANOVA compared to lmm equivalent of mixed ANOVA and lowest AIC lmm.\n\n\ncontrast\nestimate\nSE\ndf\nlower.CL\nupper.CL\nt.ratio\np.value\n\n\n\n\nexp1g_m1_aov (mixed ANOVA)\n\n\n(Gpr174- M) - (Gpr174+ M)\n2.50\n0.599\n3.000\n0.60\n4.41\n4.18\n0.025\n\n\n(Gpr174- F) - (Gpr174+ F)\n-2.73\n0.808\n3.000\n-5.30\n-0.16\n-3.39\n0.043\n\n\nInteraction\n-5.24\n1.087\n3.000\n-8.70\n-1.78\n-4.82\n0.017\n\n\nexp1g_m1_c (lmm equivalent of mixed ANOVA)\n\n\n(Gpr174- M) - (Gpr174+ M)\n2.57\n0.715\n5.784\n0.80\n4.33\n3.59\n0.012\n\n\n(Gpr174- F) - (Gpr174+ F)\n-2.74\n0.724\n6.100\n-4.50\n-0.97\n-3.78\n0.009\n\n\nInteraction\n-5.30\n1.017\n2.972\n-8.56\n-2.05\n-5.21\n0.014\n\n\nexp1g_md (lmm with min AIC)\n\n\n(Gpr174- M) - (Gpr174+ M)\n2.57\n0.714\n5.784\n0.81\n4.33\n3.60\n0.012\n\n\n(Gpr174- F) - (Gpr174+ F)\n-2.74\n0.722\n6.100\n-4.50\n-0.98\n-3.79\n0.009\n\n\nInteraction\n-5.30\n1.016\n5.942\n-7.80\n-2.81\n-5.22\n0.002",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Models for non-independence -- linear mixed models</span>"
    ]
  },
  {
    "objectID": "chapters/lmm.html#working-in-r",
    "href": "chapters/lmm.html#working-in-r",
    "title": "14  Models for non-independence – linear mixed models",
    "section": "14.14 Working in R",
    "text": "14.14 Working in R\n\n14.14.1 Plotting models fit to batched data\n\n14.14.1.1 Models without subsampling - Experiment 6g\nData wrangling necessary for plot:\n\n# convert contrast table to a data.table\nfig6g_m1_planned_dt &lt;- data.table(fig6g_m1_planned)\n\n# create a pretty p-value column\nfig6g_m1_planned_dt[, pretty_p := pvalString(p.value)]\n\n# add group1 and group2 columns to exp1g_m1_planned\nfig6g_m1_planned_dt[, group1 := c(\"Saline\", \"CNO\", \"Post_CNO\")]\nfig6g_m1_planned_dt[, group2 := c(\"Lesion\", \"Saline\", \"CNO\")]\n\nExperiments are colored:\n\ngg1 &lt;- ggplot(data = fig6g,\n              aes(x = treatment,\n                  y = touch,\n                  color = mouse_id)) +\n  geom_point(position = position_dodge(width = 0.2)) +\n  geom_line(aes(group = mouse_id),\n            position = position_dodge(width = 0.2),\n            color = \"gray80\") +\n  geom_point(data = summary(fig6g_m1_emm),\n             aes(y = emmean),\n             color = \"black\",\n             size = 3) +\n  geom_errorbar(data = summary(fig6g_m1_emm),\n             aes(y = emmean,\n                 ymin = lower.CL,\n                 ymax = upper.CL),\n             color = \"black\",\n             width = .05) +\n\n  # pvalue brackets\n  stat_pvalue_manual(fig6g_m1_planned_dt,\n                     label = \"pretty_p\",\n                     y.position = c(99,97,95),\n                     size = 2.5,\n                     tip.length = 0.01) +\n\n  ylab(\"Percent ipsilateral touch\") +\n  scale_color_manual(values = pal_okabe_ito_blue) +\n  theme_pubr() +\n  theme(\n    axis.title.x = element_blank(), # no x-axis title\n    legend.position = \"none\"\n  ) + \n  NULL\n\nTreatments are colored:\n\ngg2 &lt;- ggplot(data = fig6g,\n              aes(x = treatment,\n                  y = touch)) +\n  geom_point(aes(group = mouse_id),\n             position = position_dodge(width = 0.2),\n             color = \"gray\") +\n  geom_line(aes(group = mouse_id),\n            position = position_dodge(width = 0.2),\n            color = \"gray80\") +\n  geom_point(data = summary(fig6g_m1_emm),\n             aes(y = emmean,\n             color = treatment),\n             size = 3) +\n  geom_errorbar(data = summary(fig6g_m1_emm),\n             aes(y = emmean,\n                 ymin = lower.CL,\n                 ymax = upper.CL,\n             color = treatment),\n             width = .05) +\n  \n  # pvalue brackets\n  stat_pvalue_manual(fig6g_m1_planned_dt,\n                     label = \"pretty_p\",\n                     y.position = c(99,97,95),\n                     size = 2.5,\n                     tip.length = 0.01) +\n\n  ylab(\"Percent ipsilateral touch\") +\n  scale_color_manual(values = pal_okabe_ito_blue) +\n  theme_pubr() +\n  theme(\n    axis.title.x = element_blank(), # no x-axis title\n    legend.position = \"none\"\n  ) + \n  NULL\n\n#gg2\n\n\n\n\n\n\n\n\n\n\n\n\n14.14.1.2 Models with subsampling - Experiment 1g\nData wrangling necessary for plot:\n\n# add treatment column to emmeans table\nexp1g_m1_emm_dt &lt;- summary(exp1g_m1_emm) |&gt;\n  data.table()\nexp1g_m1_emm_dt[, treatment := paste(genotype, sex)]\nexp1g_m1_emm_dt[, treatment := factor(treatment,\n                                      levels = levels(exp1g$treatment))]\n\n# create table of means for each treatment * experiment_id combination\nexp1g[, group_mean := predict(exp1g_m1)]\nexp1g_m1_emm2 &lt;- exp1g[, .(group_mean = mean(group_mean)),\n                       by = .(treatment, experiment_id)]\n\n# add group1 and group2 columns to exp1g_m1_planned\nexp1g_m1_planned_dt &lt;- data.table(exp1g_m1_planned)\nexp1g_m1_planned_dt[, pretty_p := pvalString(p.value)]\nexp1g_m1_planned_dt[, group1 := c(\"Gpr174- M\", \"Gpr174- F\", \"\")]\nexp1g_m1_planned_dt[, group2 := c(\"Gpr174+ M\", \"Gpr174+ F\", \"\")]\n\n\n# get coefficients of model\nb &lt;- exp1g_m1_coef[, \"Estimate\"]\n\n# get interaction p\np_ixn &lt;- exp1g_m1_planned_dt[contrast == \"Interaction\", pretty_p]\n\nexp1g_plot_a &lt;- ggplot(data = exp1g,\n              aes(x = treatment,\n                  y = gc,\n                  color = experiment_id)) +\n  # modeled experiment by treatment means\n  geom_point(data = exp1g_m1_emm2,\n            aes(y = group_mean,\n                color = experiment_id),\n            position = position_dodge(width = 0.4),\n            alpha = 1,\n            size = 2) +\n  geom_line(data = exp1g_m1_emm2,\n            aes(y = group_mean,\n                group = experiment_id,\n                color = experiment_id),\n            position = position_dodge(width = 0.4)) +\n  \n  # raw data\n  geom_point(position = position_dodge(width = 0.4),\n             alpha = 0.3\n  ) +\n  \n  # modeled treatment means\n  geom_point(data = exp1g_m1_emm_dt,\n             aes(x = treatment,\n                 y = emmean),\n             color = \"black\",\n             size = 3) +\n  # geom_errorbar(data = exp1g_m1_emm_dt,\n  #            aes(y = emmean,\n  #                ymin = lower.CL,\n  #                ymax = upper.CL),\n  #             color = \"black\",\n  #           width = .05) +\n  \n  # pvalue brackets\n  stat_pvalue_manual(exp1g_m1_planned_dt[1:2],\n                     label = \"pretty_p\",\n                     y.position = c(20,20),\n                     size = 2.5,\n                     tip.length = 0.01) +\n  \n  # additive line + interaction p bracket\n  geom_segment(x = 3.85,\n               y = b[1] + b[2] + b[3],\n               xend = 4.15,\n               yend = b[1] + b[2] + b[3],\n               linetype = \"dashed\",\n               color = \"gray\") +\n  geom_bracket(\n    x = 4.2,\n    y = b[1] + b[2] + b[3],\n    yend = b[1] + b[2] + b[3] + b[4],\n    label = paste0(\"interaction\\np = \", p_ixn),\n    text.size = 3,\n    text.hjust = 0,\n    color = \"black\") +\n  \n  ylab(\"GC (%)\") +\n  scale_color_manual(values = pal_okabe_ito_blue) +\n  theme_pubr() +\n  coord_cartesian(xlim = c(1, 4.1)) +\n  theme(\n    axis.title.x = element_blank(), # no x-axis title\n    legend.position = \"none\"\n  ) + \n  NULL\n  \nexp1g_plot_a &lt;- factor_wrap(exp1g_plot_a)\n\n#exp1g_plot_a\n\n\n# get coefficients of model\nb &lt;- exp1g_m1_coef[, \"Estimate\"]\n\n# get interaction p\np_ixn &lt;- exp1g_m1_planned_dt[contrast == \"Interaction\", pretty_p]\n\ndodge_width = 0.6\nexp1g_plot_b &lt;- ggplot(data = exp1g,\n              aes(x = treatment,\n                  y = gc,\n                  color = experiment_id)) +\n  # modeled experiment by treatment means\n  geom_point(data = exp1g_m1_emm2,\n            aes(y = group_mean,\n                color = experiment_id),\n            position = position_dodge(width = dodge_width),\n            alpha = 1,\n            size = 2) +\n  geom_line(data = exp1g_m1_emm2,\n            aes(y = group_mean,\n                group = experiment_id,\n                color = experiment_id),\n            position = position_dodge(width = dodge_width)) +\n  \n  # raw data\n  geom_point(position = position_dodge(width = dodge_width),\n             color = \"gray80\"\n  ) +\n  \n  # modeled treatment means\n  geom_point(data = exp1g_m1_emm_dt,\n             aes(x = treatment,\n                 y = emmean),\n             color = \"black\",\n             size = 3) +\n  # geom_errorbar(data = exp1g_m1_emm_dt,\n  #            aes(y = emmean,\n  #                ymin = lower.CL,\n  #                ymax = upper.CL),\n  #             color = \"black\",\n  #           width = .05) +\n  \n  # pvalue brackets\n  stat_pvalue_manual(exp1g_m1_planned_dt[1:2],\n                     label = \"pretty_p\",\n                     y.position = c(20,20),\n                     size = 2.5,\n                     tip.length = 0.01) +\n  \n  # additive line + interaction p bracket\n  geom_segment(x = 4 - dodge_width/2,\n               y = b[1] + b[2] + b[3],\n               xend = 4 + dodge_width/2,\n               yend = b[1] + b[2] + b[3],\n               linetype = \"dashed\",\n               color = \"gray\") +\n  geom_bracket(\n    x = 4 + dodge_width/1.9,\n    y = b[1] + b[2] + b[3],\n    yend = b[1] + b[2] + b[3] + b[4],\n    label = paste0(\"interaction\\np = \", p_ixn),\n    text.size = 3,\n    text.hjust = 0,\n    color = \"black\") +\n  \n  ylab(\"GC (%)\") +\n  scale_color_manual(values = pal_okabe_ito_blue) +\n  theme_pubr() +\n  coord_cartesian(xlim = c(1, 4.2)) +\n  theme(\n    axis.title.x = element_blank(), # no x-axis title\n    legend.position = \"none\"\n  ) + \n  NULL\n  \nexp1g_plot_b &lt;- factor_wrap(exp1g_plot_b)\n\n# exp1g_plot_b\n\n\n\n\n\n\n\n\n\n\n\n\n\n14.14.2 Repeated measures ANOVA (randomized complete block with no subsampling)\n\n# this is the rm-ANOVA\nm1 &lt;- aov_4(glucose_uptake ~ treatment * activity +\n              (treatment * activity | donor),\n            data = exp5c)\n\n# lmm equivalent\n\nm2 &lt;- lmer(glucose_uptake ~ treatment * activity +\n         (1 | donor) +\n         (1 | donor:treatment) +\n         (1 | donor:activity),\n       data = exp5c)\n\n# random intercept and slope model that is *not* equivalent\n# this isn't solvable because there is no subsampling\n\nm3 &lt;- lmer(glucose_uptake ~ treatment * activity +\n              (treatment * activity | donor),\n            data = exp5c)\n\nNotes\n\nafex computes the repeated measures anova model using both aov (classical univariate repeated measures ANOVA) and using lm with multiple response variables (the multivariate repeated measures ANOVA). As of this writing, the output from aov is the default.\nGiven only one measure for each donor within a \\(\\texttt{treatment} \\times \\texttt{activity}\\) combination, the linear mixed model m2 is equivalent to the univariate repeated measures model m1.\nThe model formula in m1 looks like that in the linear mixed model m3 but the two models are not equivalent.\n\n\n14.14.2.1 univariate vs. multivariate repeated measures ANOVA\nUse the multivariate model unless you want to replicate the result of someone who used a univariate model. By default, aov_4 computes only the multivariate model (prior to version xxx, the default was to compute both models).\n\n# default mode -- should be multivariate\nexp5c_aov &lt;- aov_4(glucose_uptake ~ treatment * activity +\n                    (treatment * activity | donor),\n                  data = exp5c)\n\n# force aov_4 to compute univariate model\nexp5c_aov1 &lt;- aov_4(glucose_uptake ~ treatment * activity +\n                    (treatment * activity | donor),\n                  data = exp5c,\n                  include_aov = TRUE)\n\n# explicitly exclude the univariate model\nexp5c_aov2 &lt;- aov_4(glucose_uptake ~ treatment * activity +\n                    (treatment * activity | donor),\n                  data = exp5c,\n                  include_aov = FALSE)\n\nNotes\n\nThe include_aov = TRUE argument forces output from aov_4 to include the univariate model.\n\ncontrasts from multivariate model\n\n# These three should give equivalent results\n\n# exp5c_aov was fit with the default -- multivariate model only\nemmeans(exp5c_aov,\n        specs = c(\"treatment\", \"activity\")) |&gt;\n  contrast(method = \"revpairwise\",\n           simple = \"each\",\n           combine = TRUE,\n           adjust = \"none\")\n\n activity treatment contrast      estimate     SE df t.ratio p.value\n Basal    .         siNR4A3 - Scr   0.0813 0.1012  5   0.804  0.4581\n EPS      .         siNR4A3 - Scr  -0.1858 0.0359  5  -5.180  0.0035\n .        Scr       EPS - Basal     0.1510 0.0395  5   3.821  0.0124\n .        siNR4A3   EPS - Basal    -0.1161 0.0626  5  -1.855  0.1228\n\n# exp5c_aov included the univariate model but the default is still the multivariate model\nemmeans(exp5c_aov1,\n        specs = c(\"treatment\", \"activity\")) |&gt;\n  contrast(method = \"revpairwise\",\n           simple = \"each\",\n           combine = TRUE,\n           adjust = \"none\")\n\n activity treatment contrast      estimate     SE df t.ratio p.value\n Basal    .         siNR4A3 - Scr   0.0813 0.1012  5   0.804  0.4581\n EPS      .         siNR4A3 - Scr  -0.1858 0.0359  5  -5.180  0.0035\n .        Scr       EPS - Basal     0.1510 0.0395  5   3.821  0.0124\n .        siNR4A3   EPS - Basal    -0.1161 0.0626  5  -1.855  0.1228\n\n# passing `model = \"multivariate\"` makes the model choice transparent\nemmeans(exp5c_aov1,\n        specs = c(\"treatment\", \"activity\"),\n        model = \"multivariate\") |&gt;\n  contrast(method = \"revpairwise\",\n           simple = \"each\",\n           combine = TRUE,\n           adjust = \"none\")\n\n activity treatment contrast      estimate     SE df t.ratio p.value\n Basal    .         siNR4A3 - Scr   0.0813 0.1012  5   0.804  0.4581\n EPS      .         siNR4A3 - Scr  -0.1858 0.0359  5  -5.180  0.0035\n .        Scr       EPS - Basal     0.1510 0.0395  5   3.821  0.0124\n .        siNR4A3   EPS - Basal    -0.1161 0.0626  5  -1.855  0.1228\n\n\nNotes\n\nIf our rmANOVA model is fit with the default specification (exp5c_aov1), we can get the multivariate output using the model = \"multivariate\" argument within emmeans (not the contrast function!). Or, if our fit excluded the univariate model (exp5c_aov2), then we don’t need the model argument.\n\ncontrasts from univariate model\n\n# get the univariate model results using $aov\nemmeans(exp5c_aov1$aov, specs = c(\"treatment\", \"activity\")) |&gt;\n  contrast(method = \"revpairwise\",\n           simple = \"each\",\n           combine = TRUE,\n           adjust = \"none\")\n\n activity treatment contrast      estimate     SE   df t.ratio p.value\n Basal    .         siNR4A3 - Scr   0.0813 0.0759 8.60   1.071  0.3132\n EPS      .         siNR4A3 - Scr  -0.1858 0.0759 8.60  -2.448  0.0380\n .        Scr       EPS - Basal     0.1510 0.0524 9.39   2.884  0.0173\n .        siNR4A3   EPS - Basal    -0.1161 0.0524 9.39  -2.218  0.0525\n\n\nNotes\n\nPassing exp5c_aov1 to emmeans will return the contrasts from the multivariate. change this to the univariate model by passing exp5c_aov1$aov.\n\n\n\n14.14.2.2 The ANOVA table\n\nnice(exp5c_aov1, correction = \"GG\")\n\nAnova Table (Type 3 tests)\n\nResponse: glucose_uptake\n              Effect   df  MSE       F   ges p.value\n1          treatment 1, 5 0.02    0.68  .006    .448\n2           activity 1, 5 0.01    0.30 &lt;.001    .609\n3 treatment:activity 1, 5 0.01 10.37 *  .037    .023\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '+' 0.1 ' ' 1\n\nnice(exp5c_aov1, correction = \"none\")\n\nAnova Table (Type 3 tests)\n\nResponse: glucose_uptake\n              Effect   df  MSE       F   ges p.value\n1          treatment 1, 5 0.02    0.68  .006    .448\n2           activity 1, 5 0.01    0.30 &lt;.001    .609\n3 treatment:activity 1, 5 0.01 10.37 *  .037    .023\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '+' 0.1 ' ' 1\n\n\nNotes\n\nThe Greenhouse-Geiger (“GG”) correction is the default. While the correction = \"GG\" argument is not needed, it makes the script more transparent.\nFor these data the Greenhouse-Geiger correction doesn’t make a difference in the table.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Models for non-independence -- linear mixed models</span>"
    ]
  },
  {
    "objectID": "chapters/lmm.html#hidden-code",
    "href": "chapters/lmm.html#hidden-code",
    "title": "14  Models for non-independence – linear mixed models",
    "section": "14.15 Hidden code",
    "text": "14.15 Hidden code\n\n14.15.1 Import exp5c\n\ndata_from &lt;- \"Transcriptomic profiling of skeletal muscle adaptations to exercise and inactivity\"\nfile_name &lt;- \"41467_2019_13869_MOESM6_ESM.xlsx\"\nfile_path &lt;- here(data_folder, data_from, file_name)\n\nexp5c_wide &lt;- read_excel(file_path,\n                         sheet = \"Fig5c\",\n                         range = \"A2:M3\",\n                         col_names = FALSE) |&gt;\n  data.table() |&gt;\n  transpose(make.names = 1)\n\nNew names:\n• `` -&gt; `...1`\n• `` -&gt; `...2`\n• `` -&gt; `...3`\n• `` -&gt; `...4`\n• `` -&gt; `...5`\n• `` -&gt; `...6`\n• `` -&gt; `...7`\n• `` -&gt; `...8`\n• `` -&gt; `...9`\n• `` -&gt; `...10`\n• `` -&gt; `...11`\n• `` -&gt; `...12`\n• `` -&gt; `...13`\n\nactivity_levels &lt;- c(\"Basal\", \"EPS\")\ntreatment_levels &lt;- names(exp5c_wide)\nexp5c_wide[, activity := rep(activity_levels, each = 6)]\nexp5c_wide[, activity := factor(activity, levels = activity_levels)]\n\nexp5c &lt;- melt(exp5c_wide,\n              id.vars = \"activity\",\n              variable.name = \"treatment\",\n              value.name = \"glucose_uptake\")\nexp5c[, treatment := factor(treatment, levels = treatment_levels)]\n\nexp5c[, donor := rep(paste0(\"donor_\", 1:6), 4)]\n\n\n\n14.15.2 Import exp1g\n\ndata_from &lt;- \"A GPR174–CCL21 module imparts sexual dimorphism to humoral immunity\"\nfile_name &lt;- \"41586_2019_1873_MOESM3_ESM.xlsx\"\nfile_path &lt;- here(data_folder, data_from, file_name)\n\nexp1g_wide &lt;- read_excel(file_path,\n                         sheet = \"Fig 1g\",\n                         range = \"B4:E25\",\n                         col_types = c(\"numeric\"),\n                         col_names = FALSE) |&gt;\n  data.table()\n\ngenotype_levels &lt;- c(\"Gpr174+\", \"Gpr174-\")\nsex_levels &lt;- c(\"M\", \"F\")\ng.by.s_levels &lt;- do.call(paste, expand.grid(genotype_levels, sex_levels))\ncolnames(exp1g_wide) &lt;- g.by.s_levels\n\nexp_levels &lt;- paste0(\"exp_\", 1:4)\nexp1g_wide[, experiment_id := rep(exp_levels, c(5,6,6,5))] #check!\nexp1g_wide[, experiment_id := factor(experiment_id)] #check!\n\nexp1g &lt;- melt(exp1g_wide,\n              id.vars = \"experiment_id\",\n              measure.vars = g.by.s_levels,\n              variable.name = \"treatment\",\n              value.name = \"gc\") |&gt; # cell count\n  na.omit()\n\nexp1g[, c(\"genotype\", \"sex\"):= tstrsplit(treatment,\n                                             \" \",\n                                             fixed = TRUE)]\nexp1g[, genotype := factor(genotype,\n                           levels = genotype_levels)]\nexp1g[, sex := factor(sex,\n                           levels = sex_levels)]\n\nexp1g_means &lt;- exp1g[, .(gc = mean(gc)),\n                     by = .(treatment, genotype, sex, experiment_id)]",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Models for non-independence -- linear mixed models</span>"
    ]
  }
]