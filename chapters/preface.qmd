# Preface {.unnumbered}

This book is an introduction to the statistical analysis of data from biological experiments with a focus on the estimation of treatment effects and measures of the uncertainty of theses estimates. Instead of a flowchart of "which statistical test", this book emphasizes a **regression modeling** approach using **linear models** and extensions of linear models.

"What what? I learned from the post-doc in my lab that regression was for data with a continuous independent variable and that *t*-tests and ANOVA were for data with categorical independent variables." No! This misconception has roots in the history of regression vs. ANOVA and is reinforced by how introductory biostatistics textbooks, and their instructors, *choose* to teach statistics.

Classical linear regression, *t*-tests and ANOVA are all special cases of a **linear model**. The different linear models in this text are all variations of the equation for a line $Y = mX + b$ using slightly different notation:

$$
Y = \beta_0 + \beta_1 X
$$ {#eq-index-regression-model}

Chapter @sec-intro-linear-models explains the meaning of this notation. Here, just recognize that this is a **regression model**, but in modern statistics, we use this to not only estimate the effects of a continuous $X$ variable on some response (classical regression) $Y$ but also for the estimation of effects of a categorical treatment variable on some response (as in classical *t*-tests and ANOVA). A regression model with a categorical treatment variable is possible because the treatment variable is recoded into a numeric **indicator variable** indicating group membership ("wildtype" or "knockout"). Classical *t*-tests and ANOVA are equivalent to special cases of regression models but the ANOVA model is usually presented in a different way, one that allows simple "paper and pencil math" (addition, subtraction, multiplication, division). The linear model underneath a classical ANOVA is some variation of

$$
\overline{Y}_k = \mu + \alpha_k
$$ {#eq-index-anova-model}

where $\mu$ is the grand mean and $\alpha_k$ is the difference between the mean of treatment *k* and the grand mean.

In this text, I use **linear model** for any model that looks like @eq-index-regression-model and **ANOVA model** for any model that looks like @eq-index-anova-model (many statistics textbooks call these "cell-mean models"). It would be more specific to call models that look like @eq-index-regression-model a **regression model** but the word "regression" has a lot of baggage because of how it is taught -- as a method for data with continuous $X$ ("independent") variables. If I said, "this is a book of regression models for analyzing your experimental data", you might quite reasonably, but incorrectly, assume that the book wasn't really relevant to your experiments because your independent variables are categorical ("WT" vs."KO") and not continuous.

## Aren't *t*-tests and ANOVA good enough? {-}

For many experiments, the linear models advocated here will give the same *p*-value as a *t*-test or ANOVA, which raises the question, why bother with linear models? Some answers include

1. **Flexibility**. Linear models and extensions of linear models are all variations of $Y = \beta_0 + \beta_1 X$. Generalizations of this basic model include linear models with added covariates or multiple factors, generalized least squares models for heterogeneity of variance or correlated error, linear mixed models for correlated error or hierarchical grouping, generalized linear models for counts and other biological data that don't have normal distributions, generalized additive models for responses that vary non-linearly over time, causal graphical models for inferring causal effects with observational data, multivariate models for multiple responses, and some machine learning models. This book is not a comprehensive source for any of these methods but an introduction to the common foundations of all.
2. **Issues in inference**. Issues in inference from *t*-test/ANOVA occur when the data violate the assumptions of independence, homogeneity of variances, or a normal conditional response. Linear (regression!) models were expanded in different ways to specifically model these violations. Many of these models have no ANOVA equivalent -- consequently, researchers using ANOVA are forced to use the wrong model or kludgy alternatives, such as non-parametric tests.
3. **Gateway drug**. Many of the statistical models used in genomics are variations of the linear models introduced here. There will be a steep learning curve for these methods if your statistical training consists of *t*-tests, ANOVA, and Mann-Whitney non-parametric tests.
4. **Biologically meaningful focus**. Linear models encourage looking at, thinking about, and reporting estimates of the size of a treatment effect and the uncertainty of the estimate. The estimated treatment effect is the difference in the response between two treatments. If the mean plasma glucose concentration over the period of a glucose tolerance test is 15.9 mmol/l in the knockout group and 18.9 mmol/l in the wildtype group, the estimated effect is -3.0 mmol/l. The magnitude of this effect is our measure of the difference in glucose tolerance between the two treatments. What is the physiological consequence of this difference? Is this a big difference that would excite NIH or a trivial difference that encourages us to pursue some other line of research? I don't know the answers to these questions-- I'm not a metabolic physiologist. Researchers in metabolic physiology should know the answers, but if they do, they don't indicate this in the literature. Effect sizes are rarely reported in the experimental bench-biology literature.

What is reported are *p*-values. Small *p*-values give the researchers confidence that "an effect exists" and an abundance of small *p*-values from a series of experiments that have rigorously probed a system give the researchers confidence that they have discovered knowledge about some biological mechanism. This inference strategy works okay in bench biology. But, *p*-values can also create misconceptions. For example, extremely small *p*-values give some researchers the confidence that an effect is large or important. This confidence is unwarranted. *P*-values are not a useful measure of effect size.

*P*-values are neither necessary nor sufficient for good data analysis. But, a *p*-value is a useful tool in the data analysis toolkit. If the conduction of the experiment and analysis of the results closely approximate the model underlying the computation of the *p*-value, then a *p*-value dampens the frequency that we are fooled by randomness and gives a researcher some confidence in the direction (positive or negative) of an effect. Importantly, the estimation of effects and uncertainty and the computation of a *p*-value are not alternatives. Throughout this text, linear models are used to compute a *p*-value *in addition to* the estimates of effects and their uncertainty.

::: {.callout-note title="NHST Blues"}
The emphasis on *p*-values as *the* measure to report is a consequence of the "which statistical test?" strategy of data analysis. This practice, known as Null-Hypothesis Significance Testing (NHST), has been criticized by statisticians for many, many decades. Nevertheless, introductory biostatistics textbooks written by both biologists and statisticians continue to organize textbooks around a collection of hypothesis tests, with a great deal of emphasis on "which statistical test?" and much less emphasis on estimation and uncertainty. The NHST/which-statistical-test strategy of learning or doing statistics is easy in that it requires little understanding of the statistical model underneath the tests and its assumptions, limitations, and behavior. The NHST strategy in combination with point-and-click software enables [mindless statistics](https://www.sciencedirect.com/science/article/abs/pii/S1053535704000927){target="_blank"} and encourages the belief that statistics is a tool like a word processor is a tool, after all, a rigorous analysis of one's data requires little more than getting p-values and creating bar plots. Indeed, many PhD programs in the biosciences require no statistics coursework and the only training available to students is from the other graduate students and postdocs in the lab. As a consequence, the biological sciences literature is filled with error bars that imply data with negative values and p-values that have little relationship to the probability of the data given the experimental design. More importantly for science, the reported statistics are often not doing for the study what the researchers and journal editors think they are doing.
:::

## What is unusual about this book? {-}

1. **Real data sets from the recent experimental biology literature**. All of the data in this text are from articles in which the researchers have followed open science best practices and made the data freely available by archiving the data with the article. This not only let's me explain modern statistics relevant to experimental biologist's experiments using data collected by experimental biologists but also allows me to connect the statistical explanation with the goals of the experiment. A potential downside of using real data is that understanding any analysis requires some level of understanding of the underlying biology and the underlying methods of data collection. I try to give enough background biology to understand the motivation of the experiment and experimental methodology to understand the data.
2. Because I scan many, many articles in experimental biology looking for good archived data to use as examples in this text, I've developed an appreciation for the variation in how experimental biologists think about and do statistics. A consequence of this is the recognition of the many ways that researchers use non-best practices and occasionally worst practices. Much of the material in each chapter and one entire chapter ([Issues in inference](#issues)) is devoted to addressing the consequences of these non-best and worst practices. 
