# Models for non-independence -- linear mixed models {#sec-lmm}

```{r lmm-setup, echo=FALSE, message=FALSE, warning=FALSE}

library(here)
library(janitor)
library(readxl)
library(data.table)

# analysis packages
library(mvtnorm)
library(emmeans)
library(car) # qqplot, spreadlevel
library(afex)
library(lmerTest)
library(nlme)
library(quantreg)

# graphing and tabling packages
library(ggplot2) # ggplot environment
library(ggsci) # color palettes
library(ggpubr) # publication ready plots
library(cowplot) # combine plots
library(knitr)
library(kableExtra) #tables

source(here::here("R/ggplot_the_model.R"))
source(here::here("R/ggptm.R"))
source(here::here("simulations/grcbds/simulator.R"))

here <- here::here
clean_names <- janitor::clean_names
data_folder <- "data"
minus <- "\u2013"

here <- here::here
data_path <- "data"
image_folder <- "images"
```

```{r lmm-fig-sizes, echo=FALSE}
dpi <- 72
# width of bookdown page is 800 pix
# width of standard bookdown fig is 560 pix or 70% of page
std_width <- 504/dpi # 7 in
full_width <- 800/dpi
small_scale = 6/7
small_width <- std_width*small_scale # 6 in

# standard aspect ratio is .7 so
std_ar <- 5/7 # .71
response_ar <- .8 # for use with response plots with p-values
effect_ar <- 0.6 # for effects
harrell_ar <- 1 # for harrell effect & response plots

# dims (width, height)
small_dim <- c(small_width, small_width*std_ar)
std_dim <- c(std_width, std_width*std_ar)
response_dim <- c(std_width, std_width*response_ar)
effect_dim <- c(std_width, std_width*effect_ar)
harrell_dim <- c(std_width, std_width*harrell_ar)
full_dim_three_eights <- c(full_width, full_width * 3/8)
full_dim_three_eights <- c(full_width, full_width * 0.5)

# out.width percents
out.width_std <- paste0(std_width/full_width*100, "%")
out.width_small <- paste0(small_width/full_width*100, "%")

```


```{r lmer-check, echo=FALSE}
lmer_check <- function(fit){
  return(fit@optinfo$conv$lme4$messages)
}
```

```{r echo=FALSE}
varcor <- function(res){
  # vc is the value from VarCorr(m1)
  vc <- cov2cor(res)
  diag(vc) <- sqrt(diag(res))
  colnames(vc) <- rep(NA, ncol(vc))
  vc[upper.tri(vc)] <- NA
  return(vc)
}

```


```{r pless, echo=FALSE}
  pless <- function(x){
    value <- sum(x < 0.05, na.rm=TRUE)/length(na.omit(x))
    return(value)
  }

```

Probably no chapter in this book is more important for the best-practice analysis of experimental data than this chapter. Why? Because many if not most experimental data violates the assumption of independence and any analysis using standard *t*-tests and ANOVA will always lead to quantitative error in inference (confidence intervals and *p*-values) and often lead to qualitative errors in inference (statements about "significance"). Standard analysis of non-independent data can lead to absurdly liberal inference (the *p*-values are far lower than the data support), but can also lead to moderately conservative inference (the *p*-values are higher than the data supports). Liberal inference generates false discovery and lures researchers down dead-end research pathways. Conservative inference steers researchers away from true discovery.

**Linear Mixed Models** are an extension of linear models that appropriately adjust inferential statistics for non-independent data. **Paired t-tests** and **Repeated Measures ANOVA** are classical tests that are special cases of linear mixed models. Linear mixed models are more flexible than these classical tests because the models can include added covariates or more complex models generally. And, linear mixed models can be extended to **Generalized Linear Mixed Models** for counts, binary responses, skewed responses, and ratios.

Before introducing experimental designs that generate non-independent data and the models used to analyze these, let's explore two ubiquitous examples, one that leads to liberal inference and one that leads to conservative inference.

## Liberal inference from pseudoreplication

Researchers are interested in regulation and repair of DNA double-stranded breaks and use a proximity ligation assay (PLA) of HeLa cells to investigate the number of damage response events ("foci") per cell with and without an inhibitor of transcription elongation (DRB). The number of foci in each of fifty cells per treatment is measured. The experiment is replicated three times. The researchers use a *t*-test to compare the effect of DRB on foci count and naively include all measures in the analysis.

What is naive about the analysis? The fifty measures per cell are **technical replicates** and the values within a cell are not independent of each other because they share aspects of cell environment not shared by values in other cells. Including technical replicates in an analysis without accounting for this non-independence is a kind of **pseudoreplication**

To show how this naive analysis results in extremely liberal inference and an increase in false discovery, I simulate this experiment using a case in which there is no effect of DRB treatment, so a low *p*-value indicates a false-discovery. The simulation is simplified with the two following conditions: 1) the  model pretends that count data are normally distributed (this is because we want to focus on pseudoreplication and not a misspecified distribution) and 2) the model pretends that values from each treatment within an experiment are independent (this is because we want to focus on pseudoreplication).

```{r lmm-pseudorep-example, echo=FALSE, warning=FALSE, eval=TRUE}
# fig 1f https://www.nature.com/articles/s41586-024-07217-2
# experimental replicates = 3, technical replicates = 50
sigma_among <- 4
sigma_within = 4
n_sim <- 1000
fake_crds <- simulator(
  seed_i = 1,
  n_sim = n_sim,
  n_treat = 2, # number of treatment levels
  n_block = 1, # number of litters (blocks)
  n_rep = 3, # number of mice per litter:treatment
  n_ss = 50, # number of subsamples
  n_exp = 1, # number of experiments
  design = "pseudoreplicated",
  correlated_slopes = TRUE, # TRUE uses random int/slope model to generate data
  beta = c(20, 0),
  sigma_rep = sigma_among, # sd among experimental replicates of treatment:block
  sigma_ss = sigma_within, # sd among subsamples within replication of treatment:block. This is just sigma
  block_name = "litter",
  rep_name = "experiment",
  ss_name = "cell"
)

p <- numeric(n_sim)
for(i in 1:n_sim){
  form_i <- paste0(paste0("sim_", i), " ~ treatment") |>
    formula()
  m1 <- lm(form_i, data = fake_crds)
  p[i] <- coef(summary(m1))[2, 4]
}

type_1 <- paste0(sum(p < 0.05)/n_sim * 100, "%")
median_p <- format(round(median(p), 6), scientific = F)

p1 <- format(round(p[1], 6), scientific = F)

# type_1
# median_p
# p1
```

In this naive analysis of the experiment, the researcher finds an effect of treatment with a *p*-value of 0.000073 and uses this small *p*-value to justify a decision to move forward with follow-up experiments. But this low *p*-value is not supported by the data -- this discovery is false. This very small *p*-value is not an example of a "rare event". In fact, if the researcher repeats the experiment 1000 times, then the median *p*-value is 0.000195 and 72.3% of the 1000 *p*-values lead to the same false discovery if 0.05 is used to make the decision to move forward.

While this example is fake, I see this naive analysis a lot: tumor area of multiple tumors per mouse, islet area of multiple islets per mouse, number of vesicles docked to a membrane in multiple cells per mouse, the number of mitochondria in multiple cells per mouse, the number of neurites in multiple neurons in multiple cells per mouse, etc. etc. Indeed, when I'm looking for examples of pseudoreplication to teach, I just look for figures with a bunch of points per treatment -- something similar to this plot of the fake data experiment. Regardless, this is a huge source of false discovery that could disappear overnight.

```{r}
#| label: fig-lmm-pseudoreplication-fake-example
#| echo: false
#| warning: false
#| eval: true
#| fig-width: 4
#| fig-height: 3
#| fig-cap: "A fake example of pseudoreplication motivated by a real experiment."
m1 <- lm(sim_1 ~ treatment, data = fake_crds)
m1_emm <- emmeans(m1, specs = "treatment")
m1_pairs <- contrast(m1_emm, method = "revpairwise")
ggplot_the_response(m1, m1_emm, m1_pairs,
                    y_label = "PLA foci per cell")

```

## Conservative inference from failure to identify blocks

Researchers are investigating the regulation of the differentiation of stem blood cells into osteoclasts, which can cause osteoporosis if overactivated. The researchers randomly sample three mice from 6 litters and use a **littermate control** design: within a litter, one sib is assigned to control (CN), one to glucortacoid (GC) treatment, and one to buthionine sulphoximine (BSO) treatment. The researchers are investigating the mechanism of GC-induced osteoporosis and if the researcher's model is correct, then BSO should block this mechanism. The response variable is Bone Mineral Density. Following an ANOVA, the researchers report the unadjusted *p*-value for each pairwise comparison with the expectation that BSO will reverse the effect of GC on Bone Mineral Density.

```{r}
#| label: fig-lmm-failed-discovery-example
#| echo: false
#| warning: false
#| eval: true
#| fig-width: 8
#| fig-height: 3
#| fig-cap: "Results of the fake experiment. A) results using ANOVA using on initial six litters, B) published results with four additional litters, C) results using LMM on initial six litters"

# https://www.nature.com/articles/s41467-024-49159-3
sigma_among <- 0.11
sigma_within = 0.06
n_sim <- 1
fake_rcbd <- simulator(
  n_sim <- n_sim,
  seed_i = 1,
  n_treat = 3, # number of treatment levels
  n_block = 10, # number of litters (blocks)
  n_ss = 1, # number of subsamples
  design = "rcbd",
  beta = c(0.4, -0.12, 0),
  sigma_exp.block = sigma_among, # sd among exp:block (or block if n_exp = 1)
  sigma_exp.block.treat = c(0), # sd among exp:block:treat
  sigma_ss = sigma_within, # sd among subsamples within replication of treatment:block
  group_names = c("CN", "GC", "BSO"),
  block_name = "litter",
  rep_name = "mouse",
  ss_name = "ss"
)

fake_rcbd <- fake_rcbd[, .SD, .SDcols = c("litter", "mouse", "treatment","sim_1")]
# initial experiment, six litters only
lmm_litter <- paste0("litter_0", 1:6)
fake_rcbd_2 <- fake_rcbd[litter %in% lmm_litter,]

# what the researchers did
# initial experiment
m1 <- lm(sim_1 ~ treatment, data = fake_rcbd_2)
m1_emm <- emmeans(m1, specs = "treatment")
m1_pairs <- contrast(m1_emm, method = "revpairwise") |>
  summary(infer = TRUE)
# extra 4 litters
m1b <- lm(sim_1 ~ treatment, data = fake_rcbd)
m1b_emm <- emmeans(m1b, specs = "treatment")
m1b_pairs <- contrast(m1b_emm, method = "revpairwise") |>
  summary(infer = TRUE)

# a linear mixed model to account for shared variance. On initial experiment
m2 <- lmer(sim_1 ~ treatment + (1 | litter), data = fake_rcbd_2)
m2_emm <- emmeans(m2, specs = "treatment")
m2_pairs <- contrast(m2_emm, method = "revpairwise") |>
  summary(infer = TRUE)

# plots
set.seed(1)
gg1 <- ggplot_the_response(m1, m1_emm, m1_pairs, y_label = "Bone Mineral Density") +
  ggtitle("naive ANOVA\n(n = 6)")
set.seed(1)
gg2 <- ggplot_the_response(m1b, m1b_emm, m1b_pairs, y_label = "Bone Mineral Density") +
  ggtitle("naive ANOVA\n(n = 10)")
set.seed(1)
gg3 <- ggplot_the_response(m2, m2_emm, m2_pairs,
                           y_label = "Bone Mineral Density") +
  ggtitle("Linear Mixed Model\n(n = 6)")
plot_grid(gg1, gg2, gg3, ncol = 3, labels = "AUTO")
```

What is naive about the analysis? Any value measured from sibling mice within a litter are not independent of each other because the sibling mice share aspects of genetics and maternal environment not shared by mice in other litters. This shared variance adds correlated noise to data and failure to account for the shared variance will almost always lead to more conservative inference.

In the experiment, the results "looked" like the expected results if the model GC-induced osteoporosis were correct, but the *p*-value of the focal contrast (BSO - GC) was not < 0.05 (@fig-lmm-failed-discovery-example A). The researchers expanded the experiment, adding data from four more litters (@fig-lmm-failed-discovery-example B). Had the researchers analyzed the initial set of data using a statistical model that accounts for the shared variance within a litter (@fig-lmm-failed-discovery-example C), the researchers would have been satisfied and fewer mice would have been killed, fewer resources used, and more time to pursue continued probing of the mechanism. The statistical model used to take advantage of the littermate control design is a **linear-mixed model**. An experiment with littermate control is known as a **blocked design**.

::: {.callout-warning title="NHST Blues"}
NHST encourages peeking at the data to see if p < 0.05 for a focal contrast, and collecting more data if this isn't the case. Don't do this. Peeking makes the *p*-value non-valid and will increase the false discovery rate. Peeking is a variation of the multiple test problem. In clinical trials there are statistically rigorous methods for peeking, which allows a trial to stop early.
:::

The small *p*-value in the BSO - GC contrast using a linear mixed model is not an example of a "rare event". In fact, if the researchers repeated the experiment 1000 times, 89.1% of the BSO - GC *p*-values using the linear mixed model are less than 0.05 while only 29.9% of the *p*-values following classical ANOVA are less than 0.05.

```{r}
#| label: lmm-failed-discovery-example-sim
#| echo: false
#| warning: false
#| eval: false

# https://www.nature.com/articles/s41467-024-49159-3
sigma_among <- 0.11
sigma_within = 0.06
n_sim <- 1000
fake_rcbd <- simulator(
  n_sim <- n_sim,
  seed_i = 1,
  n_treat = 3, # number of treatment levels
  n_block = 6, # number of litters (blocks)
  n_ss = 1, # number of subsamples
  design = "rcbd",
  beta = c(0.4, -0.12, 0),
  sigma_exp.block = sigma_among, # sd among exp:block (or block if n_exp = 1)
  sigma_exp.block.treat = c(0), # sd among exp:block:treat
  sigma_ss = sigma_within, # sd among subsamples within replication of treatment:block
  group_names = c("CN", "GC", "BSO"),
  block_name = "litter",
  rep_name = "mouse",
  ss_name = "ss"
)

# make GC the ref to easily get the p-value
fake_rcbd[, treatment := factor(treatment, levels = c("GC", "BSO", "CN"))]
p_lm <- numeric(n_sim) # pvalue from lm
p_lmer <- numeric(n_sim) # pvalue from lmer
for(i in 1:n_sim){
  form_lm <- paste0(paste0("sim_", i), " ~ treatment") |>
    formula()
  m1 <- lm(form_lm, data = fake_rcbd)
  p_lm[i] <- coef(summary(m1))[2, 4]
  form_lmer <- paste0(paste0("sim_", i), " ~ treatment + (1 | litter)") |>
    formula()
  m2 <- lmer(form_lmer, data = fake_rcbd)
  p_lmer[i] <- coef(summary(m2))[2, 5]
  
}

power_lm <- paste0(sum(p_lm < 0.05)/n_sim * 100, "%")
power_lmer <- paste0(sum(p_lmer < 0.05)/n_sim * 100, "%")

power_lm
power_lmer

```

While this example is fake, I see this naive analysis a lot -- littermate controls is very, very common but other examples are too, including replicated experiments (each experiment is a block). There are certain instances where researchers do recognize non-independence and do use a paired *t*-test but the vast majority of blocked designs (occurring in almost all experimental biology papers) go unrecognized. This is a huge source of failed discovery that could disappear overnight.

## Introduction to models for non-independent data (linear mixed models)

This chapter is about models for **correlated error**, including linear models with added **random factors**, which are known as **linear mixed models**. In classical hypothesis testing, a **paired t-test**, **repeated measures ANOVA**, and **mixed-effect ANOVA** are equivalent to specific cases of linear mixed models. Linear mixed models are used for analyzing data composed of subsets -- or **batches** -- of data that were measured from the "same thing", such as multiple measures within a mouse, or multiple mice within a litter. Batched data results in **correlated error**, which violates a key assumption of linear models (and their "which test" equivalents) and muddles statistical inference unless the correlated error is modeled, explicitly or implicitly. In some experimental designs (**blocked designs**), failure to model the correlated error reduces precision and power, contributing to reduced rates of discovery or confirmation. In other designs (**nested designs**), failure to model the correlated error results in falsely high precision and low *p*-values, leading to increased rates of false discovery. The falsely high precision is due to **pseudoreplication**. I think it's fair to infer from the experimental biology literature, that experimental biologists don't recognize the ubiquitousness of batched data and correlated error. This is probably the biggest issue in inference in the field (far more of an issue than say, a *t*-test on non-normal data).

What do I mean by "batch" and how can correlated error both increase and decrease false discovery? Consider an experiment to measure pancreatic islet area in response to two experimental factors: $\texttt{genotype}$ (WT, KO) and $\texttt{treatment}$ (presence/absence of some drug believed to be an agonist of the knocked out protein). While it may seem like the data from this experiment should be analyzed using the ANOVA option in GraphPad Prism (or, as advocated in this book, a general linear model that is equivalent to the ANOVA), the best practice statistical model actually depends on the **experimental design**. Experimental design matters because different designs introduce different patterns of **correlated error** due to shared genetics and environment. Recall that inference from a linear model (including *t*-tests and ANOVA) assumes *independence* (Chapter xxx) -- that is, each response value has no relationship to any other value, other than that due to treatment. Lack of independence results in patterns of correlation among the residuals, or correlated error. 

Something like the first experiment below (Design 1) is the necessary design to use the statistics that have been covered in this book to this point, without extreme violation of the independence assumption. But many (most?) experiments in experimental bench biology do not look like the design in Design 1 below. Instead, many (most?) experiments are variants of Designs 2-4, all of which have extreme violations of the independence assumption. Interestingly, some of these violations result in conservative statistics and reduced, true discovery rate while others result in liberal statistics and increased, false discovery rate.

Design 1. The design in @fig-lmm-design-1 is a factorial design with two factors, $\texttt{genotype}$ and $\texttt{treatment}$, each with two levels. Twenty mice of the same sex, each from a different litter from a unique dam and sire mating, are randomly sampled and assigned to one $genotype \times treatment$ combination (five mice per combination). All mice are housed individually (20 cages). The pancreatic tissue from all mice is prepared in a single batch and the area of a single islet is measured from each mouse. The entire experiment is carried out at the same time and each component (tissue preparation, measuring) is carried out by the same person (these could be different people for each component). This is a **Completely Randomized Design** (CRD). The five replicate mice per treatment are **treatment replicates** (often called **biological replicates** in experimental biology. A CRD does not have batched data.

![Design 2: Randomized Complete Block Design. Image created with BioRender.com](../images/experimental_designs/lmm-biological-replicates 500 W.png){width=400 #fig-lmm-design-2}

Design 2. In the design in @fig-lmm-design-2, four littermates are randomly sampled from five litters, each with a different dam and sire. Within each litter, mice are randomly assigned to each of the four treatment combinations (one per combination). Each litter is randomly assigned to cage with only a single litter per cage. All other aspects of this design are as in Design 1. This is a **Randomized Complete Block Design**. The five replicate mice per treatment are the treatment replicates. Each litter/cage combination is a type of batch called a **block**. A blocked design typically functions to reduce noise in the model fit (this increases power) and to reduce the number of litters and cages needed for an experiment. The four measures of Islet Area within a litter/cage (one per mouse) are not independent of each other. Each cage has four mice from the same litter and these mice share genetic and maternal factors that contribute to mouse anatomy and physiology that are not shared by mice in other litters. Additionally, each cage has a unique set of environmental factors that contribute to the error variance of the measure of the response. Each cage shares a cage-specific history of temperature, humidity, food, light, interactions with animal facilities staff, and behavioral interactions among the mice. All response measures within a litter/cage share the component of the error variance unique to that litter/cage and, as a consequence, the error (residuals) within a litter/cage are more similar to each other than they are to the residuals among litters/cages.

![Design 3: Randomized Complete Block Design with subsampling. Image created with BioRender.com](../images/experimental_designs/lmm-technical-replicates 500 W.png){width=400 #fig-lmm-design-3}

Design 3. The design in @fig-lmm-design-3 is exactly like that in Design 2, except that the researchers take three measures of iselet area  per mouse. The three measures are **subsampled replicates**. Experimental biologists often call these **technical replicates**, especially when the multiple measures are taken from the same preparation. Subsampling is a kind of **nested design** in which one variable is nested within (as opposed to crossed with) another variable. Here, the subsampled variable (subsample_id) is nested within the mouse_id variable. In addition to each litter/cage being a batch, each mouse is a batch. Each mouse has a unique set of factors that contribute to the error variance of the measures of the response in that mouse. All response measures within a mouse share the component of the error variance unique to that mouse and, as a consequence, the error (residuals) within a mouse are more similar to each other than they are to the residuals between mice

![Design 4: Segregated design. Don't do this! Image created with BioRender.com](../images/lmm-pseudoreplication.png){width=400 #fig-lmm-design-4}

Design 4. The design in @fig-lmm-design-4 is a variation of Design 2, but the five treatment replicates of each combination are housed together in the same cage. In this design, each litter is a batch and each cage is a batch but these are different batches, unlike Design 2.

In each of these experiments, there is systematic variation at multiple levels: among treatments due to treatment effects and among batches due to **batch effects**. Batches come in lots of flavors, including experiment, litter, cage, flask, plate, slide, donor, and individual. The among-treatment differences in means are the **fixed effects**. The among-batch differences are the **random effect**. An assumption of modeling random effects is that the batches are a random sample of the batches that could have been sampled. This is often not strictly true as batches are often **convenience samples** (example: the human donors of the Type 2 diabetes beta cells are those that were in the hospital).

The variation among batches/lack of independence within batches has different consequences on the uncertainty of the estimate of a treatment effect. The batches in Experiment 1 contain all treatment combinations. The researcher is interested in the treatment effect but not the variation due to differences among the batches. The batches are nuissance factors that add additional variance to the response, with the consequence that estimates of treatment effects are less precise, unless the variance due to the batches is explicitly modeled. **Modeling a batch that contains some or all treatment combinations will increase precision and power**.

Batches that contain at least two treatment combinations are known as **blocks**. A block that contains all treatment combinations is a **complete block**. A block that contains fewer than all combinations is an **incomplete block**. Including block structure in the design is known as **blocking**. Blocks are non-experimental factors. Adding a blocking factor to a statistical model is used to increase the precision of an estimated treatment effect. Design 2 is an example of a **randomized complete block design**.

In Design 3, there are multiple measures per mouse and the design is a **randomized complete block design with subsampling**. The subsampling is not the kind of replication that can be used to infer the among treatment effect because the treatment assignment was not at the level of the subsamples. The **treatment replicates** are the litters/cages, because *it was at this level that treatment assignment was randomized*. A statistical analysis of all measures from a subsampled design without modeling the correlated error due to the subsampling is a kind of [pseudoreplication](https://en.wikipedia.org/wiki/Pseudoreplication){target="_blank"}. Pseudoreplication results in falsely precise standard errors and false small *p*-values and, consequently, increased rates of false discovery.

In Design 4, the treatment is randomized *to* batch, so each batch contains only a single treatment level. In these **segregated** experimental designs, the variation among batches that arises from non-treatment related differences among batches **confounds** the variation among batches due to a true treatment effect. Design 4 is an extreme example of this -- there is only a single cage with a specific treatment combination. Imagine 1) the true effect of a treatment combination is zero and 2) an aggressive mouse in the control cage stimulates the stress response in the other mice and this stress response has a large effect on the value of the response variable measured by the researchers. The researcher is fooled into thinking that the treatment caused the difference in the response.

In all of these designs, it is important for the researcher to identify the **experimental unit** and the **measurement** unit. The experimental unit is the entity that was randomly assigned the treatment. In designs 1 -- 3, the experimental unit is the mouse. In experiment 4, the experimental unit is the cage. The measurement unit is the entity that was measured. In designs 1, 2, and 4, the measurement unit is the mouse. In design 3, the measurement unit is specific islet that was measured.

::: {.callout-note title="Pseudoreplication"}
In pseudoreplication, the degrees of freedom used to compute the test statistic and the *p*-value are inflated given the experimental design and research question. An example: A researcher wants to investigate the effect of some protein on mitochondrial biogenesis and designs an experiment with a wildtype (WT) and a conditional knockout (KO) mouse. Mitochondrial counts from twenty cells in one WT mouse and one KO mouse are measured and the researcher uses a *t*-test to compare counts. The sample size used to compute the standard error in the denominator of the *t*-value is 20. The *t*-distribution used to compute the *p*-value uses 38 df (20 measures times two groups minus two estimated parameters). This is wrong. The df are inflated and the estimate of the standard error of the difference (the denominator of the t-value) is falsely small. The correct sample size for this design is 1 and the correct df is zero. The sample size and df are inflated for this design because the treatment was randomized to mouse and not to cell. Mouse is the experimental unit -- the number of experimental units is what gives the degrees of freedom. The df are correct for inference about the two individuals (how compatible are the data and a model of sampling from the same individual?), but not for inference about the effect of genotype. We cannot infer anything about genotype with a sample size of 1, even with 20 measures per mouse, because any effect of treatment is completely confounded with other differences between the two mice.
:::

## Experimental designs in experimental bench biology

Given the basic principles above, let's consider the kinds of experimental designs seen in experimental bench biology (@fig-lmm-designs).

![](../images/experimental_designs/designs_1.png){width=800}

![Experimental designs in experimental bench biology. Images created with BioRender.com](../images/experimental_designs/designs_2.png){width=550 #fig-lmm-designs}


### Completely Randomized Design (CRD)

The **Completely Randomized Design** experiment in @fig-lmm-designs A has a single factor, $\texttt{treatment}$ with two levels ("Cn" and "Tr"). Five mice are randomly assigned to each treatment level. Each mouse is bred from a different litter and housed in a separate cage. The researchers measure a single value of the response variable from each mouse. The five replicate mice per treatment are the **treatment (biological) replicates**. The design is completely randomized because there is no subgrouping due to batches. What kinds of subgrouping does this design avoid?

By using a single mouse per litter, there are no litter batches and subsets of mice don't share **litter effects** -- common litter responses to the Cn or Tr treatments. Each litter has a unique set of factors that contribute to the error variance of the measure of the response. Siblings from the same dam and sire share more genetic variation than non-siblings and this shared genetic variation contributes to phenotypes (including the response to treatment) that are more likely to be similar to each other than to non-siblings. Siblings from the same litter share the same history of maternal factors (**maternal effects**, including **epigenetic effects**) specific to the pregnancy and even the history of events leading up to the pregnancy. This shared non-genetic and epigenetic variation contributes to phenotypes (including the response to treatment) that are more likely to be similar to each other than to non-siblings. All response measures within a litter share the genetic, maternal environmental, and epigenetic components of the error variance unique to that litter and, as a consequence, the error (residuals) within a litter are more similar to each other than they are to the residuals between litters.

By housing each mouse in it's own cage, there is no cage batch and subsets of mice don't share **cage effects** -- common cage responses to the Cn or Tr treatments. As stated earlier, each cage has a unique set of factors that contribute to the error variance of the measure of the response. Each cage shares a cage-specific history of temperature, humidity, food, light, interactions with animal facilities staff and behavioral interactions among the mice. All response measures within a cage share the component of the error variance unique to that cage and, as a consequence, the error (residuals) within a cage are more similar to each other than they are to the residuals between cages.

Examples:

1. Ten mice from separate litters are sampled. Five mice are randomly assigned to control. Five mice are randomly assigned to treatment. A single measure per mouse is taken. Mouse is the experimental unit. $t=2$, $b=0$, $r=5$, and $s=1$.
2. Ten cell cultures are created. Five cultures are randomly assigned to control and five to treatment. A single measure per culture is taken. Culture is the experimental unit. $t=2$, $b=0$, $r=5$, and $s=1$.

::: {.callout-note title="Known Unknowns"}
While most data from experiments in bench biology are analyzed as if the experimental design is a CRD, a good question is, what fraction of these actually are CRD? We know that many (most?) mouse experiments will have batched measures and correlated responses because most experiments are conducted with multiple mice per litter and/or cage (or equivalents in other model systems). Many cell culture data come from multiple replicates of the whole experiment -- the experiment functions as a block. And time series experiments include multiple measures on the same experimental unit over time. Except for time series experiments, researchers in experimental bench biology are using almost exclusively statistical tests that assume independence of errors (the tests appropriate for CRDs). How this mismatch between experimental design and statistical practice affects the rate of false and true discovery in cell and molecular biology is entirely unknown.
:::

### Completely Randomized Design with Subsampling (CRDS) or "Nested Design"

The **Completely Randomized Design with Subsampling** experiment in @fig-lmm-designs B is exactly like the CRD except that the researchers measure multiple values of the response variable from each mouse *under the same condition* (that is, not different in treatment or time). The multiple measures are **subsampled (technical) replicates**.

1. Do not confuse subsampled replicates with measures of the response under different conditions in the same mouse, for example a measure from one brain slice under the control treatment and a measure from a second brain slice under the drug treatment. This example is a kind of Randomized Complete Block Design, which is outlined next and the core design in this chapter.
2. Do not confuse subsampled replicates with a measures of the response at different times in the same mouse, for example, the plasma glucose levels at baseline and at five post-baseline time points. This example is a kind of longitudinal design, which is outlined below and more thoroughly in the chapter [Linear models for longitudinal experiments](#pre-post).
3. Do not confuse subsampled replicates with measures of different response variables from the same mouse, for example measures of the weights of five different skeletal muscles. This example is a kind of **multiple response** which is addressed xxx.

The technical replicates are a kind of **pseudoreplication**. The general linear model `y ~ treatment` fit to these data, including t-tests and traditional ANOVA, will have falsely high precision and falsely low *p*-values.

Examples:

1. Ten mice from separate litters are sampled. Five mice are randomly assigned to control. Five mice are randomly assigned to treatment. Multiple measures per mouse are taken. Example: five measures of Islet Area are measured in each pancreas. Mouse is the experimental unit. Each of the measures is a technical replicate because the treatment is not randomly assigned to each islet but to the whole mouse. $t=2$, $b=0$, $r=5$, and $s=5$.
2. Ten cell cultures are created. Five cultures are randomly assigned to control and five to treatment. Multiple measures per culture are taken. Example: Mitochondrial counts are measured from five cells in each culture. Culture is the experimental unit. Each of the counts is a technical replicate because the treatment is not randomly assigned to each cell but to the whole culture. $t=2$, $b=0$, $r=5$, and $s=5$.

Notes:

1. Subsampling can occur at multiple levels. Example: Ten mice from separate litters are sampled. Five mice are randomly assigned to control. Five mice are randomly assigned to treatment. Five neurons in a slice of brain tissue are identified in each mouse. From each neuron, the length of five dendrite spines are measured. The five measures of spine length are "nested within" neuron and the five neurons are "nested within" mouse. Nested subsampling can quickly lead to massive pseudoreplication and false discovery.

### Randomized Complete Block Design (RCBD)

The **Randomized Complete Block Design** experiment in @fig-lmm-designs C is similar to the CRD except that all treatment combinations (two here) are randomly assigned to sibling mice within a litter. Here, two mice from each litter are randomly selected and one is randomly assigned to "Cn" and the other to "Tr". Each litter is randomly assigned to a unique cage. The researchers measure a single value of the response variable from each mouse. The five replicate mice per treatment are the treatment replicates. The litters (or cage) are the blocks. In this design, litter and cage effects are confounded but this has no consequence on the statistical model and inference unless the researchers want to explicitly estimate these effects separately. Compared to the CRD, this design requires fewer resources (five litters instead of ten, five cages instead of ten). Compared to the general linear model fit to data from the CRD (`y ~ treatment`), including t-tests and traditional ANOVA, the linear mixed model fit to the RCBD has increased precision and power. While many researchers seem to be designing experiments similar to this ("littermate controls"), most are failing to fit a statistical model that accounts for the batching and taking advantage of the increased precision and power. 

Examples:

1. Ten mice are sampled. In each mouse, one forelimb is assigned to control and the other forelimb is assigned to treatment. Only a single measure on each side is taken. Limb is the experimental unit. Mouse is a block. $t=2$, $b=10$, $r=1$, and $s=1$.
2. Ten litters are sampled. In each litter, one sib is assigned to control and the other sib is assigned to treatment. Only a single measure on each sib is taken. Mouse is the experimental unit. Litter is a block. $t=2$, $b=10$, $r=1$, and $s=1$.
3. Two mice, each from a separate litter are sampled. One is randomly assigned to control and the other to treatment. Only a single measure of the response variable is taken per mouse. The experiment is replicated five times (five different days, each with a newly made set of reagents and machine calibrations). Mouse is the experimental unit. Experiment is a block. $t=2$, $b=5$, $r=1$, and $s=1$.

### Randomized Split Plot Design (RSPD)

The **Randomized Split Plot Design** experiment in @fig-lmm-designs D is similar to the RCBD except that there is now a second experimental factor that is crossed with the first experimental factor. An individual mouse acts as single experimental unit for one factor (here, $\texttt{genotype}$ with levels "WT" and "KO") but acts as a block for the second experimental factor (here, $\texttt{treatment}$ with levels "Cn" and "Tr"). The first factor ($\texttt{genotype}$) is the **main plot** -- the levels of the factor are randomly assigned to the main plots. The second factor ($\texttt{treatment}$) is the **subplot** -- the levels of this factor are randomly assigned to the subplots. $\texttt{Litter}$ is a replicated block. Also in Figure \@ref(fig:lmm-designs)D, a 2 x 2 RCBD with the same two experimental factors is shown for comparison. In the 2 x 2 RCBD, four mice per block (litter) are each randomly assigned one of the 2 x 2 combinations of $\texttt{genotype}$ and $\texttt{treatment}$).

### Generalized Randomized Complete Block Design (GRCBD)

The **Generalized Randomized Complete Block Design** experiment in @fig-lmm-designs E is similar to the RCBD except that two treatment replicates per block (litter/cage) are assigned. 

**Important and somewhat not intuitive** Because the treatment replicates within a litter share common error variance, these do not act like independent replicates. One consequence of this is, the sample size ($n$) is five and not ten ($litters \times treatment replicates$). The general linear model `y ~ treatment` fit to these data, including t-tests and traditional ANOVA, will generally have falsely high precision and falsely low *p*-values.

### Nested Randomized Complete Block Design (NRCBD)

The **Nested Randomized Complete Block Design** experiment in @fig-lmm-designs F is similar to the RCBD except that there are now replicated experiments. This is a nested block design with litter (a block) nested within experiment (a block).

### Longitudinal Randomized Complete Block Design (LRCBD)

The **Longitudinal Randomized Complete Block Design** experiment in @fig-lmm-designs G is similar to the RCBD except that there are multiple measures of the response variable taken, each taken at a different time point, including **baseline** (time zero).

### Variations due to multiple measures of the response variable

Similar to the CRDS above, the other basic designs can include subsampling, resulting in, for example, RCBDS or RSPDS. If there is subsampling within subsampled units, then we can designated these with "SS", for example RCBDSS.

Similar to the LRCBD above, the other basic designs can include longitudinal sampling, resulting in, for example, LCRD or LRSPD.

## Building the linear (mixed) model for clustered data

Notation

1. i = 1..t (treatments)
2. j = 1..b (blocks)
3. k = 1..r (experimental replications within a block)
4. m = 1..s (subsamples or technical replicates)


$$y_i = \beta_0 + \beta_i treatment_i + (\gamma_j block_j) + (\gamma_{ij} block_j treatment_i) + \varepsilon$$

## Statistical models for experimental designs
### Models for Completely Randomized Designs (CRD)

```{r, class.source = "fold-show", eval=FALSE}
lm0 <- lm(y ~ treatment,
             data = figx)
```


### Models for batched data (CRDS, RCBD, RSPD, GRCBD, NRCBD)
#### Linear models
##### fixed effect model

```{r, class.source = "fold-show", eval=FALSE}
lm1 <- lm(y ~ treatment + block,
             data = figx)
```

##### linear model of batch means

```{r, class.source = "fold-show", eval=FALSE}
figx_means <- figx[, .(y = mean(y)),
               by = .(treatment, batch)]
lm2 <- lm(y ~ treatment,
             data = figx_means)
```

####  linear mixed models ("random effects" models)
##### Random intercept model

```{r, class.source = "fold-show", eval=FALSE}
lmm1 <- lmer(y ~ treatment + (1 | block),
              data = figx)
```

##### lmm for correlated error

```{r, class.source = "fold-show", eval=FALSE}
lmm2 <- lme(y ~ treatment,
             random = ~1 | block,
             correlation = corSymm(form = ~ 1 | block),
             weights = varIdent(form = ~ 1 | treatment),
             data = figx)
```

##### random intercept and slope model

```{r, class.source = "fold-show", eval=FALSE}
lmm3 <-  lmer(y ~ treatment + (treatment | block),
               data = figx)
```

##### random interaction intercept model

```{r, class.source = "fold-show", eval=FALSE}
lmm4 <-  lmer(y ~ treatment + (1 | block) + (1 | block:treatment),
               data = figx)

```

##### random interaction model with subsampling

```{r, class.source = "fold-show", eval=FALSE}
lmm5 <-  lmer(y ~ treatment + (1 | block) + (1 | block:treatment) + (1 | block:treatment:replicate),
               data = figx)
```

##### CRD split plot model

```{r, class.source = "fold-show", eval=FALSE}
# tr1 is the main plot
# tr2 is the subplot
# main_plot is tr1:rep, where rep is the rep id
lmm6 <-  lmer(y ~ tr1 * tr2 + (1 | main_plot),
               data = figx)

```

##### RCBD split plot model

```{r, class.source = "fold-show", eval=FALSE}
# tr1 is the main plot
# tr2 is the subplot
# main_plot is tr1:block
lmm7 <-  lmer(y ~ tr1 * tr2 + (1 | block) + (1 | block:tr1),
               data = figx)

lmm7 <-  lmer(y ~ tr1 * tr2 + block + (1 | block:tr1),
               data = figx)
# the two versions are numerically equivalent
```
            
#### ANOVA models ("mixed models", "repeated measures ANOVA")
##### Multivariate repeated measures ANOVA

```{r, class.source = "fold-show", eval=FALSE}
aov1 <- aov_4(y ~ treatment + (treatment | block),
             data = figx)
```

##### Univariate repeated measures ANOVA

```{r, class.source = "fold-show", eval=FALSE}
aov2 <- aov_4(y ~ treatment + (treatment | block),
           include_aov = TRUE,
             data = figx)
```

##### Pairwise paired, t-tests

```{r, class.source = "fold-show", eval=FALSE}
pptt <- pairwise_t_tests(y_col = "y",
                         g_col = "treatment",
                         id_col = "block",
                         data = figx)
```

## Which model and why?

### CRD

1. No batch to model!

### CRDS

1. lmm1 -- random intercept model
2. lm2 -- linear model of batch means 

Notes

1. These are numerically equivalent and should result in same estimates, SE, CIs, and p-values
2. These are equivalent to a **Nested t-test** or **Nested ANOVA**

### RCBD

Reasonable models:

1. lmm1 -- random intercept model
2. lmm2 -- lmm for correlated error
3. lm1 -- fixed effect model
4. aov1 -- multivariate RM-ANOVA
5. aov2 -- univariate RM-ANOVA
6. pptt -- pairwise, paired t-test

(Cannot use lmm3 or lmm4 because there is no replication of each block:treatment combination.)

Assumptions:

1. lmm1, lm1, and aov2 assume
    * **compound symmetry**.
    * **sphericity**.
2. lmm2, aov1, and pptt do not assume compound symmetry and sphericity.

Notes:

1. lmm1 is the standard. If the number of treatments = 2, then lmm1 is eqivalent with a paired t-test.
2. If the design is balanced (all blocks have a single value for both treatments) AND the number of treatments = 2 then *all six methods are numerically equivalent*.
3. If the design is balanced AND the number of treatments > 2, then
    * lmm1, lm1, and aov2 are numerically equivalent
    * lmm2, aov1, pptt result in the same estimates and SE but lmm2 has more df, so the CIs, and p-values of lmm2 are less conservative.
4. If the number of treatments = 2 AND the design is not balanced (at least one block is missing value for one treatment level) then the linear models (lm1, aov1, aov2, pptt) are equivalent but the linear mixed models (lmm1, lmm2) differ from each other and from the linear models.
5. If a treatment within a block is missing, the whole block is deleted in the RM-ANOVA models. This reduces power (the loss of power depends partly on the number of missing values).
6. If a treatment within a block is missing, the block is deleted only in the comparisons including the missing treatment in the pairwise, paired t-tests. This makes pptt more powerful than aov1.
7. the lmm models are very flexible -- covariates can be added or these can be used as generalized lmms for non-normal distributions.
8. the lmm models and especially lmm2 sometimes (often?) fail to converge, especially with small samples (small number of blocks) or if the among-block component of variance is small.

Performance:

```{r rcbd-performance-sim, warning=FALSE, message=FALSE, error=FALSE, echo = FALSE}
seed_starter <- 1
n_sims <- 1000
do_sim <- FALSE

  data_from <- "simulations/grcbds/data"
  outfile_name <- "rcbd.Rds"
  save_file_path <- here(data_from, outfile_name)


param_sets <- 3
treatment_levels <- c("Cn", "Tr1", "Tr2")
model_levels <- c("lm0", "lm1", "aov1", "aov2", "pptt", "lmm2")
  
p1_mat <- matrix(as.numeric(NA), nrow = n_sims, ncol = length(model_levels))
p2_mat <- matrix(as.numeric(NA), nrow = n_sims, ncol = length(model_levels))
p3_mat <- matrix(as.numeric(NA), nrow = n_sims, ncol = length(model_levels))
r_mat <- matrix(as.numeric(NA), nrow = n_sims, ncol = 3)

# parameters
beta_i = c(10, 0, 0)

# each row is different param_set
sigma_exp.block.treat_matrix <- matrix(c(
  c(2, 1, 0.2),
  c(0.5, .25, 0.05),
  c(0, 0, 0),
  c(0, 0, 0)
), nrow = 3, ncol = 4) %>%
  t()
colnames(sigma_exp.block.treat_matrix) <- treatment_levels

# each value is different param_set
sigma_exp.block_list <- c(1, 0.25, 0.8, 0)

# each value is different param_set
sigma_ss_list <- c(.1, .4, .4, 1)

param_set_matrix <- data.table(
  sim_id = 1:length(sigma_exp.block_list),
  sigma_block = sigma_exp.block_list,
  sigma_exp.block.treat_matrix,
  sigma_ss = sigma_ss_list
)

res_table <- matrix(as.numeric(NA), nrow = nrow(param_set_matrix), ncol = 15)
sim_table <- data.table(NULL)
big_table <- data.table(NULL)

if(do_sim == TRUE){
  for(param_set in 1:nrow(param_set_matrix)){
    sigma_exp.block_i <- param_set_matrix[param_set, sigma_block]
    sigma_exp.block.treat_i <- param_set_matrix[param_set, .SD, .SDcols = treatment_levels] %>% as.numeric()
    sigma_ss_i <- param_set_matrix[param_set, sigma_ss]
    sigma_rep_i <- 0.1
    sigma_exp.treat_i <- 0
    sigma_exp_i <- 0
    n_treat_i <- 3
    n_block_i <- 10
    n_rep_i <- 1
    n_ss_i <- 1
    fake_rcbd <- simulator(
      seed_i = seed_starter,
      n_sim = n_sims,
      n_treat = n_treat_i, # number of treatment levels
      n_block = n_block_i, # number of litters (blocks)
      n_rep = n_rep_i, # number of mice per litter:treatment
      n_ss = n_ss_i, # number of subsamples
      design = "rcbd",
      correlated_slopes = FALSE, # TRUE uses random int/slope model to generate data
      beta = beta_i,
      gamma = c(1, 1, 0), # sd of random intercept and slopes for non-ref
      rho = 0.0, # r between random intercept and slopes
      sigma_exp = sigma_exp_i, # sd among experiments
      sigma_exp.block = sigma_exp.block_i, # sd among exp:block (or block if n_exp = 1)
      sigma_exp.treat = sigma_exp.treat_i, # sd among exp:treat
      sigma_exp.block.treat = sigma_exp.block.treat_i, # sd among exp:block:treat
      sigma_rep = sigma_rep_i,
      sigma_ss = sigma_ss_i, # sd among subsamples within replication of treatment:block
      # sigma_exp = 0, # sd among experiments
      # sigma_exp.block = 0.1, # sd among exp:block (or block if n_exp = 1)
      # sigma_exp.treat = 0, # sd among exp:treat
      # sigma_exp.block.treat = c(0.1, 0.1, 0.1), # sd among exp:block:treat
      # sigma_rep = 0.1,
      # sigma_ss = 1, # sd among subsamples within replication of treatment:block
      block_name = "experiment",
      rep_name = "mouse",
      ss_name = "ss",
    )
    
    for(i in 1:n_sims){
      
      # models
      form_lm0 <- paste(paste0("sim_", i), "~ treatment") %>%
        formula()
      form_lm1 <- paste(paste0("sim_", i), "~ treatment + experiment") %>%
        formula()
      form_aov <- paste(paste0("sim_", i), "~ treatment + (treatment | experiment)") %>%
        formula()
      lm0 <- lm(form_lm0, data = fake_rcbd)
      lm1 <- lm(form_lm1, data = fake_rcbd)
      lmm2 <- lme(form_lm0,
                  random = ~1 | experiment,
                  correlation = corSymm(form = ~ 1 | experiment),
                  weights = varIdent(form = ~ 1 | treatment),
                  data = fake_rcbd) %>%
        try()
      if(inherits(lmm2, "try-error")){
        lmm2 <- NULL
      }
      
      # aov2 used for both multivariate (aov1) and univariate (aov2)
      aov2 <- aov_4(form_aov,
                    fun_aggregate = mean,
                    include_aov = TRUE,
                    data = fake_rcbd)
      fake_rcbd_means <- fake_rcbd[, .(sim_1 = mean(get(paste0("sim_", i)))),
                                     by = .(treatment, experiment)]
      
      pptt_pairs <- pairwise_t_tests(y_col = "sim_1",
                              g_col = "treatment",
                              id_col = "experiment",
                              data = fake_rcbd_means)
      
      lm0_pairs <- emmeans(lm0, specs = "treatment") %>%
        contrast(method = "revpairwise", adjust = "none")
      lm1_pairs <- emmeans(lm1, specs = "treatment") %>%
        contrast(method = "revpairwise", adjust = "none")
      aov1_pairs <- emmeans(aov2, specs = "treatment", model = "multivariate") %>%
        contrast(method = "revpairwise", adjust = "none")
      aov2_pairs <- emmeans(aov2, specs = "treatment", model = "univariate") %>%
        contrast(method = "revpairwise", adjust = "none")
      
      fake_rcbd[, y_res := residuals(lm0)]
      fake_rcbd_wide <- dcast(fake_rcbd,
                               experiment + rep ~ treatment,
                               value.var = "y_res")
      residual_correlations <- cor(fake_rcbd_wide[, .SD, .SDcols = treatment_levels])
      r_mat[i, 1] <- residual_correlations[2,1]
      r_mat[i, 2] <- residual_correlations[3,1]
      r_mat[i, 3] <- residual_correlations[3,2]
      
      lm0_p <- summary(lm0_pairs)[, "p.value"]
      lm1_p <- summary(lm1_pairs)[, "p.value"]
      aov1_p <- summary(aov1_pairs)[, "p.value"]
      aov2_p <- summary(aov2_pairs)[, "p.value"]
      pptt_p <- pptt_pairs[, p.value]
      
      p1_mat[i, 1] <- lm0_p[1]
      p1_mat[i, 2] <- lm1_p[1]
      p1_mat[i, 3] <- aov1_p[1]
      p1_mat[i, 4] <- aov2_p[1]
      p1_mat[i, 5] <- pptt_p[1]
      
      p2_mat[i, 1] <- lm0_p[2]
      p2_mat[i, 2] <- lm1_p[2]
      p2_mat[i, 3] <- aov1_p[2]
      p2_mat[i, 4] <- aov2_p[2]
      p2_mat[i, 5] <- pptt_p[2]
      
      p3_mat[i, 1] <- lm0_p[3]
      p3_mat[i, 2] <- lm1_p[3]
      p3_mat[i, 3] <- aov1_p[3]
      p3_mat[i, 4] <- aov2_p[3]
      p3_mat[i, 5] <- pptt_p[3]
      
      if(!is.null(lmm2)){
        lmm2_pairs <- emmeans(lmm2, specs = "treatment") %>%
          contrast(method = "revpairwise", adjust = "none")
        p1_mat[i, 6] <- summary(lmm2_pairs)[1, "p.value"]
        p2_mat[i, 6] <- summary(lmm2_pairs)[2, "p.value"]
        p3_mat[i, 6] <- summary(lmm2_pairs)[3, "p.value"]
      }else{
        p1_mat[i, 6] <- lm1_p[1]
        p2_mat[i, 6] <- lm1_p[2]
        p3_mat[i, 6] <- lm1_p[3]
      }

      
    }
    
    sim_table <- cbind(r_mat, p1_mat, p2_mat, p3_mat)
    sim_table_names <- paste(rep(model_levels, 3),
          rep(c("contrast_1", "contrast_2", "contrast_3"), each = length(model_levels)),
          sep = "_")
    colnames(sim_table) <- c("r21", "r31", "r32", sim_table_names)
    
    big_table <- rbind(
      big_table,
      data.table(
        sim_id = param_set,
        n_treat = n_treat_i,
        n_block = n_block_i,
        n_rep = n_rep_i,
        n_ss = n_ss_i,
        sigma_block = sigma_exp.block_i,
        sigma_block.treat_1 = sigma_exp.block.treat_i[1],
        sigma_block.treat_2 = sigma_exp.block.treat_i[2],
        sigma_block.treat_3 = sigma_exp.block.treat_i[3],
        sigma_exp.treat = sigma_exp.treat_i,
        sigma_rep = sigma_rep_i,
        sigma_exp = sigma_exp_i,
        sigma_ss = sigma_ss_i,
        sim_table
      )
    )
  } # end sim
  
  saveRDS(object = big_table, file = save_file_path)
  
}else{
  big_table <- readRDS(save_file_path)
} # end do_sim


out_table <- data.table(NULL)
for(param_set in 1:nrow(param_set_matrix)){
  r_mat <- big_table[sim_id == param_set, .SD, .SDcols = c("r21", "r31", "r32")]
  p1_mat <- big_table[sim_id == param_set, .SD, .SDcols = paste0(model_levels, "_contrast_1")]
  p2_mat <- big_table[sim_id == param_set, .SD, .SDcols = paste0(model_levels, "_contrast_2")]
  p3_mat <- big_table[sim_id == param_set, .SD, .SDcols = paste0(model_levels, "_contrast_3")]
  out_table <- rbind(
    out_table,
    data.table(
      contrast = c("Cn, Tr1", "Cn, Tr2", "Tr1, Tr2"),
      cor = apply(r_mat, 2, mean),
      rbind(apply(p1_mat, 2, pless),
            apply(p2_mat, 2, pless),
            apply(p3_mat, 2, pless))
    ))
}

setnames(out_table,
         names(out_table),
         c("Contrast", "Cor Error", model_levels))
out_table %>%
  kable(digits = c(1, 3, 3, 3, 3, 3, 3, 4),
        caption = "Type I error rate the RCBD statistical models under different levels of correlation structure among the observations. The Cor Error column is the average correlated error of the residuals fit by a simple linear model y ~ treatment. Model abbreviations as in text.") %>%
  kable_styling() %>%
  pack_rows("sim 1 - high correlated error", 1, 3) %>%
  pack_rows("sim 2 - low correlated error", 4, 6) %>%
  pack_rows("sim 3 - equal correlated error", 7, 9) %>%
  pack_rows("sim 4 - zero correlated error", 10, 12)

```

1. aov1, pptt, and lmm2 have well-behaved type I error rates (lmm2 is a little anti-conservative) regardless of correlation structure -- False discoveries should be well controlled.
2. lm1, lmm1, and aov2 have poorly-behaved type I error rates when there is a block:treatment interaction (sim 1 and 2), resulting in heterogenous correlation structure (remember that this is moot when there are only two treatment levels) -- this consequence is small when the block and block:treatment variance is small (small correlations in residuals). When there is block:treatment interaction, these models are conservative in contrasts associated with large residual correlation, resulting in low power, and anti-conservative in contrasts associated with small residual correlations. resulting in high false discovery rates.

```{r rcbd-perforance-dissection, eval = FALSE, echo = FALSE}
max_r <- 0.2
p_mat <- big_table[sim_id == 1 & abs(r21) < max_r & abs(r31) < max_r & abs(r32) < max_r, .SD, .SDcols = paste0(model_levels, "_contrast_3")]
p_mat <- big_table[sim_id == 1, .SD, .SDcols = paste0(model_levels, "_contrast_1")]

p_mat %>%
  apply(2, pless)

big_table[, pless_aov1_contrast_1 := ifelse(aov1_contrast_1 < 0.05, 1, 0)]
big_table[, pless_aov1_contrast_3 := ifelse(aov1_contrast_3 < 0.05, 1, 0)]
big_table[, pless_aov2_contrast_1 := ifelse(aov2_contrast_1 < 0.05, 1, 0)]
big_table[, pless_aov2_contrast_3 := ifelse(aov2_contrast_3 < 0.05, 1, 0)]

sim1_table <- data.table(r21 = seq(0,1,0.1),
                         r32 = seq(0,1,0.1))

# sim 1 - high heterogenous correlations
m1 <- glm(pless_aov1_contrast_1 ~ r21,
          family = binomial,
          data = big_table[sim_id == 1,])
sim1_table[, aov1_contrast_1 := predict(m1, sim1_table,
                   type = "response")]
m1 <- glm(pless_aov2_contrast_1 ~ r21,
          family = binomial,
          data = big_table[sim_id == 1,])
sim1_table[, aov2_contrast_1 := predict(m1, sim1_table,
                   type = "response")]
m1 <- glm(pless_aov1_contrast_3 ~ r32,
          family = binomial,
          data = big_table[sim_id == 1,])
sim1_table[, aov1_contrast_3 := predict(m1, sim1_table,
                   type = "response")]
m1 <- glm(pless_aov2_contrast_3 ~ r32,
          family = binomial,
          data = big_table[sim_id == 1,])
sim1_table[, aov2_contrast_3 := predict(m1, sim1_table,
                   type = "response")]

# sim 4 zero correlation
m1 <- glm(pless_aov1_contrast_1 ~ r21,
          family = binomial,
          data = big_table[sim_id == 4,])
sim1_table[, aov1_contrast_1_sim4 := predict(m1, sim1_table,
                   type = "response")]
m1 <- glm(pless_aov2_contrast_1 ~ r21,
          family = binomial,
          data = big_table[sim_id == 4,])
sim1_table[, aov2_contrast_1_sim4 := predict(m1, sim1_table,
                   type = "response")]

sim1_table %>%
  kable(digits = 3) %>%
  kable_styling()
```

Best Practices:

1. If the number of treatments = 2 and
    * the design is balanced: it doesn't matter which method you use.
    * the design is not balanced: use lmm1 or lmm2
2. If the number of treatments > 2 and
    * the design is balanced: use aov1/pptt (these are numerically equivalent). lmm1 is the standard but it assumes equal correlated error among treatment levels and equal standard errors among contrasts. lmm2 explicitly models heterogeneity of correlations and variances but 1) the model often fails and 2) has slightly anti-conservative Type I error.
    * the design is not balanced: lmm2 is attractive if the model runs. pptt is attractive if there are few missing values. 
3. If there are covariates, then start with lmm2. Use lmeControl if lmm2 fails. If this fails, use pairwise lmm1, which is the same as pairwise t-tests but allows covariates to be added to the model. Note that the covariate will be modeled independently in each pair, which effectively models the consequence of a treatment:covariate interaction.

### RSPD

Reasonable models:

1. lmm4 -- random interaction intercept model

Assumptions:

1.

Notes:

1. 

Performance:

```{r rspd-sim, message=FALSE, warning=FALSE, error=FALSE, echo = FALSE}

data_from <- "simulations/grcbds/data"

doit <- FALSE
writeit <- FALSE
readit <- TRUE
sim_id_i <- 7

n_sim <- 1000
n_main_plot <- 2 # types of main plot, each assigned single Tr1
n_sub_plot <- 3 # type os sub plot, each assigned single Tr2
n_block <- 10
n_ss <- 1
N <- n_main_plot * n_sub_plot * n_block * n_ss

beta <- c(10, 0, 0, 0, 0, 0)
sigma_block <- 0.8
sigma_ss <- 0.5

if(sim_id_i == 1){
  # sim 1
  sigma_block_main <- c(0.6, 0.6) # 0.4, 0, 0.4, 0
  sigma_block_sub <- c(0.0, 0.0, 0.0) # 0, 0.4, 0.4, 0
}
if(sim_id_i == 2){
  # sim 2
  sigma_block_main <- c(0.0, 0.0) # 0.4, 0, 0.4, 0
  sigma_block_sub <- c(0.6, 0.6, 0.6) # 0, 0.4, 0.4, 0
}
if(sim_id_i == 3){
  # sim 3
  sigma_block_main <- c(0.6, 0.6) # 0.4, 0, 0.4, 0
  sigma_block_sub <- c(0.4, 0.4, 0.4) # 0, 0.4, 0.4, 0
}
if(sim_id_i == 4){
  # sim 4
  sigma_block_main <- c(0.0, 0.0) # 0.4, 0, 0.4, 0
  sigma_block_sub <- c(0.0, 0.0, 0.0) # 0, 0.4, 0.4, 0
}
if(sim_id_i == 5){
  # sim 5
  sigma_block_main <- c(0.6, 0.2) # 0.4, 0, 0.4, 0
  sigma_block_sub <- c(0.0, 0.0, 0.0) # 0, 0.4, 0.4, 0
}
if(sim_id_i == 6){
  # sim 6
  sigma_block_main <- c(0.0, 0.0) # 0.4, 0, 0.4, 0
  sigma_block_sub <- c(0.6, 0.4, 0.2) # 0, 0.4, 0.4, 0
}
if(sim_id_i == 7){
  # sim 7
  sigma_block_main <- c(0.6, 0.2) # 0.4, 0, 0.4, 0
  sigma_block_sub <- c(0.6, 0.4, 0.2) # 0, 0.4, 0.4, 0
}

tr1_main_levels <- c("WT", "KO")
tr2_sub_levels <- c("Cn", "Tr1", "Tr2")

if(doit == TRUE){
  fake_data <- data.table(
    tr1_main = rep(rep(tr1_main_levels, each = n_sub_plot), n_block),
    tr2_sub = rep(rep(tr2_sub_levels, n_main_plot), n_block),
    main_plot = rep(1:(n_main_plot * n_block), each = n_sub_plot),
    block = rep(1:n_block, each = n_sub_plot*n_main_plot)
  )
  
  fake_data[, tr1_main := factor(tr1_main, levels = tr1_main_levels)]
  fake_data[, tr2_sub := factor(tr2_sub, levels = tr2_sub_levels)]
  fake_data[, block := factor(block)]
  
  # fixed effect matrix
  X <- model.matrix(~ tr1_main * tr2_sub, data = fake_data)
  
  
  # random effect matrix
  Z1a <- model.matrix(~ 0 + block, data = fake_data)
  Z1b <- model.matrix(~ 0 + block:tr1_main, data = fake_data)
  Z1c <- model.matrix(~ 0 + block:tr2_sub, data = fake_data)
  Z1 <- cbind(Z1a, Z1b, Z1c)
  
  y_mat <- matrix(as.numeric(NA), nrow = N, ncol = n_sim)
  
  # get n_sim sets of fake data
  for(sim_i in 1:n_sim){
    seed_starter <- seed_starter + 1
    set.seed(seed_starter)
    
    u1a <- rnorm(n_block, mean = 0, # block
                 sd = sigma_block)
    u1b <- rnorm(n_block*n_main_plot, # block:tr1_main
                 mean = rep(0, n_block*n_main_plot),
                 sd = rep(sigma_block_main, each = n_block))
    u1c <- rnorm(n_block*n_sub_plot,  # block:tr2_sub
                 mean = rep(0, n_block*n_sub_plot),
                 sd = rep(sigma_block_sub, each = n_block))         
    u1 <- c(u1a, u1b, u1c)
    e <- rnorm(N, 0, sigma_ss)
    
    y_mat[, sim_i] <-  X %*% beta + Z1 %*% u1 + e
    
  }  
    colnames(y_mat) <- paste0("sim_", 1:n_sim)
    fake_data <- cbind(fake_data, y_mat)
  
  
  # run models on fake data
  n_pairs <- (n_main_plot * n_sub_plot*(n_sub_plot-1)/2) +
    (n_sub_plot * n_main_plot*(n_main_plot-1)/2)
  lm0_p <- matrix(as.numeric(NA), nrow = n_sim, ncol = n_pairs)
  lm1_p <- matrix(as.numeric(NA), nrow = n_sim, ncol = n_pairs)
  lmm1_p <- matrix(as.numeric(NA), nrow = n_sim, ncol = n_pairs)
  lmm3_p <- matrix(as.numeric(NA), nrow = n_sim, ncol = n_pairs)
  lmm4_p <- matrix(as.numeric(NA), nrow = n_sim, ncol = n_pairs)
  lmm5_p <- matrix(as.numeric(NA), nrow = n_sim, ncol = n_pairs)
  lmm6_p <- matrix(as.numeric(NA), nrow = n_sim, ncol = n_pairs)
  r_mat <- matrix(as.numeric(NA), nrow = n_sim, ncol = n_pairs)
  lmm3_check <- array(data = NA)
  lmm4_check <- array(data = NA)
  lmm5_check <- array(data = NA)
  for(i in 1:n_sim){
    fake_i <- fake_data[, .SD, .SDcols = c("tr1_main",
                                           "tr2_sub", "block", paste0("sim_", i))]
    setnames(fake_i, paste0("sim_", i), "y")
    
    lm0 <- lm(y ~ tr1_main * tr2_sub,
              data = fake_i)
    lm1 <- lm(y ~ tr1_main * tr2_sub + block + tr1_main:block,
              data = fake_i)
    lmm1 <- lmer(y ~ tr1_main * tr2_sub + (1 | block),
                 data = fake_i)
    lmm3 <- lmer(y ~ tr1_main * tr2_sub + (tr1_main|block),
                 data = fake_i)
    lmm4 <- lmer(y ~ tr1_main * tr2_sub + (1 | block) + (1 | block:tr1_main),
                 data = fake_i)
    lmm5 <- lmer(y ~ tr1_main * tr2_sub + (1 | block) + (1 | block:tr2_sub),
                 data = fake_i)
    lmm6 <- lmer(y ~ tr1_main * tr2_sub + (1 | block) + (1 | block:tr2_sub) + (1 | block:tr1_main),
                 data = fake_i)
    # lmm5 <- lmer(y ~ tr1_main * tr2_sub + block + (1 | block:tr1_main),
    #              data = fake_data)
    
    # aov1 <- aov_4(y ~ tr1_main * tr2_sub + (tr1_main | block),
    #               data = fake_i)
    # 
    lm0_pairs <- emmeans(lm0, specs = c("tr1_main", "tr2_sub")) %>%
      contrast(method = "revpairwise",
               simple = "each",
               combine = TRUE,
               adjust = "none") %>%
      summary(infer = TRUE)
    lm0_p[i,] <- lm0_pairs[, "p.value"]
    
    lm1_pairs <- emmeans(lm1, specs = c("tr1_main", "tr2_sub")) %>%
      contrast(method = "revpairwise",
               simple = "each",
               combine = TRUE,
               adjust = "none") %>%
      summary(infer = TRUE)
    lm1_p[i,] <- lm1_pairs[, "p.value"]
    
    lmm1_pairs <- emmeans(lmm1, specs = c("tr1_main", "tr2_sub")) %>%
      contrast(method = "revpairwise",
               simple = "each",
               combine = TRUE,
               adjust = "none") %>%
      summary(infer = TRUE)
    lmm1_p[i,] <- lmm1_pairs[, "p.value"]
    
    lmm3_pairs <- emmeans(lmm3, specs = c("tr1_main", "tr2_sub")) %>%
      contrast(method = "revpairwise",
               simple = "each",
               combine = TRUE,
               adjust = "none") %>%
      summary(infer = TRUE)
    lmm3_p[i,] <- lmm3_pairs[, "p.value"]

    lmm4_pairs <- emmeans(lmm4, specs = c("tr1_main", "tr2_sub")) %>%
      contrast(method = "revpairwise",
               simple = "each",
               combine = TRUE,
               adjust = "none") %>%
      summary(infer = TRUE)
    lmm4_p[i,] <- lmm4_pairs[, "p.value"]
    
    lmm5_pairs <- emmeans(lmm5, specs = c("tr1_main", "tr2_sub")) %>%
      contrast(method = "revpairwise",
               simple = "each",
               combine = TRUE,
               adjust = "none") %>%
      summary(infer = TRUE)
    lmm5_p[i,] <- lmm5_pairs[, "p.value"]
    
    lmm6_pairs <- emmeans(lmm6, specs = c("tr1_main", "tr2_sub")) %>%
      contrast(method = "revpairwise",
               simple = "each",
               combine = TRUE,
               adjust = "none") %>%
      summary(infer = TRUE)
    lmm6_p[i,] <- lmm6_pairs[, "p.value"]

    # get correlations
    fake_i[, y_res := residuals(lm0)]
    fake_i_wide <- dcast(fake_i,
                         block ~ tr1_main + tr2_sub,
                         value.var = "y_res")
    treatment_levels <- names(fake_i_wide)[-1]
    residual_correlations <- cor(fake_i_wide[, .SD, .SDcols = treatment_levels])
    r_mat[i, 1] <- residual_correlations[4,1]
    r_mat[i, 2] <- residual_correlations[5,2]
    r_mat[i, 3] <- residual_correlations[6,3]
    r_mat[i, 4] <- residual_correlations[2,1]
    r_mat[i, 5] <- residual_correlations[3,1]
    r_mat[i, 6] <- residual_correlations[3,2]
    r_mat[i, 7] <- residual_correlations[5,4]
    r_mat[i, 8] <- residual_correlations[6,4]
    r_mat[i, 9] <- residual_correlations[6,5]
    
    # get checks
    #  lmm3_check[i] <- is.null(lmer_check(lmm3))
    lmm4_check[i] <- is.null(lmer_check(lmm4))
    lmm5_check[i] <- is.null(lmer_check(lmm5))
    
  }
  
  # lmm3_pairs
  # lmm4_pairs
  
  big_table <- data.table(NULL)
  model_labels <- c("lm0", "lm1", "lmm1", "lmm3", "lmm4", "lmm5", "lmm6", "r_mat")
  models <- list(lm0_p, lm1_p, lmm1_p, lmm3_p, lmm4_p, lmm5_p, lmm6_p, r_mat)
  for(model_i in 1:length(models)){
   big_table <- rbind(
     big_table,
    data.table(
      sim_id = sim_id_i,
      sig_block = sigma_block,
      sig_ss = sigma_ss,
      sig_main = sigma_block_main,
      sig_sub = sigma_block_sub,
      model = model_labels[model_i],
      models[[model_i]]
    ))
  }

  outfile_name <- paste0("rspd-", sim_id_i, ".Rds")
  save_file_path <- here(data_from, outfile_name)
  
  if(writeit == TRUE){
    saveRDS(object = big_table, file = save_file_path)
  }
  
  data.table(
    r = apply(r_mat, 2, mean),
    lm0 = apply(lm0_p, 2, pless),
    lm1 = apply(lm1_p, 2, pless),
    lmm1 = apply(lmm1_p, 2, pless),
    lmm3 = apply(lmm3_p, 2, pless),
    lmm4 = apply(lmm4_p, 2, pless),
    lmm5 = apply(lmm5_p, 2, pless),
    lmm6 = apply(lmm5_p, 2, pless)
  )
  
  # data.table(
  #   all = apply(lmm3_p, 2, pless),
  #   fail = apply(lmm3_p[lmm3_check == FALSE, ], 2, pless),
  #   pass = apply(lmm3_p[lmm3_check == TRUE,], 2, pless)
  # )
  # 
  # data.table(
  #   all = apply(lmm4_p, 2, pless),
  #   fail = apply(lmm4_p[lmm3_check == FALSE, ], 2, pless),
  #   pass = apply(lmm4_p[lmm3_check == TRUE,], 2, pless)
  # )
  
}


if(readit == TRUE){
  # combine 4 data sets and summarize
  res_out <- data.table(NULL)
  params <- c("sim_id", "sig_block", "sig_ss", "sig_main", "sig_sub")
  p_cols <- c("V1", "V2", "V3", "V4", "V5", "V6", "V7", "V8", "V9")
  for(sim_id_i in 1:4){
    outfile_name <- paste0("rspd-", sim_id_i, ".Rds")
    save_file_path <- here(data_from, outfile_name)
    big_table <- readRDS(save_file_path)
    r_mat <- big_table[model == "r_mat", ]
    lm0_p <- big_table[model == "lm0", ]
    lm1_p <- big_table[model == "lm1", ]
    lmm1_p <- big_table[model == "lmm1", ]
    lmm3_p <- big_table[model == "lmm3", ]
    lmm4_p <- big_table[model == "lmm4", ]
    lmm5_p <- big_table[model == "lmm5", ]
    lmm6_p <- big_table[model == "lmm6", ]
    out_table <- data.table(
      contrast = rep(c("main", "sub"), c(3,6)),
      r = apply(r_mat[, .SD, .SDcols = p_cols], 2, mean),
      lm0 = apply(lm0_p[, .SD, .SDcols = p_cols], 2, pless),
      lm1 = apply(lm1_p[, .SD, .SDcols = p_cols], 2, pless),
      lmm1 = apply(lmm1_p[, .SD, .SDcols = p_cols], 2, pless),
      lmm3 = apply(lmm3_p[, .SD, .SDcols = p_cols], 2, pless),
      lmm4 = apply(lmm4_p[, .SD, .SDcols = p_cols], 2, pless),
      lmm5 = apply(lmm5_p[, .SD, .SDcols = p_cols], 2, pless),
      lmm6 = apply(lmm6_p[, .SD, .SDcols = p_cols], 2, pless)
    )
    summary_i <- out_table[, .(r = mean(r),
                               lm0 = mean(lm0),
                               lm1 = mean(lm1),
                               lmm1 = mean(lmm1),
                               lmm3 = mean(lmm3),
                               lmm4 = mean(lmm4),
                               lmm5 = mean(lmm5),
                               lmm6 = mean(lmm6)),
                           by = .(contrast)]
    res_out <- rbind(
      res_out,
      data.table(
        sim_id  = big_table[1, sim_id],
        sig_block  = big_table[1, sig_block],
        sig_ss  = big_table[1, sig_ss],
        sig_main  = big_table[1, sig_main],
        sig_sub  = big_table[1, sig_sub],
        summary_i
      )
    )
    
  }
  
  res_out %>%
    kable(digits = c(0,2,2,2,2,1,3,3,3,3,3,3,3,3),
          caption = "Type I error rate for the RSPD statistical models under different models of random variance. The design is 2 (main plot: WT, KO) x 3 (subplot: Cn, Tr1, Tr2) with ten blocks. All four simulations have a component of variance due to block. Sim 1 includes a block:main plot component, Sim 2  includes a block:sub plot component, Sim 3 includes both block:main and block:sub components. Type I error rates for the 9 simple effect contrasts were averaged within the two sets: contrast = main is the aggregate of the single main plot contrast (KO - WT) at each level of the subplot factor. contrast = sub is the aggregate of the three subplot contrasts (Tr1 - Cn, Tr2 - Cn, Tr2 - Tr1) at each level of the main plot factor. The Cor Error column is the average correlated error of the residuals fit by a simple linear model y ~ treatment. Model abbreviations as in text."
    ) %>%
    kable_styling() %>%
    pack_rows("sim 1 - block:main plot", 1, 2) %>%
    pack_rows("sim 2 - block:sub plot", 3, 4) %>%
    pack_rows("sim 3 - block:main + block:sub", 5, 6) %>%
    pack_rows("sim 4 - block only", 7, 8)
}

```

### RCBDS

1. lmm3 -- random intercept and slopes model
2. lmm4 -- random interaction intercept model
3. lmm1 -- random intercept model on batch means
4. lmm2 -- lmm for correlated error on batch means
6. aov1 -- multivariate RM-ANOVA
7. aov2 -- univariate RM-ANOVA
8. pptt -- pairwise, paired t-test


Notes

1. lmm3 is the "maximal model" -- it fits the most parameters and has the fewest assumptions. lmm1 on the full data is the minimal model. I'm not recommending this at all as this is pseudoreplication and will result in very anti-conservative inference. lmm4 is less conservative than lmm3 (and makes more assumptions).
2. lmm1 and lmm2 *here* are the models for RCBD designs but using the aggregated data - that is the subsampled replicates within a batch (block:treatment combination) are averaged.
3. If design is balanced
    * lmm3 and aov1 result in same estimates, SE, CIs, and p-values
    * lmm4, lmm1, and aov2 result in same estimates, SE, CIs, and p-values
    * lmm2 has same estimates and SE as lmm3/aov1 but more df so less conservative
4. If a treatment within a block is missing, the whole block is deleted in the RM-ANOVA models. This reduces power
5. the lmm models are very flexible -- covariates can be added or these can be used as generalized lmms for non-normal distributions.
6. the lmm models, and especially lmm2 and lmm3, sometimes (often?) fail to converge, especially with small samples (small number of blocks) or if within block correlation is small
7. lmm1, lmm4, and aov2 will be less conservative (more power but at higher type I error/false positive rate)

Best practices

1. If balanced and no covariates then use lmm3/lmm2/aov1 if discovery is expensive (want to avoid false positives) or lmm4/aov2/lmm1 if discovery is cheap (can afford false positives). lmm2 is less conservative than lmm3/aov1. Use lmm3 if interested in the variance at different levels and/or covariance structure.
2. If unbalanced, or if there are covariates, then start with lmm3 or lmm2. Use lmeControl if lmm2 fails. If these fail, go to 1.

### GRCBD

1. lmm3 -- random intercept and slopes model
2. lmm4 -- random interaction intercept model
3. lmm1 -- random intercept model on block:treatment means
4. lmm2 -- lmm for correlated error on block:treatment means
5. aov1 -- multivariate RM-ANOVA
6. aov2 -- univariate RM-ANOVA
7. lm1 -- fixed effect model on all data
8. pptt -- pairwise, paired t-test on block:treatment means

### GRCBDS

1. lmm3 -- random intercept and slopes model
2. lmm4 -- random interaction intercept model
3. lmm1 -- random intercept model on block:treatment means
4. lmm2 -- lmm for correlated error on block:treatment means
5. aov1 -- multivariate RM-ANOVA
6. aov2 -- univariate RM-ANOVA
7. lm1 -- fixed effect model on all block:treatment:rep means
8. pptt -- pairwise, paired t-test on block:treatment:rep means


## Example 2: RCBD with >2 groups (classical equivalent: repeated measures ANOVA) (fig6g) {#sec-lmm-example2}

This example introduces linear mixed models for batches that contain all treatment levels of a single factor but no subsampling replication. In this example, the batch is the individual mouse ($\texttt{mouse\_id}$). There are four measures of the response variable on each mouse, one measure per treatment level. When there is no subsampling replication, we cannot add a random slope to the model because there is only a single observation at each treatment level and a slope would fit the point at the reference level and the point at the non-reference level perfectly. However, we can explicitly model variation in the correlated error and heterogeneity in the variances among treatments as an alternative to modeling a random slope.

[Reversing a model of Parkinson’s disease with in situ converted nigral neurons](https://www.nature.com/articles/s41586-020-2388-4){target="_blank"}

[Public source](https://www.ncbi.nlm.nih.gov/pmc/articles/7521455/){target="_blank"}

Source figure: [Fig. 6g](https://www.nature.com/articles/s41586-020-2388-4#Fig6){target="_blank"}

Source data: [Source Data Fig. 6](https://www.nature.com/articles/s41586-020-2388-4#MOESM11){target="_blank"}

```{r lmm-import6g, echo=FALSE}
data_from <- "Reversing a model of Parkinson’s disease with in situ converted nigral neurons"
file_name <- "41586_2020_2388_MOESM11_ESM.xlsx"
file_path <- here(data_folder, data_from, file_name)

fig6g_wide <- read_excel(file_path,
                         sheet = "fig6g",
                         range = "A3:E10",
                         col_names = TRUE) %>%
  data.table() %>%
  clean_names()

treatment_levels <- c("Lesion", "Saline", "CNO", "Post_CNO")
setnames(fig6g_wide,
         old = names(fig6g_wide),
         new = c("mouse_id", treatment_levels))

fig6g <- melt(fig6g_wide,
              id.vars = "mouse_id",
              variable.name = "treatment",
              value.name = "touch")
fig6g[, treatment := factor(treatment, levels = treatment_levels)]
fig6g[, mouse_id := paste0("mouse_", mouse_id)]
```

### Understand the data

In this study, the researchers investigated the effectiveness of knocking down the protein [PTBP1](https://en.wikipedia.org/wiki/PTBP1){target="_blank"} to induce astrocytes to convert to neurons in a motor processing region of the brain. Experimental lesions of this region of the brain is a model of [Parkinson's disease](https://en.wikipedia.org/wiki/Parkinson%27s_disease){target="_blank"}. In Experiment 6g, the researchers

1. Generated a lesion in the motor processing region using 6-hydroxydopamine (6-OHDA). The lesion disrupts the ability to control the **contralateral** (opposite side) forelimb.
2. One month after the lesion, measured the percent of ipsilateral (same side) forepaw touches (the forelimb extending out and touching the surface) in a test of exploration in a new environment (the "cylinder test"). The expected percent in an intact mouse is 50%. In a lesioned mouse, the percent should be much greater than 50% since there is less control of the contralateral limb. The measure at this point is in the treatment "Lesion". This is the positive control.
3. Converted astrocytes in the lesion to functional neurons by knocking down PTBP1.
4. Two months after knockdown, gave the mouse saline and remeasured percent ipsilateral touches in a cylinder test. If the knockdown worked as expected, there should be closer to 50% ipsilateral touches. The measure at this point is in the treatment "Saline". The comparison with Lesion is a focal test.
5. Inhibited neuron action in the converted neurons using clozapine-N-oxide (CNO), which suppresses neuron electrical activity. Then, remeasured percent ipsilateral touches in a cylinder test. If the CNO worked as expected, there should be much greater than 50% ipsilateral touches since there should be re-loss of control of the contralateral limb. The measure at this point is in the treatment "CNO". The comparison with Saline is a focal test.
6. Allowed three days for the CNO to degrade, then, remeasured percent ipsilateral touches in a cylinder test. If the CNO degraded as expected, the converted neurons should be functional and there should be closer to 50% ipsilateral touches. The measure at this point is in the treatment "Post_CNO". The comparison with CNO is a focal test.

The design is $4 \times 1$ -- a single treatment with four levels ("Lesion", "Saline", "CNO", "Post_CNO")

The planned contrasts are

1. Saline - Lesion. This measures the effect of the knockdown and conversion of astrocytes to functional neurons.
2. CNO - Saline. This measures the effect of inhibiting the converted neurons to test if it was these and not some other neurons that account for the effect in contrast 1.
3. Post_CNO - CNO. This is probing the same expectation as contrast 2.

### Model fit and inference
#### Fit the model

```{r lmm-6g_m1}
fig6g_m1a <- lmer(touch ~ treatment + (1|mouse_id), data = fig6g)

# alt model
fig6g_m1b <- lme(touch ~ treatment,
                random = ~1|mouse_id,
                correlation = corSymm(form = ~ 1 | mouse_id),
                weights = varIdent(form = ~ 1 | treatment),
                data = fig6g)

AIC(fig6g_m1a, fig6g_m1b)

# report model a
fig6g_m1 <- fig6g_m1a

```

fig6g_m1b overparameterizes, report fig6g_m1a (see [Alternative models for fig6g ](#lmm-fig6g-alt) below) 

#### Inference from the model

```{r lmm-6g_m1_coef, message=FALSE}
fig6g_m1_coef <- cbind(coef(summary(fig6g_m1)),
                       confint(fig6g_m1)[-c(1:2),])

# exp5c_m1_coef %>%
#   kable(digits = c(2,3,1,1,4,2,2)) %>%
#   kable_styling()
```

```{r lmm-6g_m1_emm}
fig6g_m1_emm <- emmeans(fig6g_m1, specs = c("treatment"))
```


```{r lmm-6g_m1_emm-show, echo=FALSE}
fig6g_m1_emm %>%
  kable(digits = c(1,1,2,1,1,2,2)) %>%
  kable_styling()
```

```{r lmm-6g_m1_planned}
# fig6g_m1_emm # print in console to get row numbers
# set the mean as the row number from the emmeans table
lesion <- c(1,0,0,0)
saline <- c(0,1,0,0)
cno <- c(0,0,1,0)
post_cno <- c(0,0,0,1)

fig6g_m1_planned <- contrast(fig6g_m1_emm,
                       method = list(
                         "Saline - Lesion" = c(saline - lesion),
                         "CNO - Saline" = c(cno - saline),
                         "Post_CNO - CNO" = c(post_cno - cno)
                       ),
                       adjust = "none"
) %>%
  summary(infer = TRUE)
```

```{r lmm-6g_m1_planned-show, echo=FALSE}
fig6g_m1_planned %>%
  kable(digits = c(1,2,3,3,2,2,2,5)) %>%
  kable_styling()
```

#### Plot the model {#lmm-fig6g-plotthemodel}

```{r echo=FALSE}
# add group1 and group2 columns to fig6g_m1_planned
fig6g_m1_planned_dt <- data.table(fig6g_m1_planned)
fig6g_m1_planned_dt[, pretty_p := pvalString(p.value)]
fig6g_m1_planned_dt[, group1 := c("Saline", "CNO", "Post_CNO")]
fig6g_m1_planned_dt[, group2 := c("Lesion", "Saline", "CNO")]

```

```{r echo=FALSE}
fig6g_effects <- ggplot_the_effects(fig6g_m1_emm,
                   fig6g_m1_planned,
                   effect_label = "Difference in % ipsilateral touch")

```

```{r echo=FALSE}
fig6g_response <- ggplot(data = fig6g,
              aes(x = treatment,
                  y = touch)) +
  geom_point(aes(group = mouse_id),
             position = position_dodge(width = 0.2),
             color = "gray") +
  geom_line(aes(group = mouse_id),
            position = position_dodge(width = 0.2),
            color = "gray80") +
  geom_point(data = summary(fig6g_m1_emm),
             aes(y = emmean,
             color = treatment),
             size = 3) +
  geom_errorbar(data = summary(fig6g_m1_emm),
             aes(y = emmean,
                 ymin = lower.CL,
                 ymax = upper.CL,
             color = treatment),
             width = .05) +
  
  ylab("Percent ipsilateral touch") +
  scale_color_manual(values = pal_okabe_ito_blue) +
  theme_pubr() +
  theme(
    axis.title.x = element_blank(), # no x-axis title
    legend.position = "none"
  ) + 
  NULL
```

```{r echo = FALSE, fig.dim=harrell_dim*small_scale, fig.cap="Treatment effect on ipsilateral touch, as percent of all touches. Gray dots connected by lines are individual mice."}
plot_grid(fig6g_effects, fig6g_response,
          nrow=2,
          align = "v",
          axis = "lr",
          rel_heights = c(0.5,1))
```

#### Alternaplot the model

```{r echo=FALSE, fig.dim=response_dim*small_scale, fig.cap = "Ipsilateral touch (percent of all touches) response to different treatments. Gray dots connected by lines are individual mice."}
fig6g_response_2 <- fig6g_response + stat_pvalue_manual(fig6g_m1_planned_dt,
                     label = "pretty_p",
                     y.position = c(99,97,95),
                     size = 2.5,
                     tip.length = 0.01)


fig6g_response_2
```

## Example 3 -- RCBD with two factors (exp5c)

Example 3 is similar to example 2 in that there is no subsampling replication and we cannot add random slopes to the linear mixed model. Example 3 differs in that the design is factorial -- there are two, crossed fixed factors. Consequently, there are several alternative models with different sets of random intercepts. The reported model includes two random intercepts, one of which models differences in batch effects among treatment levels (treatment by batch interactions). This **interaction intercept** is an alternative to random slope for modeling treatment by batch interactions.

[Transcriptomic profiling of skeletal muscle adaptations to exercise and inactivity](https://www.nature.com/articles/s41467-019-13869-w){target="_blank"}

Source figure: [Fig. 5c](https://www.nature.com/articles/s41467-019-13869-w#Fig5){target="_blank"}

Source data: [Source Data Fig. 5](https://www.nature.com/articles/s41467-019-13869-w#Sec27){target="_blank"}


```{r lmm-exp5c-import, echo = FALSE}
data_from <- "Transcriptomic profiling of skeletal muscle adaptations to exercise and inactivity"
file_name <- "41467_2019_13869_MOESM6_ESM.xlsx"
file_path <- here(data_folder, data_from, file_name)

exp5c_wide <- read_excel(file_path,
                         sheet = "Fig5c",
                         range = "A2:M3",
                         col_names = FALSE) %>%
  data.table() %>%
  transpose(make.names = 1)

activity_levels <- c("Basal", "EPS")
treatment_levels <- names(exp5c_wide)[1:2]
exp5c_wide[, activity := rep(activity_levels, each = 6)]
exp5c_wide[, activity := factor(activity, levels = activity_levels)]

exp5c <- melt(exp5c_wide,
              id.vars = "activity",
              variable.name = "treatment",
              value.name = "glucose_uptake")
exp5c[, treatment := factor(treatment, levels = treatment_levels)]

exp5c[, donor := rep(paste0("donor_", 1:6), 4)]
```

### Understand the data

The data for Example 1 are from Figure 5c. Six muscle source cells were used to start six independent cultures. Cells from each culture were treated with either a negative control ("Scr") or a siRNA ("siNR4A3") that "silences" expression of the NR4A3 gene product by [cleaving the mRNA](https://en.wikipedia.org/wiki/Small_interfering_RNA){target="_blank"}. Glucose uptake in the two cell types was measured at rest ("Basal") and during electrical pulse stimulation ("EPS").

The design is a $2 \times 2$ [Randomized complete block with no subsampling](https://en.wikipedia.org/wiki/Blocking_(statistics)#Randomized_block_design){target="_blank"}. There are two factors each with two levels: $\texttt{treatment}$ ("Scr", "siNR4A3") and $\texttt{activity}$ ("Basal", "EPS"). Each source cell is a block. All four treatment combinations were measured once per block.

### Examine the data

```{r lmm-exp5c-examine, echo = FALSE}
exp5c[, t.by.a := paste(treatment, activity)]
t.by.a_levels <- c("Scr Basal", "Scr EPS", "siNR4A3 Basal", "siNR4A3 EPS")
exp5c[, t.by.a := factor(t.by.a, levels = t.by.a_levels)]

ggplot(data = exp5c,
       aes(x = t.by.a,
           y = glucose_uptake,
           color = donor)) +
  geom_point() +
  geom_line(aes(group = donor))
```

The plot shows a strong donor effect.

### Model fit and inference {#lmm-exp5c-m1}

```{r lmm-exp5c_m1}
exp5c_m1a <- lmer(glucose_uptake ~ treatment * activity +
                   (1 | donor),
                 data = exp5c)

exp5c_m1b <- lmer(glucose_uptake ~ treatment * activity +
                   (1 | donor) +
                   (1 | donor:treatment) +
                   (1 | donor:activity),
                 data = exp5c)

exp5c_m1c <- lmer(glucose_uptake ~ treatment * activity +
                   (1 | donor) +
                   (1 | donor:treatment),
                 data = exp5c)

exp5c_m1d <- lme(glucose_uptake ~ treatment * activity,
                 random =  ~ 1 | donor,
                 correlation = corSymm(form = ~ 1 | donor),
                 weights = varIdent(form = ~ 1|t.by.a),
                 data = exp5c)

# check AIC
AIC(exp5c_m1a, exp5c_m1b, exp5c_m1c, exp5c_m1d)

# check VarCorr model c
VarCorr(exp5c_m1c) # fine

# report 1c (based on AIC and VarCorr check)
exp5c_m1 <- exp5c_m1c

```
exp5c_m1b (equivalent to univariate repeated measures ANOVA) is singular fit. don't use. Trivial difference in AIC between exp5c_m1a and exp5c_m1c. Nothing in VarCorr with exp5c_m1c raises red flags. Report exp5c_m1c.

#### Check the model

```{r lmm-exp5c_m1-check}
ggcheck_the_model(exp5c_m1)
```

fine.

#### Inference from the model

```{r lmm-exp5c_emm_coef, message=FALSE}
exp5c_m1_coef <- cbind(coef(summary(exp5c_m1)),
                       confint(exp5c_m1)[-c(1:3),])
```


```{r lmm-exp5c_emm_coef-show, message=FALSE}
exp5c_m1_coef %>%
  kable(digits = c(2,3,1,1,4,2,2)) %>%
  kable_styling()
```

```{r lmm-exp5c_emm}
exp5c_m1_emm <- emmeans(exp5c_m1, specs = c("treatment", "activity"))
```

```{r lmm-exp5c_emm-show, echo=FALSE}
exp5c_m1_emm %>%
  kable(digits = c(1,1,2,3,1,2,2)) %>%
  kable_styling()
```

```{r lmm-exp5c_planned}
# exp5c_emm # print in console to get row numbers
# set the mean as the row number from the emmeans table
scr_basal <- c(1,0,0,0)
siNR4A3_basal <- c(0,1,0,0)
scr_eps <- c(0,0,1,0)
siNR4A3_eps <- c(0,0,0,1)

exp5c_m1_planned <- contrast(exp5c_m1_emm,
                       method = list(
                         "(Scr EPS) - (Scr Basal)" = c(scr_eps - scr_basal),
                         "(siNR4A3 EPS) - (siNR4A3 Basal)" = c(siNR4A3_eps - siNR4A3_basal),
                         "Interaction" = c(siNR4A3_eps - siNR4A3_basal) -
                           c(scr_eps - scr_basal)
                           
                       ),
                       adjust = "none"
) %>%
  summary(infer = TRUE)
```

```{r lmm-exp5c-planned-show, echo=FALSE}
exp5c_m1_planned %>%
  kable(digits = c(1,2,3,3,2,2,2,3)) %>%
  kable_styling()
```

#### Plot the model {#lmm-exp5c-plotthemodel}

```{r echo=FALSE}
# add treatment column to emmeans table
exp5c_m1_emm_dt <- summary(exp5c_m1_emm) %>%
  data.table
exp5c_m1_emm_dt[, t.by.a := paste(treatment, activity)]
exp5c_m1_emm_dt[, t.by.a := factor(t.by.a,
                                      levels = levels(exp5c$t.by.a))]

# add group1 and group2 columns to exp6g_m1_planned
exp5c_m1_planned_dt <- data.table(exp5c_m1_planned)
exp5c_m1_planned_dt[, pretty_p := pvalString(p.value)]
exp5c_m1_planned_dt[, group1 := c("Scr EPS", "siNR4A3 EPS", "")]
exp5c_m1_planned_dt[, group2 := c("Scr Basal", "siNR4A3 Basal", "")]

```

```{r echo=FALSE}
exp5c_effects <- ggplot_the_effects(exp5c_m1_emm,
                   exp5c_m1_planned,
                   effect_label = "Difference in Glucose uptake\n(pmol per min)")
# exp5c_effects
```

```{r echo=FALSE}

exp5c_response <- ggplot(data = exp5c,
              aes(x = t.by.a,
                  y = glucose_uptake)) +
  geom_point(aes(group = donor),
             position = position_dodge(width = 0.2),
             color = "gray") +
  geom_line(aes(group = donor),
            position = position_dodge(width = 0.2),
            color = "gray80") +
  geom_point(data = exp5c_m1_emm_dt,
             aes(y = emmean,
             color = activity),
             size = 3) +
  geom_errorbar(data = exp5c_m1_emm_dt,
             aes(y = emmean,
                 ymin = lower.CL,
                 ymax = upper.CL,
             color = activity),
             width = .05) +
  
  ylab("Glucose uptake\n(pmol per min)") +
  scale_color_manual(values = pal_okabe_ito_blue) +
  theme_pubr() +
  theme(
    axis.title.x = element_blank(), # no x-axis title
    legend.position = "none"
  ) + 
  NULL

exp5c_response <- factor_wrap(exp5c_response)

#exp5c_response
```

```{r echo = FALSE, fig.dim=harrell_dim*small_scale, fig.cap="Treatment effect on glucose uptake. Gray dots connected by lines are individual donors."}
plot_grid(exp5c_effects, exp5c_response,
          nrow=2,
          align = "v",
          axis = "lr",
          rel_heights = c(0.6,1))
```

#### Alternaplot the model

```{r lmm-exp5c-alternaplot, fig.dim=response_dim*small_scale, echo=FALSE, fig.cap = "Glucose response to different treatments. Gray dots connected by lines are individual donors. Dashed gray line is expected additive mean of \"siNR4A3 EPS\"."}

# get coefficients of model
b <- coef(summary(exp5c_m1))[, "Estimate"]

# get interaction p
p_ixn <- exp5c_m1_planned_dt[contrast == "Interaction", pretty_p] # check!

dodge_width <- 0.4

gg <- exp5c_response +
  # pvalue brackets
  stat_pvalue_manual(exp5c_m1_planned_dt[1:2],
                     label = "pretty_p",
                     y.position = c(2.1, 2.1),
                     size = 2.5,
                     tip.length = 0.01) +
  
  geom_segment(x = 4 + dodge_width/2 - 0.1,
               y = b[1] + b[2] + b[3],
               xend = 4 + dodge_width/2 + 0.1,
               yend = b[1] + b[2] + b[3],
               linetype = "dashed",
               color = "gray") +
  geom_bracket(
    x = 4.35,
    y = b[1] + b[2] + b[3],
    yend = b[1] + b[2] + b[3] + b[4],
    label = paste0("interaction\np = ", p_ixn),
    text.size = 3,
    text.hjust = 0,
    color = "black") +
  coord_cartesian(xlim = c(1,4.25))

gg
```

Notes

1. Many researchers might look at the wide confidence intervals relative to the short distance between the means and think "no effect". The confidence intervals are correct, they simply are *not* meant to be tools for inferring anything about differences in means. This is one of many reasons why plots of means and error bars can be misleading for inference, despite the ubiquity of their use for communicating results. And, its why I prefer the [effects-and-response plots](#lmm-exp5c-plotthemodel), which explicitly communicate correct inference about effects.

### Why we care about modeling batch in exp5c

Figure \@ref(fig:lmm-exp5c-why) shows the modeled means of the four treatment combinations and the individual values colored by donor. It is pretty easy to see that the glucose uptake values for donors 4 and 5 are well above the mean for all four treatments. And, the values for donors 1, 2, and 3 are well below the mean for all four treatments. The values for donor 6 are near the mean for all four treatments.

```{r lmm-exp5c-why, echo= FALSE, fig.dim=std_dim, fig.cap = "Why we care about blocking. The black dots are the modeled means of each treatment combination. The colored dots are the measured values of the response for each donor. The position of a donor relative to the mean is easy to see with these data."}

m1 <- lm(glucose_uptake ~ treatment * activity,
           data = exp5c)
m1_emm_dt <- emmeans(m1, specs = c("treatment", "activity")) %>%
  summary() %>%
  data.table()
m1_emm_dt[, t.by.a:= paste(treatment, activity)]
m1_emm_dt[, t.by.a:= factor(t.by.a, t.by.a)]

pd_width <- 0.4
gg <- ggplot(data = exp5c,
       aes(x = t.by.a,
           y = glucose_uptake,
           color = donor)) +
  
  # geom_line(aes(group = donor),
  #           position = position_dodge(pd_width),
  #           color = "gray") +
  geom_point(position = position_dodge(pd_width),
             size = 2) +
  
  scale_color_manual(values = pal_okabe_ito_blue) +
  
  geom_point(data = m1_emm_dt,
             aes(x = t.by.a,
                 y = emmean),
             color = "black",
             size = 3) +
  
  theme_pubr() +
  theme(axis.title.x = element_blank()) +
  
  NULL
gg
```

Let's compare the effects estimated by the linear mixed model `exp5c_m1` with a linear model that ignores donor.

```{r lmm-exp5c-m2, echo=TRUE}
exp5c_m2 <- lm(glucose_uptake ~ treatment * activity,
           data = exp5c)

```

```{r lmm-exp5c_fixed, echo = FALSE}
exp5c_m2_emm <- emmeans(exp5c_m2, specs = c("treatment", "activity"))
exp5c_m2_planned <- contrast(exp5c_m2_emm,
                       method = list(
                         "(Scr EPS) - (Scr Basal)" = c(scr_eps - scr_basal),
                         "(siNR4A3 EPS) - (siNR4A3 Basal)" = c(siNR4A3_eps - siNR4A3_basal),
                         "Interaction" = c(siNR4A3_eps - siNR4A3_basal) -
                           c(scr_eps - scr_basal)
                           
                       ),
                       adjust = "none"
) %>%
  summary(infer = TRUE)
```

```{r lmm-exp5c-why2, echo=FALSE, fig.cap="A. Inference from a linear mixed model with blocking factor (donor) added as a random intercept. B. Inference from a fixed effects model."}
gg1 <- ggplot_the_effects(exp5c_m1,
                 exp5c_m1_planned,
                 effect_label = "Difference in Glucose uptake\n(pmol per min)")

gg2 <- ggplot_the_effects(exp5c_m2,
                 exp5c_m2_planned,
                 effect_label = "Difference in Glucose uptake\n(pmol per min)")

plot_grid(gg1, gg2, ncol=1, labels = "AUTO")

```

Figure \@ref(fig:lmm-exp5c-why2)A is a plot of the effects from the linear mixed model that models the correlated error due to donor. Figure \@ref(fig:lmm-exp5c-why2)B is a plot of the effects from the fixed effect model that ignores the correlated error due to donor. Adding $\texttt{donor}$ as a factor to the linear model increases the precision of the estimate of the treatment effects by eliminating the among-donor component of variance from the error variance.

## Example 4 -- RCBDS Experiments with subsampling replication (exp1g) {#sec-lmm-example4}

This example is from a design with batches (independent experiments) that contain all treatment levels of a single factor *and* subsampling replication. These data were used to introduce linear mixed models in Example 1. The design of the experiment is $2 \times 2$ factorial. Example 1 flattened the analysis to simplify explanation of random intercepts and random slopes. Here, the data are analyzed with a factorial model.

[A GPR174–CCL21 module imparts sexual dimorphism to humoral immunity](https://www.nature.com/articles/s41586-019-1873-0){target="_blank"}

[Public source](https://pubmed.ncbi.nlm.nih.gov/31875850/){target="_blank"}

Source figure: [Fig. 1g](https://www.nature.com/articles/s41586-019-1873-0/figures/1){target="_blank"}

Source data: [Source Data Fig. 1](https://www.nature.com/articles/s41586-019-1873-0#MOESM3){target="_blank"}

```{r lmm-exp1g-import, echo=FALSE, message=FALSE}
data_from <- "A GPR174–CCL21 module imparts sexual dimorphism to humoral immunity"
file_name <- "41586_2019_1873_MOESM3_ESM.xlsx"
file_path <- here(data_folder, data_from, file_name)

exp1g_wide <- read_excel(file_path,
                         sheet = "Fig 1g",
                         range = "B4:E25",
                         col_types = c("numeric"),
                         col_names = FALSE) %>%
  data.table()

genotype_levels <- c("Gpr174+", "Gpr174-")
sex_levels <- c("M", "F")
g.by.s_levels <- do.call(paste, expand.grid(genotype_levels, sex_levels))
colnames(exp1g_wide) <- g.by.s_levels

exp_levels <- paste0("exp_", 1:4)
exp1g_wide[, experiment_id := rep(exp_levels, c(5,6,6,5))] #check!
exp1g_wide[, experiment_id := factor(experiment_id)] #check!

exp1g <- melt(exp1g_wide,
              id.vars = "experiment_id",
              measure.vars = g.by.s_levels,
              variable.name = "treatment",
              value.name = "gc") %>% # cell count
  na.omit()

exp1g[, c("genotype", "sex"):= tstrsplit(treatment,
                                             " ",
                                             fixed = TRUE)]
exp1g[, genotype := factor(genotype,
                           levels = genotype_levels)]
exp1g[, sex := factor(sex,
                           levels = sex_levels)]

treatment_levels <- c("Gpr174+ M", "Gpr174+ F", "Gpr174- M", "Gpr174- F")
treatment_levels <- c("Gpr174+ M", "Gpr174- M", "Gpr174+ F", "Gpr174- F")
exp1g[, treatment := factor(treatment,
                           levels = treatment_levels)]

exp1g_means <- exp1g[, .(gc = mean(gc)),
                     by = .(treatment, genotype, sex, experiment_id)]

```

### Understand the data

The researchers in this paper are interested in discovering mechanisms causing the lower [antibody-mediated immune response](https://en.wikipedia.org/wiki/Humoral_immunity){target="_blank"} in males relative to females. The data in Fig. 1 are from a set of experiments on mice to investigate how the G-protein coupled receptor protein GPR174 regulates formation of the [B-cell germinal center in secondary lymph tissue](https://en.wikipedia.org/wiki/Germinal_center){target="_blank"}. GPR174 is a X-linked gene.

**Response variable** $\texttt{gc}$ -- germinal center size (%). The units are the percent of cells expressing germinal center markers.

**Factor 1** -- $\texttt{sex}$ ("M", "F"). Male ("M") is the reference level.

**Factor 2** -- $\texttt{chromosome}$ ("Gpr174+", "Gpr174-"). "Gpr174-" is a GPR174 knockout. The wildtype ("Gpr174+") condition is the reference level. 

**Design** -- $2 \times 2$, that is, two crossed factors each with two levels. This results in four groups, each with a unique combination of the levels from each factor. "M Gpr174+" is the control. "M Gpr174+" is the knockout genotype in males ("knockout added"). "F Gpr174+" is the wildtype female ("X chromosome added"). "F Gpr174-" is the knockout female ("knockout and X chromosome added".

### Examine the data

```{r lmm-exp1g-examine}
ggplot(data = exp1g,
       aes(x = treatment,
           y = gc,
           color = experiment_id)) +
  geom_point(position = position_dodge(0.4))

```

### Fit the model

```{r lmm-exp1g-m1}
# three slope parameters
exp1g_m1a <- lmer(gc ~ genotype * sex +
                   (genotype * sex | experiment_id),
                 data = exp1g)

VarCorr(exp1g_m1a) # looks fine

# one slope parameter but capturing all treatment combinations
exp1g_m1b <- lmer(gc ~ genotype * sex +
                   (treatment | experiment_id),
                 data = exp1g)

# intercept interactions
exp1g_m1c <- lmer(gc ~ genotype * sex +
                   (1 | experiment_id) +
                    (1 | experiment_id:genotype) +
                    (1 | experiment_id:sex) +
                    (1 | experiment_id:genotype:sex),
                 data = exp1g)

VarCorr(exp1g_m1c) # id:genotype is low

# drop id:genotype which has low variance
exp1g_m1d <- lmer(gc ~ genotype * sex +
                   (1 | experiment_id) +
                    (1 | experiment_id:sex) +
                    (1 | experiment_id:sex:genotype),
                 data = exp1g)

AIC(exp1g_m1a, exp1g_m1b, exp1g_m1c, exp1g_m1d)

# go with exp1g_m1d.
exp1g_m1 <- exp1g_m1d
```

### Inference from the model

```{r}
exp1g_m1_coef <- coef(summary(exp1g_m1))
```

```{r}
exp1g_m1_coef
```

```{r lmm-exp1g-emm}
# order of factors reversed in specs because I want sex to be
# main x-axis variable in plot
exp1g_m1_emm <- emmeans(exp1g_m1, specs = c("sex", "genotype"))
```

```{r lmm-exp1g-pairs}
# exp1g_m1_emm # print in console to get row numbers
# set the mean as the row number from the emmeans table
wt_m <- c(1,0,0,0)
wt_f <- c(0,1,0,0)
ko_m <- c(0,0,1,0)
ko_f <- c(0,0,0,1)

# simple effects within males and females + interaction 
# 1. (ko_m - wt_m) 
# 2. (ko_f - wt_f)

exp1g_contrasts <- list(
  "(Gpr174- M) - (Gpr174+ M)" = c(ko_m - wt_m),
  "(Gpr174- F) - (Gpr174+ F)" = c(ko_f - wt_f),
  "Interaction" = c(ko_f - wt_f) -
    c(ko_m - wt_m)
)
exp1g_m1_planned <- contrast(exp1g_m1_emm,
                       method = exp1g_contrasts,
                       adjust = "none"
) %>%
  summary(infer = TRUE)
```


```{r lmm-exp1g-pairs-show, echo=FALSE}
exp1g_m1_planned %>%
  kable(digits = c(1,2,3,3,2,2,2,3)) %>%
  kable_styling()
```

Notes

1. The direction of the estimated effect is opposite in males and females

### Plot the model

```{r lmm-exp1g-plot-the-model, fig.dim=harrell_dim*small_scale, echo=FALSE}
exp1g_effects <- ggplot_the_effects(exp1g_m1_emm,
                       exp1g_m1_planned,
                       effect_label = "Difference in GC (%)")

#exp1g_effects
```

```{r echo=FALSE}
# add treatment column to emmeans table
exp1g_m1_emm_dt <- summary(exp1g_m1_emm) %>%
  data.table
exp1g_m1_emm_dt[, treatment := paste(genotype, sex)]
exp1g_m1_emm_dt[, treatment := factor(treatment,
                                      levels = levels(exp1g$treatment))]

# create table of means for each treatment * experiment_id combination
exp1g[, group_mean := predict(exp1g_m1)]
exp1g_m1_emm2 <- exp1g[, .(group_mean = mean(group_mean)),
                       by = .(treatment, experiment_id)]

# add group1 and group2 columns to exp1g_m1_planned
exp1g_m1_planned_dt <- data.table(exp1g_m1_planned)
exp1g_m1_planned_dt[, pretty_p := pvalString(p.value)]
exp1g_m1_planned_dt[, group1 := c("Gpr174- M", "Gpr174- F", "")]
exp1g_m1_planned_dt[, group2 := c("Gpr174+ M", "Gpr174+ F", "")]

```

```{r echo=FALSE}
exp1g_response <- ggplot(data = exp1g,
              aes(x = treatment,
                  y = gc,
                  color = experiment_id)) +
  # modeled experiment by treatment means
  geom_point(data = exp1g_m1_emm2,
            aes(y = group_mean,
                color = experiment_id),
            position = position_dodge(width = 0.4),
            alpha = 1,
            size = 2) +
  geom_line(data = exp1g_m1_emm2,
            aes(y = group_mean,
                group = experiment_id,
                color = experiment_id),
            position = position_dodge(width = 0.4)) +
  
  # raw data
  geom_point(position = position_dodge(width = 0.4),
             alpha = 0.3
  ) +
  
  # modeled treatment means
  geom_point(data = exp1g_m1_emm_dt,
             aes(y = emmean),
             color = "black",
             size = 3) +
  # geom_errorbar(data = exp1g_m1_emm_dt,
  #            aes(y = emmean,
  #                ymin = lower.CL,
  #                ymax = upper.CL),
  #             color = "black",
  #           width = .05) +
  
  ylab("GC (%)") +
  scale_color_manual(values = pal_okabe_ito_blue) +
  theme_pubr() +
  coord_cartesian(xlim = c(1, 4.1)) +
  theme(
    axis.title.x = element_blank(), # no x-axis title
    legend.position = "none"
  ) + 
  NULL
  
exp1g_response <- factor_wrap(exp1g_response)

# exp1g_response
```

```{r echo = FALSE, fig.dim=harrell_dim*small_scale, fig.cap="Treatment effect on germinal center (GC) formation. Small, pale, colored dots are independent experiments. Intermediate size colored dots are experiment means."}
plot_grid(exp1g_effects, exp1g_response,
          nrow=2,
          align = "v",
          axis = "lr",
          rel_heights = c(0.6,1))
```

### Alternaplot the model

```{r lmm-exp1g-alternaplot, echo=FALSE, fig.dim=response_dim*small_scale, fig.cap = "Germinal center (GC) formation in response to treatment. Small, pale, colored dots are independent experiments. Intermediate size colored dots are experiment means. Dashed gray line is expected additive mean of Gpr174- F"}


# get coefficients of model
b <- exp1g_m1_coef[, "Estimate"]

# get interaction p
p_ixn <- exp1g_m1_planned_dt[contrast == "Interaction", pretty_p]

dodge_width <- 0.4

gg <- exp1g_response +
  
  # pvalue brackets
  stat_pvalue_manual(exp1g_m1_planned_dt[1:2],
                     label = "pretty_p",
                     y.position = c(20,20),
                     size = 2.5,
                     tip.length = 0.01) +
  
  # additive line + interaction p bracket
  geom_segment(x = 3.85,
               y = b[1] + b[2] + b[3],
               xend = 4.15,
               yend = b[1] + b[2] + b[3],
               linetype = "dashed",
               color = "gray") +
  geom_bracket(
    x = 4.2,
    y = b[1] + b[2] + b[3],
    yend = b[1] + b[2] + b[3] + b[4],
    label = paste0("interaction\np = ", p_ixn),
    text.size = 3,
    text.hjust = 0,
    color = "black")


gg
```


### Understanding the alternative models

```{r}
exp1g_m1a <- lmer(gc ~ genotype * sex +
                   (genotype * sex | experiment_id),
                 data = exp1g)

exp1g_m1b <- lmer(gc ~ genotype * sex +
                   (treatment | experiment_id),
                 data = exp1g)

exp1g_m1c <- lmer(gc ~ genotype * sex +
                   (1 | experiment_id) +
                    (1 | experiment_id:genotype) +
                    (1 | experiment_id:sex) +
                    (1 | experiment_id:genotype:sex),
                 data = exp1g)

exp1g_m1d <- lmer(gc ~ genotype * sex +
                   (1 | experiment_id) +
                    (1 | experiment_id:sex) +
                    (1 | experiment_id:genotype:sex),
                 data = exp1g)

```

Notes

1. All four models model the same fixed effects. The models differ only in how they model the random effects.
2. Model `exp1g_m1a` fits one random intercept, two random slopes, and one random interaction.
* $\gamma_{0j}$ -- a random intercept modeling batch effects of $\texttt{experiment\_id}$ on the intercept
* $\gamma_{1j}$ -- a random slope modeling the effect of the non-reference level of "Gpr174-" on the batch effect of $\texttt{experiment\_id}$. This is a $\texttt{experiment\_id} \times \texttt{genotype}$ interaction.
* $\gamma_{2j}$ -- a random slope modeling the effect of "F" (female) on the batch effect of $\texttt{experiment\_id}$. This is a $\texttt{experiment\_id} \times \texttt{sex}$ interaction.
* $\gamma_{3j}$ -- a random slope modeling the effect of the interaction effect of "Gpr174-" and "F" on the batch effect of $\texttt{experiment\_id}$. This is a $\texttt{experiment\_id} \times \texttt{sex}  \times \texttt{genotype}$ interaction.
3. Model `exp1g_m1b` fits one random intercept and three random slopes
* $\gamma_{0j}$ -- a random intercept modeling batch effects of $\texttt{experiment\_id}$ on the intercept. This is modeling the same thing as $\gamma_{0j}$ in Model `exp1g_m1a`.
* $\gamma_{1j}$ -- a random slope modeling the effect of "Gpr174- M" on the batch effect of $\texttt{experiment\_id}$. This is modeling the same thing as $\gamma_{1j}$ in Model `exp1g_m1a`.
* $\gamma_{2j}$ -- a random slope modeling the effect of "Gpr174+ F" on the batch effect of $\texttt{experiment\_id}$. This is modeling the same thing as $\gamma_{2j}$ in Model `exp1g_m1a`.
* $\gamma_{3j}$ -- a random slope modeling the effect of "M Gpr174- F" on the batch effect of $\texttt{experiment\_id}$. This is modeling the added variance accounted for by the random interaction $\gamma_{2j}$ in Model `exp1g_m1a` but in a different way.
4. Model `exp1g_m1c` fits four random intercepts
* $\gamma_{0j}$ -- a random intercept modeling batch effects of $\texttt{experiment\_id}$ on the intercept. This is modeling the same thing as $\gamma_{0j}$ in Model `exp1g_m1a`.
* $\gamma_{0jk}$ -- a random intercept modeling the effects of the combination of $\texttt{experiment\_id}$ and $\texttt{genotype}$. This is very similar to the variance modeled by $\gamma_{1j}$ in Model `exp1g_m1a` except the draws from $\gamma_{0jk}$ are independent (uncorrelated) of draws from the other random intercepts.
* $\gamma_{0jl}$ -- a random intercept modeling the effects of the combination of $\texttt{experiment\_id}$ and $\texttt{sex}$. This is very similar to the variance modeled by $\gamma_{2j}$ in Model `exp1g_m1a` except the draws from $\gamma_{0jl}$ are independent (uncorrelated) of draws from the other random intercepts (review [The correlation among random intercepts and slopes](#lmm-varcorr) if this doesn't make sense).
* $\gamma_{0jkl}$ -- a random intercept modeling the effects of the combination of $\texttt{experiment\_id}$, $\texttt{genotype}$ and $\texttt{sex}$. This is very similar to the variance modeled by $\gamma_{3j}$ in Model `exp1g_m1a` except the draws from $\gamma_{0jkl}$ are independent (uncorrelated) of draws from the other random intercepts.
5. Model `exp1g_m1d` fits the same random intercepts as Model `exp1g_m1c` but excludes
$\gamma_{0jl}$ (the random intercept for the experiment_id by genotyp combination. This was excluded because of the low variance of this component in the fit model. 

### The VarCorr matrix of models exp1g_m1a and exp1g_m1b

The random effect similarity of models `exp1g_m1a` and `exp1g_m1b` can be seen in the estimated variance components and correlations among the random effects.

```{r, echo=FALSE}
options(knitr.kable.NA = '')
vc <- rbind(varcor(VarCorr(exp1g_m1a)$experiment_id),
            varcor(VarCorr(exp1g_m1b)$experiment_id))

vc %>%
  kable(col.names = rep("", 4),
        digits = 4,
        caption = "The Varcorr matrix. Standard deviations of random effects on the diagonal. Correlations of random effects on the off-diagonal.") %>%
  kable_styling() %>%
  pack_rows("exp1g_m1a", 1, 4) %>%
  pack_rows("exp1g_m1b", 5, 8)
  
```

### The linear mixed model has more precision and power than the fixed effect model of batch means

```{r lmm-exp1g-lm2-means-pool, echo=TRUE}
# means pooling model
exp1g_m2 <- lm(gc ~ sex * genotype,
                data = exp1g_means)

```

```{r lmm-exp1g-lm2-planned, echo=FALSE}
exp1g_m2_planned <- emmeans(exp1g_m2, specs = c("sex", "genotype")) %>%
  contrast(method = exp1g_contrasts,
           adjust = "none"
  ) %>%
  summary(infer = TRUE)

pairs_dt <- rbind(exp1g_m1_planned, exp1g_m2_planned)

pairs_dt %>%
  kable(digits = c(1,2,3,1,2,2,2,3), caption = "Planned contrasts for the linear mixed model exp1g_m1 and the fixed effects model of experiment means exp1g_m2.") %>%
  kable_styling() %>%
  pack_rows("exp1g_m1 (lmm)", 1, 3) %>%
  pack_rows("exp1g_m2 (lm means pooling)", 4, 6)
```

Notes

1. A fixed effects model fit to batch-means pooled data is strongly conservative and will result in less discovery.
2. Means pooling does not make the data independent in a randomized complete block design. A linear mixed model of batch-means pooled data is a **mixed-effect ANOVA** (Next section. Also see Section \@ref(issues-exp4d) in the Issues chapter).

### Fixed effect models and pseudoreplication

```{r lmm-exp1g_m3}
# complete pooling model
exp1g_m3 <- lm(gc ~ sex * genotype,
                data = exp1g)
```

```{r lmm-exp1g_m3-compare, echo=FALSE}
exp1g_m3_planned <- emmeans(exp1g_m3, specs = c("sex", "genotype")) %>%
  contrast(method = exp1g_contrasts,
           adjust = "none"
  ) %>%
  summary(infer = TRUE)

pairs_dt <- rbind(exp1g_m1_planned, exp1g_m3_planned)

pairs_dt %>%
  kable(digits = c(1,2,3,1,2,2,2,3), caption = "Planned contrasts for the linear mixed model exp1g_m1 and the fixed effects model exp1g_m3 with complete pooling.") %>%
  kable_styling() %>%
  pack_rows("exp1g_m1 (lmm)", 1, 3) %>%
  pack_rows("exp1g_m3 (lm complete pooling)", 4, 6)
```

Notes

1. Complete pooling is strongly anti-conservative and will result in increased false discovery. Complete pooling is a type of pseudoreplication -- the subsamples have been analyzed as if they are independent replicates. Subsamples are not independent.
2. For the Experiment 1g data, the 95% confidence intervals are wider and the *p*-values are larger in the complete pool model `exp1g_m3` compared to the linear mixed model `exp1g_m1`. This is an unusual result.

### Mixed-effect ANOVA

```{r lmm-exp1g_m1_aov}
exp1g_m1_aov <- aov_4(gc ~ sex * genotype +
                  (sex * genotype | experiment_id),
                data = exp1g)
```

Notes

1. The formula has the same format as that in Example 3 for repeated measures ANOVA on RCB designs with no subsampling. In biology, this is often called a **mixed effect ANOVA** with two fixed factors and one random factor.
2. The data are aggregated prior to fitting the model -- this means the subsamples are averaged within each batch by treatment combination.
3. The mixed-effect ANOVA is equivalent to the linear mixed model `exp1g_m1c` if there are the same number of replicates in each treatment combination and the same number of subsamples in all treatment by $\texttt{experiment\_id}$ combinations. The design is not balanced in this example.

```{r lmm-exp1g_m1_aov-pairs, echo=FALSE}
pairs_a <- emmeans(exp1g_m1_aov, specs = c("sex", "genotype")) %>%
  contrast(method = exp1g_contrasts,
           adjust = "none"
  )%>%
  summary(infer = TRUE)
pairs_b <- emmeans(exp1g_m1c, specs = c("sex", "genotype")) %>%
  contrast(method = exp1g_contrasts,
           adjust = "none"
  )%>%
  summary(infer = TRUE)
pairs_c <- emmeans(exp1g_m1d, specs = c("sex", "genotype")) %>%
  contrast(method = exp1g_contrasts,
           adjust = "none"
  )%>%
  summary(infer = TRUE)


pairs_dt <- rbind(pairs_a, pairs_b, pairs_c)

pairs_dt %>%
  kable(digits = c(1,2,3,3,2,2,2,3), caption = "Planned contrasts from mixed ANOVA compared to lmm equivalent of mixed ANOVA and lowest AIC lmm.") %>%
  kable_styling() %>%
  pack_rows("exp1g_m1_aov (mixed ANOVA)", 1, 3) %>%
  pack_rows("exp1g_m1_c (lmm equivalent of mixed ANOVA)", 4, 6) %>%
  pack_rows("exp1g_md (lmm with min AIC)", 7, 9)
```


## Working in R
### Plotting models fit to batched data
#### Models without subsampling - Experiment 6g

Data wrangling necessary for plot:

```{r}
# convert contrast table to a data.table
fig6g_m1_planned_dt <- data.table(fig6g_m1_planned)

# create a pretty p-value column
fig6g_m1_planned_dt[, pretty_p := pvalString(p.value)]

# add group1 and group2 columns to exp1g_m1_planned
fig6g_m1_planned_dt[, group1 := c("Saline", "CNO", "Post_CNO")]
fig6g_m1_planned_dt[, group2 := c("Lesion", "Saline", "CNO")]

```

Experiments are colored:

```{r}
gg1 <- ggplot(data = fig6g,
              aes(x = treatment,
                  y = touch,
                  color = mouse_id)) +
  geom_point(position = position_dodge(width = 0.2)) +
  geom_line(aes(group = mouse_id),
            position = position_dodge(width = 0.2),
            color = "gray80") +
  geom_point(data = summary(fig6g_m1_emm),
             aes(y = emmean),
             color = "black",
             size = 3) +
  geom_errorbar(data = summary(fig6g_m1_emm),
             aes(y = emmean,
                 ymin = lower.CL,
                 ymax = upper.CL),
             color = "black",
             width = .05) +

  # pvalue brackets
  stat_pvalue_manual(fig6g_m1_planned_dt,
                     label = "pretty_p",
                     y.position = c(99,97,95),
                     size = 2.5,
                     tip.length = 0.01) +

  ylab("Percent ipsilateral touch") +
  scale_color_manual(values = pal_okabe_ito_blue) +
  theme_pubr() +
  theme(
    axis.title.x = element_blank(), # no x-axis title
    legend.position = "none"
  ) + 
  NULL
  
```

Treatments are colored:

```{r}
gg2 <- ggplot(data = fig6g,
              aes(x = treatment,
                  y = touch)) +
  geom_point(aes(group = mouse_id),
             position = position_dodge(width = 0.2),
             color = "gray") +
  geom_line(aes(group = mouse_id),
            position = position_dodge(width = 0.2),
            color = "gray80") +
  geom_point(data = summary(fig6g_m1_emm),
             aes(y = emmean,
             color = treatment),
             size = 3) +
  geom_errorbar(data = summary(fig6g_m1_emm),
             aes(y = emmean,
                 ymin = lower.CL,
                 ymax = upper.CL,
             color = treatment),
             width = .05) +
  
  # pvalue brackets
  stat_pvalue_manual(fig6g_m1_planned_dt,
                     label = "pretty_p",
                     y.position = c(99,97,95),
                     size = 2.5,
                     tip.length = 0.01) +

  ylab("Percent ipsilateral touch") +
  scale_color_manual(values = pal_okabe_ito_blue) +
  theme_pubr() +
  theme(
    axis.title.x = element_blank(), # no x-axis title
    legend.position = "none"
  ) + 
  NULL

#gg2
```

```{r, echo=FALSE, fig.width=full_width, fig.asp=0.5}
plot_grid(gg1, gg2, ncol=2)
```

#### Models with subsampling - Experiment 1g

Data wrangling necessary for plot:

```{r}
# add treatment column to emmeans table
exp1g_m1_emm_dt <- summary(exp1g_m1_emm) %>%
  data.table
exp1g_m1_emm_dt[, treatment := paste(genotype, sex)]
exp1g_m1_emm_dt[, treatment := factor(treatment,
                                      levels = levels(exp1g$treatment))]

# create table of means for each treatment * experiment_id combination
exp1g[, group_mean := predict(exp1g_m1)]
exp1g_m1_emm2 <- exp1g[, .(group_mean = mean(group_mean)),
                       by = .(treatment, experiment_id)]

# add group1 and group2 columns to exp1g_m1_planned
exp1g_m1_planned_dt <- data.table(exp1g_m1_planned)
exp1g_m1_planned_dt[, pretty_p := pvalString(p.value)]
exp1g_m1_planned_dt[, group1 := c("Gpr174- M", "Gpr174- F", "")]
exp1g_m1_planned_dt[, group2 := c("Gpr174+ M", "Gpr174+ F", "")]

```

```{r}
# get coefficients of model
b <- exp1g_m1_coef[, "Estimate"]

# get interaction p
p_ixn <- exp1g_m1_planned_dt[contrast == "Interaction", pretty_p]

exp1g_plot_a <- ggplot(data = exp1g,
              aes(x = treatment,
                  y = gc,
                  color = experiment_id)) +
  # modeled experiment by treatment means
  geom_point(data = exp1g_m1_emm2,
            aes(y = group_mean,
                color = experiment_id),
            position = position_dodge(width = 0.4),
            alpha = 1,
            size = 2) +
  geom_line(data = exp1g_m1_emm2,
            aes(y = group_mean,
                group = experiment_id,
                color = experiment_id),
            position = position_dodge(width = 0.4)) +
  
  # raw data
  geom_point(position = position_dodge(width = 0.4),
             alpha = 0.3
  ) +
  
  # modeled treatment means
  geom_point(data = exp1g_m1_emm_dt,
             aes(x = treatment,
                 y = emmean),
             color = "black",
             size = 3) +
  # geom_errorbar(data = exp1g_m1_emm_dt,
  #            aes(y = emmean,
  #                ymin = lower.CL,
  #                ymax = upper.CL),
  #             color = "black",
  #           width = .05) +
  
  # pvalue brackets
  stat_pvalue_manual(exp1g_m1_planned_dt[1:2],
                     label = "pretty_p",
                     y.position = c(20,20),
                     size = 2.5,
                     tip.length = 0.01) +
  
  # additive line + interaction p bracket
  geom_segment(x = 3.85,
               y = b[1] + b[2] + b[3],
               xend = 4.15,
               yend = b[1] + b[2] + b[3],
               linetype = "dashed",
               color = "gray") +
  geom_bracket(
    x = 4.2,
    y = b[1] + b[2] + b[3],
    yend = b[1] + b[2] + b[3] + b[4],
    label = paste0("interaction\np = ", p_ixn),
    text.size = 3,
    text.hjust = 0,
    color = "black") +
  
  ylab("GC (%)") +
  scale_color_manual(values = pal_okabe_ito_blue) +
  theme_pubr() +
  coord_cartesian(xlim = c(1, 4.1)) +
  theme(
    axis.title.x = element_blank(), # no x-axis title
    legend.position = "none"
  ) + 
  NULL
  
exp1g_plot_a <- factor_wrap(exp1g_plot_a)

#exp1g_plot_a
```

```{r}
# get coefficients of model
b <- exp1g_m1_coef[, "Estimate"]

# get interaction p
p_ixn <- exp1g_m1_planned_dt[contrast == "Interaction", pretty_p]

dodge_width = 0.6
exp1g_plot_b <- ggplot(data = exp1g,
              aes(x = treatment,
                  y = gc,
                  color = experiment_id)) +
  # modeled experiment by treatment means
  geom_point(data = exp1g_m1_emm2,
            aes(y = group_mean,
                color = experiment_id),
            position = position_dodge(width = dodge_width),
            alpha = 1,
            size = 2) +
  geom_line(data = exp1g_m1_emm2,
            aes(y = group_mean,
                group = experiment_id,
                color = experiment_id),
            position = position_dodge(width = dodge_width)) +
  
  # raw data
  geom_point(position = position_dodge(width = dodge_width),
             color = "gray80"
  ) +
  
  # modeled treatment means
  geom_point(data = exp1g_m1_emm_dt,
             aes(x = treatment,
                 y = emmean),
             color = "black",
             size = 3) +
  # geom_errorbar(data = exp1g_m1_emm_dt,
  #            aes(y = emmean,
  #                ymin = lower.CL,
  #                ymax = upper.CL),
  #             color = "black",
  #           width = .05) +
  
  # pvalue brackets
  stat_pvalue_manual(exp1g_m1_planned_dt[1:2],
                     label = "pretty_p",
                     y.position = c(20,20),
                     size = 2.5,
                     tip.length = 0.01) +
  
  # additive line + interaction p bracket
  geom_segment(x = 4 - dodge_width/2,
               y = b[1] + b[2] + b[3],
               xend = 4 + dodge_width/2,
               yend = b[1] + b[2] + b[3],
               linetype = "dashed",
               color = "gray") +
  geom_bracket(
    x = 4 + dodge_width/1.9,
    y = b[1] + b[2] + b[3],
    yend = b[1] + b[2] + b[3] + b[4],
    label = paste0("interaction\np = ", p_ixn),
    text.size = 3,
    text.hjust = 0,
    color = "black") +
  
  ylab("GC (%)") +
  scale_color_manual(values = pal_okabe_ito_blue) +
  theme_pubr() +
  coord_cartesian(xlim = c(1, 4.2)) +
  theme(
    axis.title.x = element_blank(), # no x-axis title
    legend.position = "none"
  ) + 
  NULL
  
exp1g_plot_b <- factor_wrap(exp1g_plot_b)

# exp1g_plot_b
```


```{r, echo=FALSE, fig.width=full_width, fig.asp=0.5}
plot_grid(exp1g_plot_a, exp1g_plot_b, ncol=2)
```

### Repeated measures ANOVA (randomized complete block with no subsampling)

```{r, eval = FALSE}
# this is the rm-ANOVA
m1 <- aov_4(glucose_uptake ~ treatment * activity +
              (treatment * activity | donor),
            data = exp5c)

# lmm equivalent

m2 <- lmer(glucose_uptake ~ treatment * activity +
         (1 | donor) +
         (1 | donor:treatment) +
         (1 | donor:activity),
       data = exp5c)

# random intercept and slope model that is *not* equivalent
# this isn't solvable because there is no subsampling

m3 <- lmer(glucose_uptake ~ treatment * activity +
              (treatment * activity | donor),
            data = exp5c)

```

Notes

1. `afex` computes the repeated measures anova model using both `aov` (classical univariate repeated measures ANOVA) and using `lm` with multiple response variables (the multivariate repeated measures ANOVA). As of this writing, the output from `aov` is the default.
2. Given only one measure for each donor within a $\texttt{treatment} \times \texttt{activity}$ combination, the linear mixed model `m2` is equivalent to the univariate repeated measures model m1.
3. The model formula in `m1` looks like that in the linear mixed model `m3` but the two models are not equivalent. 

#### univariate vs. multivariate repeated measures ANOVA

Use the multivariate model unless you want to replicate the result of someone who used a univariate model. By default, `aov_4` computes only the multivariate model (prior to version xxx, the default was to compute both models).

```{r}
# default mode -- should be multivariate
exp5c_aov <- aov_4(glucose_uptake ~ treatment * activity +
                    (treatment * activity | donor),
                  data = exp5c)

# force aov_4 to compute univariate model
exp5c_aov1 <- aov_4(glucose_uptake ~ treatment * activity +
                    (treatment * activity | donor),
                  data = exp5c,
                  include_aov = TRUE)

# explicitly exclude the univariate model
exp5c_aov2 <- aov_4(glucose_uptake ~ treatment * activity +
                    (treatment * activity | donor),
                  data = exp5c,
                  include_aov = FALSE)
```


Notes

1. The `include_aov = TRUE` argument forces output from `aov_4` to include the univariate model.

**contrasts from multivariate model**

```{r}
# These three should give equivalent results

# exp5c_aov was fit with the default -- multivariate model only
emmeans(exp5c_aov,
        specs = c("treatment", "activity")) %>%
  contrast(method = "revpairwise",
           simple = "each",
           combine = TRUE,
           adjust = "none")

# exp5c_aov included the univariate model but the default is still the multivariate model
emmeans(exp5c_aov1,
        specs = c("treatment", "activity")) %>%
  contrast(method = "revpairwise",
           simple = "each",
           combine = TRUE,
           adjust = "none")

# passing `model = "multivariate"` makes the model choice transparent
emmeans(exp5c_aov1,
        specs = c("treatment", "activity"),
        model = "multivariate") %>%
  contrast(method = "revpairwise",
           simple = "each",
           combine = TRUE,
           adjust = "none")


```

Notes

1. If our rmANOVA model is fit with the default specification (`exp5c_aov1`), we can get the multivariate output using the `model = "multivariate"` argument within `emmeans` (not the `contrast` function!). Or, if our fit excluded the univariate model (`exp5c_aov2`), then we don't need the `model` argument.

**contrasts from univariate model**

```{r}
# get the univariate model results using $aov
emmeans(exp5c_aov1$aov, specs = c("treatment", "activity")) %>%
  contrast(method = "revpairwise",
           simple = "each",
           combine = TRUE,
           adjust = "none")
```

Notes

1. Passing `exp5c_aov1` to `emmeans` will return the contrasts from the multivariate. change this to the univariate model by passing `exp5c_aov1$aov`.

#### The ANOVA table

```{r}
nice(exp5c_aov1, correction = "GG")
nice(exp5c_aov1, correction = "none")
```

Notes

1. The Greenhouse-Geiger ("GG") correction is the default. While the `correction = "GG"` argument is not needed, it makes the script more transparent.
2. For *these data* the Greenhouse-Geiger correction doesn't make a difference in the table.

## Hidden code
### Import exp5c

```{r lmm-exp5c-import-show, echo = TRUE}
data_from <- "Transcriptomic profiling of skeletal muscle adaptations to exercise and inactivity"
file_name <- "41467_2019_13869_MOESM6_ESM.xlsx"
file_path <- here(data_folder, data_from, file_name)

exp5c_wide <- read_excel(file_path,
                         sheet = "Fig5c",
                         range = "A2:M3",
                         col_names = FALSE) %>%
  data.table() %>%
  transpose(make.names = 1)

activity_levels <- c("Basal", "EPS")
treatment_levels <- names(exp5c_wide)
exp5c_wide[, activity := rep(activity_levels, each = 6)]
exp5c_wide[, activity := factor(activity, levels = activity_levels)]

exp5c <- melt(exp5c_wide,
              id.vars = "activity",
              variable.name = "treatment",
              value.name = "glucose_uptake")
exp5c[, treatment := factor(treatment, levels = treatment_levels)]

exp5c[, donor := rep(paste0("donor_", 1:6), 4)]
```

### Import exp1g

```{r lmm-exp1g-import-show, echo=TRUE, message=FALSE}
data_from <- "A GPR174–CCL21 module imparts sexual dimorphism to humoral immunity"
file_name <- "41586_2019_1873_MOESM3_ESM.xlsx"
file_path <- here(data_folder, data_from, file_name)

exp1g_wide <- read_excel(file_path,
                         sheet = "Fig 1g",
                         range = "B4:E25",
                         col_types = c("numeric"),
                         col_names = FALSE) %>%
  data.table()

genotype_levels <- c("Gpr174+", "Gpr174-")
sex_levels <- c("M", "F")
g.by.s_levels <- do.call(paste, expand.grid(genotype_levels, sex_levels))
colnames(exp1g_wide) <- g.by.s_levels

exp_levels <- paste0("exp_", 1:4)
exp1g_wide[, experiment_id := rep(exp_levels, c(5,6,6,5))] #check!
exp1g_wide[, experiment_id := factor(experiment_id)] #check!

exp1g <- melt(exp1g_wide,
              id.vars = "experiment_id",
              measure.vars = g.by.s_levels,
              variable.name = "treatment",
              value.name = "gc") %>% # cell count
  na.omit()

exp1g[, c("genotype", "sex"):= tstrsplit(treatment,
                                             " ",
                                             fixed = TRUE)]
exp1g[, genotype := factor(genotype,
                           levels = genotype_levels)]
exp1g[, sex := factor(sex,
                           levels = sex_levels)]

exp1g_means <- exp1g[, .(gc = mean(gc)),
                     by = .(treatment, genotype, sex, experiment_id)]

```
